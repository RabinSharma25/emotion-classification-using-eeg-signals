{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RabinSharma25/emotion-classification-using-eeg-signals/blob/main/2_classes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pwwbhMy0pssx"
      },
      "outputs": [],
      "source": [
        "#import the required libraries\n",
        "import os\n",
        "import time\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "#for feature extraction\n",
        "from scipy.signal import welch\n",
        "from scipy.integrate import simps\n",
        "\n",
        "#classifier libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "from sklearn import model_selection\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import itertools\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_recall_curve"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(filename):\n",
        "    x = pickle._Unpickler(open(filename, 'rb'))\n",
        "    x.encoding = 'latin1'\n",
        "    p = x.load()\n",
        "    return p"
      ],
      "metadata": {
        "id": "7ZUAXilSp_n9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating the file names of the dataset to load it\n",
        "files = []\n",
        "for n in range (1, 33):\n",
        "    s = 's'\n",
        "    if n < 10:\n",
        "        s += '0'\n",
        "    s += str(n)\n",
        "    s+=str(\".dat\")\n",
        "    files.append(s)\n",
        "print(files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYK5hgFMqFNy",
        "outputId": "063e0725-2774-49b1-d11a-0a06758d153f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['s01.dat', 's02.dat', 's03.dat', 's04.dat', 's05.dat', 's06.dat', 's07.dat', 's08.dat', 's09.dat', 's10.dat', 's11.dat', 's12.dat', 's13.dat', 's14.dat', 's15.dat', 's16.dat', 's17.dat', 's18.dat', 's19.dat', 's20.dat', 's21.dat', 's22.dat', 's23.dat', 's24.dat', 's25.dat', 's26.dat', 's27.dat', 's28.dat', 's29.dat', 's30.dat', 's31.dat', 's32.dat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 32x40 = 1280 trials for 32 participants\n",
        "labels = []\n",
        "data = []\n",
        "drive.mount('/content/drive')\n",
        "for i in files:\n",
        "  filename = \"/content/drive/My Drive/major-project/major-project-dataset/\" + i\n",
        "  trial = read_data(filename)\n",
        "  labels.append(trial['labels'])\n",
        "  data.append(trial['data'])\n"
      ],
      "metadata": {
        "id": "2Ru2cxT3qrtI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c7ad3b4-a258-4abf-df05-d159e504d308"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets see the shapes of the raw data\n",
        "labels = np.array(labels)\n",
        "data = np.array(data)\n",
        "print(\"Labels: \", labels.shape) # participants x videos x labels\n",
        "print(\"Data: \", data.shape) # participants x videos x channels x data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwIUirXzuY_3",
        "outputId": "6ecd9203-36ff-445f-dcf2-1200023e1b04"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels:  (32, 40, 4)\n",
            "Data:  (32, 40, 40, 8064)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-shape arrays into desired shapes\n",
        "labels = labels.flatten()\n",
        "labels = labels.reshape(1280, 4)\n",
        "\n",
        "data = data.flatten()\n",
        "data = data.reshape(1280, 40, 8064)"
      ],
      "metadata": {
        "id": "6JhtYReeudzh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Double-check the new arrays\n",
        "#Here trial = participants x vidoes = 32 x 40 = 1280\n",
        "\n",
        "print(\"Labels: \", labels.shape) # trial x label\n",
        "print(\"Data: \", data.shape) # trial x channel x data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pw4kk4WasX14",
        "outputId": "7deb0a4b-315f-491a-a359-b1fdcadbc259"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels:  (1280, 4)\n",
            "Data:  (1280, 40, 8064)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating the dataframes for the labels\n",
        "labelsDf = pd.DataFrame(labels)\n",
        "print(\"printing the labels dataframe\\n\")\n",
        "print(labelsDf)\n",
        "print(\"\\n\\nDescribing the labels dataframe\")\n",
        "labelsDf.describe()"
      ],
      "metadata": {
        "id": "puNQxQcOx0Gq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "outputId": "8458bc44-ddbe-49be-f31b-1832ed9b4b34"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "printing the labels dataframe\n",
            "\n",
            "         0     1     2     3\n",
            "0     7.71  7.60  6.90  7.83\n",
            "1     8.10  7.31  7.28  8.47\n",
            "2     8.58  7.54  9.00  7.08\n",
            "3     4.94  6.01  6.12  8.06\n",
            "4     6.96  3.92  7.19  6.05\n",
            "...    ...   ...   ...   ...\n",
            "1275  3.91  6.96  5.82  3.12\n",
            "1276  2.81  6.13  6.06  1.04\n",
            "1277  3.05  7.01  5.10  1.10\n",
            "1278  3.99  7.17  4.85  1.00\n",
            "1279  7.15  4.03  9.00  1.88\n",
            "\n",
            "[1280 rows x 4 columns]\n",
            "\n",
            "\n",
            "Describing the labels dataframe\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 0            1            2            3\n",
              "count  1280.000000  1280.000000  1280.000000  1280.000000\n",
              "mean      5.254313     5.156711     5.382750     5.518133\n",
              "std       2.130816     2.020499     2.096321     2.282780\n",
              "min       1.000000     1.000000     1.000000     1.000000\n",
              "25%       3.867500     3.762500     3.932500     3.960000\n",
              "50%       5.040000     5.230000     5.240000     6.050000\n",
              "75%       7.050000     6.950000     7.040000     7.090000\n",
              "max       9.000000     9.000000     9.000000     9.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9414a060-0f4b-4ece-a42e-145a379a8549\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1280.000000</td>\n",
              "      <td>1280.000000</td>\n",
              "      <td>1280.000000</td>\n",
              "      <td>1280.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>5.254313</td>\n",
              "      <td>5.156711</td>\n",
              "      <td>5.382750</td>\n",
              "      <td>5.518133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.130816</td>\n",
              "      <td>2.020499</td>\n",
              "      <td>2.096321</td>\n",
              "      <td>2.282780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>3.867500</td>\n",
              "      <td>3.762500</td>\n",
              "      <td>3.932500</td>\n",
              "      <td>3.960000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>5.040000</td>\n",
              "      <td>5.230000</td>\n",
              "      <td>5.240000</td>\n",
              "      <td>6.050000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>7.050000</td>\n",
              "      <td>6.950000</td>\n",
              "      <td>7.040000</td>\n",
              "      <td>7.090000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9414a060-0f4b-4ece-a42e-145a379a8549')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9414a060-0f4b-4ece-a42e-145a379a8549 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9414a060-0f4b-4ece-a42e-145a379a8549');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cd1e2a53-0705-41a4-92c1-3956fc375e63\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cd1e2a53-0705-41a4-92c1-3956fc375e63')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cd1e2a53-0705-41a4-92c1-3956fc375e63 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"labelsDf\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 450.87147400594165,\n        \"min\": 1.0,\n        \"max\": 1280.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          5.2543125,\n          5.04,\n          1280.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 1,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 450.88279505775586,\n        \"min\": 1.0,\n        \"max\": 1280.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          5.1567109375,\n          5.23,\n          1280.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 2,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 450.85389766830843,\n        \"min\": 1.0,\n        \"max\": 1280.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          5.38275,\n          5.24,\n          1280.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 3,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 450.79289858765816,\n        \"min\": 1.0,\n        \"max\": 1280.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          5.5181328125,\n          6.05,\n          1280.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#giving names to the label columns\n",
        "labelsDf= pd.DataFrame({'Valence': labels[:,0], 'Arousal': labels[:,1], 'Dominance': labels[:,2], 'Liking': labels[:,3]})\n",
        "print(labelsDf.describe())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SGS2ghM3uYC",
        "outputId": "3aeac8cc-1ede-49d0-f779-15bbcadcb52c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Valence      Arousal    Dominance       Liking\n",
            "count  1280.000000  1280.000000  1280.000000  1280.000000\n",
            "mean      5.254313     5.156711     5.382750     5.518133\n",
            "std       2.130816     2.020499     2.096321     2.282780\n",
            "min       1.000000     1.000000     1.000000     1.000000\n",
            "25%       3.867500     3.762500     3.932500     3.960000\n",
            "50%       5.040000     5.230000     5.240000     6.050000\n",
            "75%       7.050000     6.950000     7.040000     7.090000\n",
            "max       9.000000     9.000000     9.000000     9.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dropping the Dominance and Liking columns\n",
        "labelsDf=labelsDf.drop('Dominance',axis=1)\n",
        "labelsDf=labelsDf.drop('Liking',axis=1)\n",
        "print(labelsDf.describe())\n",
        "# df = df.drop('B', axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3boVQ1FE4var",
        "outputId": "e6a19467-0474-4877-b4f1-7b8315a4f397"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Valence      Arousal\n",
            "count  1280.000000  1280.000000\n",
            "mean      5.254313     5.156711\n",
            "std       2.130816     2.020499\n",
            "min       1.000000     1.000000\n",
            "25%       3.867500     3.762500\n",
            "50%       5.040000     5.230000\n",
            "75%       7.050000     6.950000\n",
            "max       9.000000     9.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset with only Valence column\n",
        "df_val = labelsDf['Valence']\n",
        "# Dataset with only Arousal column\n",
        "df_aro = labelsDf['Arousal']"
      ],
      "metadata": {
        "id": "Pjhzu_pQlWDv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to check if each trial has positive or negative valence\n",
        "def positive_valence(trial):\n",
        "    return 1 if labels[trial,0] >= np.median(labels[:,0]) else 0\n",
        "# Function to check if each trial has high or low arousal\n",
        "def high_arousal(trial):\n",
        "    return 1 if labels[trial,1] >= np.median(labels[:,1]) else 0"
      ],
      "metadata": {
        "id": "u8H4KOWLll-C"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all ratings to boolean values\n",
        "labels_encoded = []\n",
        "for i in range (len(labels)):\n",
        "    labels_encoded.append([positive_valence(i), high_arousal(i)])\n",
        "labels_encoded = np.reshape(labels_encoded, (1280, 2))\n",
        "df_labels = pd.DataFrame(data=labels_encoded, columns=[\"High Valence\", \"High Arousal\"])\n",
        "print(df_labels.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37Gsz-Z_lp_2",
        "outputId": "14445673-4ee8-4c86-a79f-87139f3319e3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       High Valence  High Arousal\n",
            "count   1280.000000   1280.000000\n",
            "mean       0.531250      0.500000\n",
            "std        0.499218      0.500195\n",
            "min        0.000000      0.000000\n",
            "25%        0.000000      0.000000\n",
            "50%        1.000000      0.500000\n",
            "75%        1.000000      1.000000\n",
            "max        1.000000      1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset with only Valence column\n",
        "df_valence = df_labels['High Valence']\n",
        "# Dataset with only Arousal column\n",
        "df_arousal = df_labels['High Arousal']"
      ],
      "metadata": {
        "id": "4BrMAjqult5R"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FEATURE EXTRACTION USING WELCH'S METHOD"
      ],
      "metadata": {
        "id": "Gh0IevZcnKtX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eeg_channels = np.array([\"Fp1\", \"AF3\", \"F3\", \"F7\", \"FC5\", \"FC1\", \"C3\", \"T7\", \"CP5\", \"CP1\", \"P3\", \"P7\", \"PO3\", \"O1\", \"Oz\", \"Pz\", \"Fp2\", \"AF4\", \"Fz\", \"F4\", \"F8\", \"FC6\", \"FC2\", \"Cz\", \"C4\", \"T8\", \"CP6\", \"CP2\", \"P4\", \"P8\", \"PO4\", \"O2\"])\n",
        "peripheral_channels = np.array([\"hEOG\", \"vEOG\", \"zEMG\", \"tEMG\", \"GSR\", \"Respiration belt\", \"Plethysmograph\", \"Temperature\"])"
      ],
      "metadata": {
        "id": "QGtC7nRipvA8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eeg_data = []\n",
        "for i in range (len(data)):\n",
        "  for j in range (len(eeg_channels)):\n",
        "    eeg_data.append(data[i,j])\n",
        "eeg_data = np.reshape(eeg_data, (len(data), len(eeg_channels), len(data[0,0])))\n",
        "print(eeg_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyC_7IWLp93t",
        "outputId": "3e59f4c1-0fee-419a-8d5c-bbd6d3d889ee"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1280, 32, 8064)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "peripheral_data = []\n",
        "for i in range (len(data)):\n",
        "  for j in range (32,len(data[0])):\n",
        "    peripheral_data.append(data[i,j])\n",
        "peripheral_data = np.reshape(peripheral_data, (len(data), len(peripheral_channels), len(data[0,0])))\n",
        "print(peripheral_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5P3G0PWqEsM",
        "outputId": "1daac2a2-d78f-4641-fce3-382ac979682b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1280, 8, 8064)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def bandpower(data, sf, band, window_sec=None, relative=False):\n",
        "    band = np.asarray(band)\n",
        "    low, high = band\n",
        "\n",
        "    # Define window length\n",
        "    if window_sec is not None:\n",
        "        nperseg = window_sec * sf\n",
        "    else:\n",
        "        nperseg = (2 / low) * sf\n",
        "\n",
        "    # Compute the modified periodogram (Welch)\n",
        "    freqs, psd = welch(data, sf, nperseg=nperseg)\n",
        "\n",
        "    # Frequency resolution\n",
        "    freq_res = freqs[1] - freqs[0]\n",
        "\n",
        "    # Find closest indices of band in frequency vector\n",
        "    idx_band = np.logical_and(freqs >= low, freqs <= high)\n",
        "\n",
        "    # Integral approximation of the spectrum using Simpson's rule.\n",
        "    bp = simps(psd[idx_band], dx=freq_res)\n",
        "\n",
        "    if relative:\n",
        "        bp /= simps(psd, dx=freq_res)\n",
        "    return bp"
      ],
      "metadata": {
        "id": "Xc9rTRGHnYKt"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_band_power(trial, channel, band):\n",
        "  bd = (0,0)\n",
        "\n",
        "  if (band == \"theta\"): # drownsiness, emotional connection, intuition, creativity\n",
        "    bd = (4,8)\n",
        "  elif (band == \"alpha\"): # reflection, relaxation\n",
        "    bd = (8,12)\n",
        "  elif (band == \"beta\"): # concentration, problem solving, memory\n",
        "    bd = (12,30)\n",
        "  elif (band == \"gamma\"): # cognition, perception, learning, multi-tasking\n",
        "    bd = (30,64)\n",
        "\n",
        "  return bandpower(eeg_data[trial,channel], 128, bd)\n"
      ],
      "metadata": {
        "id": "mIxttAuQnrel"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_band_power(0,31,\"theta\"))\n",
        "print(get_band_power(0,31,\"alpha\"))\n",
        "print(get_band_power(0,31,\"beta\"))\n",
        "print(get_band_power(0,31,\"gamma\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpNyS4s8n9Q3",
        "outputId": "b904bafd-6ce7-46ec-baa8-4fb62be7515e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.434119660168186\n",
            "5.369595513295193\n",
            "6.286556266834863\n",
            "0.9879159580139809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process new datasets with 6 EEG regions and 4 band power values"
      ],
      "metadata": {
        "id": "8psQHMB9xqpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform 1280x 32 x 8064 => 1280 x 128\n",
        "eeg_band_arr = []\n",
        "for i in range (len(eeg_data)):\n",
        "  for j in range (len(eeg_data[0])):\n",
        "    eeg_band_arr.append(get_band_power(i,j,\"theta\"))\n",
        "    eeg_band_arr.append(get_band_power(i,j,\"alpha\"))\n",
        "    eeg_band_arr.append(get_band_power(i,j,\"beta\"))\n",
        "    eeg_band_arr.append(get_band_power(i,j,\"gamma\"))\n",
        "eeg_band_arr = np.reshape(eeg_band_arr, (1280, 128))"
      ],
      "metadata": {
        "id": "r0y_hNHIxoxi"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frontal = np.array([\"F3\", \"FC1\", \"Fz\", \"F4\", \"FC2\"])\n",
        "parietal = np.array([\"P3\", \"P7\", \"Pz\", \"P4\", \"P8\"])\n",
        "occipital = np.array([\"O1\", \"Oz\", \"O2\", \"PO3\", \"PO4\"])\n",
        "central = np.array([\"CP5\", \"CP1\", \"Cz\", \"C4\", \"C3\", \"CP6\", \"CP2\"])"
      ],
      "metadata": {
        "id": "HVksA4V2ypUD"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eeg_theta = []\n",
        "for i in range (len(eeg_data)):\n",
        "  for j in range (len(eeg_data[0])):\n",
        "    eeg_theta.append(get_band_power(i,j,\"theta\"))\n",
        "eeg_theta = np.reshape(eeg_theta, (1280, 32))\n",
        "\n",
        "df_theta = pd.DataFrame(data = eeg_theta, columns=eeg_channels)\n",
        "print(df_theta.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YQbtKTRzmdG",
        "outputId": "c9f72e34-b62e-4025-9c3e-c9b1f7550645"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                Fp1           AF3            F3            F7           FC5  \\\n",
            "count   1280.000000   1280.000000   1280.000000   1280.000000   1280.000000   \n",
            "mean     517.431002    974.493856    795.093910   1515.004071    746.435950   \n",
            "std     1165.295163   3147.555132   3020.711666   4990.544508   2188.101734   \n",
            "min        2.698928      1.995398      1.820656      3.283107      1.311200   \n",
            "25%       23.306027     17.117475     18.392335     30.827191     14.023436   \n",
            "50%       65.468048     82.102529     60.241953     91.266539     48.704968   \n",
            "75%      331.636624    304.949248    175.394835    248.308696    218.358248   \n",
            "max    15524.135098  38122.870846  39431.320394  49272.793208  20182.668545   \n",
            "\n",
            "               FC1            C3           T7           CP5          CP1  ...  \\\n",
            "count  1280.000000   1280.000000  1280.000000   1280.000000  1280.000000  ...   \n",
            "mean    345.936665    486.095113   354.004358    664.656754   276.667812  ...   \n",
            "std     717.328768   1497.636647   641.432373   2146.857585   498.823733  ...   \n",
            "min       2.025214      1.200156     3.418167      1.857737     1.340340  ...   \n",
            "25%      17.383454     18.273844    43.838157     19.178089    28.281757  ...   \n",
            "50%      55.561232     46.451432   128.629588     61.998792    60.330974  ...   \n",
            "75%     281.828747    234.342275   348.249862    312.653366   237.040645  ...   \n",
            "max    8542.244175  26021.167025  7023.985761  23255.512271  5972.589469  ...   \n",
            "\n",
            "                FC2            Cz            C4            T8           CP6  \\\n",
            "count   1280.000000   1280.000000   1280.000000   1280.000000   1280.000000   \n",
            "mean     669.953166    518.975872    471.905917    763.990397    544.785103   \n",
            "std     1714.340609   1407.210210   1896.584105   2236.655420   1563.878765   \n",
            "min        2.809501      1.637028      1.713283      2.133474      1.496299   \n",
            "25%       14.927239     23.440039      9.724978     25.143155     18.202863   \n",
            "50%       66.078472     67.044674     24.318235     90.110040     77.517763   \n",
            "75%      214.293557    305.766182    109.715681    386.232875    297.742897   \n",
            "max    18617.218099  16408.471958  24826.365142  28038.714329  19908.437378   \n",
            "\n",
            "               CP2            P4           P8           PO4            O2  \n",
            "count  1280.000000   1280.000000  1280.000000   1280.000000   1280.000000  \n",
            "mean    303.158814    527.491149   222.759444    780.700914    327.242856  \n",
            "std     650.093432   1429.084909   451.119818   1802.083002   1080.707954  \n",
            "min       1.439077      1.791296     1.641482      1.995230      3.681268  \n",
            "25%      26.754006     18.110041    18.064264     23.491991     19.333926  \n",
            "50%      88.083088     62.269206    90.914662     69.772647     49.630345  \n",
            "75%     257.959593    246.195582   225.463786    495.202699    113.117403  \n",
            "max    7210.746793  17568.065100  5487.311705  17167.017710  12314.030522  \n",
            "\n",
            "[8 rows x 32 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform 880 x 32 x 8064 => 880 x 32\n",
        "eeg_alpha = []\n",
        "for i in range (len(eeg_data)):\n",
        "  for j in range (len(eeg_data[0])):\n",
        "    eeg_alpha.append(get_band_power(i,j,\"alpha\"))\n",
        "eeg_alpha = np.reshape(eeg_alpha, (1280, 32))\n",
        "\n",
        "df_alpha = pd.DataFrame(data = eeg_alpha, columns=eeg_channels)\n",
        "print(df_alpha.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1uT6ZlnzqL-",
        "outputId": "7910c210-3bec-4953-c262-de7646d292f5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               Fp1           AF3            F3            F7          FC5  \\\n",
            "count  1280.000000   1280.000000   1280.000000   1280.000000  1280.000000   \n",
            "mean    173.544174    333.184883    285.545436    573.725974   249.793720   \n",
            "std     378.371554   1058.758333   1036.330438   1943.692827   702.436934   \n",
            "min       2.770151      1.793012      1.724442      2.793554     0.975527   \n",
            "25%      14.082681     10.737435     10.012318     15.888935    10.074042   \n",
            "50%      33.713172     34.992442     30.026795     38.178299    20.079100   \n",
            "75%     125.508396    114.334883     73.488279     93.987498    74.040939   \n",
            "max    5627.906982  12380.702125  12764.724842  20843.070851  6575.781434   \n",
            "\n",
            "               FC1           C3           T7          CP5          CP1  ...  \\\n",
            "count  1280.000000  1280.000000  1280.000000  1280.000000  1280.000000  ...   \n",
            "mean    138.011594   170.423962   124.101183   232.750895    97.610644  ...   \n",
            "std     275.218934   497.507620   200.171342   735.717912   166.430024  ...   \n",
            "min       1.682977     1.963403     2.545447     2.251935     1.682386  ...   \n",
            "25%       9.228670    10.197058    21.452775    11.461012    13.367379  ...   \n",
            "50%      24.929907    22.318468    51.473641    25.364340    26.229818  ...   \n",
            "75%     117.988147    83.518174   129.890552   110.822931    91.941255  ...   \n",
            "max    3030.077791  9215.549575  2142.437275  7894.273428  2148.109415  ...   \n",
            "\n",
            "               FC2           Cz            C4           T8          CP6  \\\n",
            "count  1280.000000  1280.000000   1280.000000  1280.000000  1280.000000   \n",
            "mean    228.331178   199.941306    193.380344   276.638039   213.620700   \n",
            "std     569.202821   578.947819    812.031964   782.782127   661.135030   \n",
            "min       2.390011     1.315395      1.227024     1.636114     2.181082   \n",
            "25%       8.715210    12.127290      6.665166    15.980086    11.300805   \n",
            "50%      26.880000    29.827367     14.169702    34.252038    31.722499   \n",
            "75%      95.964685   102.480314     40.800135   142.704220   106.673844   \n",
            "max    6038.451512  6881.486185  10501.570698  9022.269308  8394.200372   \n",
            "\n",
            "               CP2           P4           P8          PO4           O2  \n",
            "count  1280.000000  1280.000000  1280.000000  1280.000000  1280.000000  \n",
            "mean    112.138023   181.507936    89.003586   269.572920   126.053116  \n",
            "std     236.260122   482.163785   186.841170   597.330120   400.465505  \n",
            "min       1.520363     2.158950     1.212432     2.002769     3.617116  \n",
            "25%      15.797490    11.564300    10.971965    13.489443    10.548845  \n",
            "50%      37.807211    28.494994    38.758864    29.247807    23.906708  \n",
            "75%      86.070815    92.261617    88.107272   162.117585    45.294454  \n",
            "max    2368.537880  5666.733316  2323.499853  5542.263376  3966.714864  \n",
            "\n",
            "[8 rows x 32 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform 880 x 32 x 8064 => 880 x 32\n",
        "eeg_beta = []\n",
        "for i in range (len(eeg_data)):\n",
        "  for j in range (len(eeg_data[0])):\n",
        "    eeg_beta.append(get_band_power(i,j,\"beta\"))\n",
        "eeg_beta = np.reshape(eeg_beta, (1280, 32))\n",
        "\n",
        "df_beta = pd.DataFrame(data = eeg_beta, columns=eeg_channels)\n",
        "print(df_beta.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUNGxdRLzvAW",
        "outputId": "ca9c835a-0919-45a9-8ad4-d86d17c28459"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               Fp1          AF3           F3            F7          FC5  \\\n",
            "count  1280.000000  1280.000000  1280.000000   1280.000000  1280.000000   \n",
            "mean     92.200266   200.684361   193.372441    341.951096   140.642394   \n",
            "std     176.162562   668.027842   668.771074   1274.679748   386.740178   \n",
            "min       4.486703     2.650970     2.784376      3.363422     2.788531   \n",
            "25%      17.999409    13.190613    10.170223     17.951892    11.341571   \n",
            "50%      34.413828    29.155242    28.120517     33.617573    22.402198   \n",
            "75%      80.774739   107.619088    59.807546     62.087123    53.162211   \n",
            "max    3528.485435  5784.160193  5841.475721  14848.700463  3094.687504   \n",
            "\n",
            "               FC1           C3           T7          CP5          CP1  ...  \\\n",
            "count  1280.000000  1280.000000  1280.000000  1280.000000  1280.000000  ...   \n",
            "mean     99.397187    96.901859    77.540922   137.387625    55.047411  ...   \n",
            "std     197.556641   247.621839    90.107877   409.366974    85.846400  ...   \n",
            "min       3.145262     2.991550     2.477519     2.886994     2.237445  ...   \n",
            "25%       8.919796     9.331138    23.668656    11.097811     9.370318  ...   \n",
            "50%      17.625226    19.698019    44.350493    22.622621    18.748879  ...   \n",
            "75%      79.066898    46.494215    93.087474    58.273844    51.308093  ...   \n",
            "max    1702.542675  5187.441594   634.645675  4094.116788  1238.024108  ...   \n",
            "\n",
            "               FC2           Cz           C4           T8          CP6  \\\n",
            "count  1280.000000  1280.000000  1280.000000  1280.000000  1280.000000   \n",
            "mean    130.756212   131.961699   136.536661   178.534190   144.021213   \n",
            "std     338.843374   409.481195   603.902680   506.405387   484.806320   \n",
            "min       3.037555     1.816169     2.079646     2.429178     2.524762   \n",
            "25%       9.115906    10.803851     7.350435    17.466794    11.400277   \n",
            "50%      17.479204    25.300435    12.926731    30.157902    23.679679   \n",
            "75%      75.259937    54.745901    25.609433    93.320360    64.338413   \n",
            "max    2786.622358  4842.537927  7489.480637  4064.418656  5955.637009   \n",
            "\n",
            "               CP2           P4           P8          PO4           O2  \n",
            "count  1280.000000  1280.000000  1280.000000  1280.000000  1280.000000  \n",
            "mean     74.751315   103.971557    62.850280   148.868580    90.837375  \n",
            "std     165.803855   288.941591   134.177873   329.650542   275.150372  \n",
            "min       2.272088     2.914214     3.059450     2.461123     5.084168  \n",
            "25%      13.014515    11.560334    12.312069    13.732279    11.291516  \n",
            "50%      24.705624    20.439224    27.763129    23.940855    19.688820  \n",
            "75%      53.062550    67.593128    64.187473    99.947093    36.076683  \n",
            "max    1265.234960  2533.794744  1653.743590  2516.334382  2454.759737  \n",
            "\n",
            "[8 rows x 32 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform 880 x 32 x 8064 => 880 x 32\n",
        "eeg_gamma = []\n",
        "for i in range (len(eeg_data)):\n",
        "  for j in range (len(eeg_data[0])):\n",
        "    eeg_gamma.append(get_band_power(i,j,\"gamma\"))\n",
        "eeg_gamma = np.reshape(eeg_gamma, (1280, 32))\n",
        "\n",
        "df_gamma = pd.DataFrame(data = eeg_gamma, columns=eeg_channels)\n",
        "print(df_gamma.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJP58i-60AEV",
        "outputId": "c4f7c8ce-fc8e-4743-b7e3-3d5b99cf15fc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               Fp1          AF3           F3           F7          FC5  \\\n",
            "count  1280.000000  1280.000000  1280.000000  1280.000000  1280.000000   \n",
            "mean     50.663247   150.492372   149.210824   122.775294    91.633571   \n",
            "std     135.591365   645.635199   632.049548   496.744509   340.562025   \n",
            "min       1.069233     1.119910     0.791859     1.177512     0.712624   \n",
            "25%       8.292302     5.605668     4.352724     7.591098     5.172920   \n",
            "50%      18.329100    14.069330    11.828027    15.919877    12.648067   \n",
            "75%      39.995543    43.454610    28.586812    36.456420    38.094737   \n",
            "max    3213.255538  6060.539788  5758.756812  6348.177208  3472.515858   \n",
            "\n",
            "               FC1           C3           T7          CP5          CP1  ...  \\\n",
            "count  1280.000000  1280.000000  1280.000000  1280.000000  1280.000000  ...   \n",
            "mean     60.487715    56.236468    43.792285    77.048369    29.250057  ...   \n",
            "std     139.230040   180.285647    62.089299   267.809883    68.503827  ...   \n",
            "min       0.694928     0.684210     0.562676     0.788359     0.509259  ...   \n",
            "25%       2.666459     3.534375    10.492723     3.837095     3.156311  ...   \n",
            "50%       5.313502     9.056340    23.540152     9.532963     7.735783  ...   \n",
            "75%      41.027482    29.838450    54.382720    22.302862    18.604669  ...   \n",
            "max    1413.985901  4376.356658   809.935644  2294.561961  1011.985310  ...   \n",
            "\n",
            "               FC2           Cz           C4           T8          CP6  \\\n",
            "count  1280.000000  1280.000000  1280.000000  1280.000000  1280.000000   \n",
            "mean     79.191679    56.547283    55.989289   117.745424    64.340498   \n",
            "std     306.464243   167.968314   239.143748   428.381018   199.387966   \n",
            "min       0.692350     0.395680     0.697486     1.027260     0.609425   \n",
            "25%       2.893590     3.800780     2.286229     7.221083     3.850387   \n",
            "50%       7.535055     8.493068     4.640151    17.921722    10.563661   \n",
            "75%      26.896601    20.882885    13.538524    39.981006    30.943717   \n",
            "max    2829.052404  1892.813293  3054.309237  3954.519861  2405.614840   \n",
            "\n",
            "               CP2           P4           P8          PO4           O2  \n",
            "count  1280.000000  1280.000000  1280.000000  1280.000000  1280.000000  \n",
            "mean     46.943309    68.100629    29.805883    82.197819    55.821892  \n",
            "std     142.501802   265.168885    57.843154   272.880509   203.288109  \n",
            "min       0.478152     0.761387     0.804002     0.838747     0.877576  \n",
            "25%       3.849956     3.690054     4.513405     4.620006     4.380690  \n",
            "50%       8.646393     7.300040    10.911662    10.430337     6.994829  \n",
            "75%      22.614200    28.908609    26.489674    42.960182    14.567389  \n",
            "max    1426.462453  2446.765512   647.533599  2505.826127  1794.801694  \n",
            "\n",
            "[8 rows x 32 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training/testing sets\n",
        "def split_train_test(x, y):\n",
        "  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)\n",
        "  return x_train, x_test, y_train, y_test"
      ],
      "metadata": {
        "id": "kmTpi6Qc02Bu"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature scaling\n",
        "def feature_scaling(train, test):\n",
        "  sc = StandardScaler()\n",
        "  train = sc.fit_transform(train)\n",
        "  test = sc.transform(test)\n",
        "  return train, test"
      ],
      "metadata": {
        "id": "cOCvaWDba3pa"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "band_names = np.array([\"theta\", \"alpha\", \"beta\", \"gamma\"])\n",
        "channel_names = np.array([\"frontal\",  \"central\", \"parietal\", \"occipital\"])\n",
        "label_names = np.array([\"valence\", \"arousal\"])"
      ],
      "metadata": {
        "id": "EHrmKN1nf8ys"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing different kernels (linear, sigmoid, rbf, poly) to select the most optimal one\n",
        "clf_svm = SVC(kernel = 'rbf', random_state = 42, probability=True)"
      ],
      "metadata": {
        "id": "tYVZwPvFgATk"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing different k (odd) numbers, algorithm (auto, ball_tree, kd_tree) and weight (uniform, distance) to select the most optimal one\n",
        "clf_knn = KNeighborsClassifier(n_neighbors=5, weights='distance', algorithm='auto')"
      ],
      "metadata": {
        "id": "t4pznSOZgHmK"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_dtree=DecisionTreeClassifier(max_depth=20,min_samples_split=4)"
      ],
      "metadata": {
        "id": "7gi_aMFOgL08"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_rf=RandomForestClassifier(n_estimators=50,max_depth=20,min_samples_split=5)"
      ],
      "metadata": {
        "id": "T9hH4fktgPOX"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_nb= GaussianNB()"
      ],
      "metadata": {
        "id": "7lJ9wwn0gUmH"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing different learning rate (alpha), solver (adam, sgd, lbfgs) and activation (relu, tanh, logistic) to select the most optimal one\n",
        "clf_mlp = MLPClassifier(solver='adam', activation='tanh', alpha=0.3, max_iter=400)"
      ],
      "metadata": {
        "id": "h-f5ay_0gYey"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_adaboost = AdaBoostClassifier(n_estimators=50, random_state=42)\n"
      ],
      "metadata": {
        "id": "vOCq_XttoT4e"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_xgb = xgb.XGBClassifier(learning_rate=0.1, max_depth=3, n_estimators=100, random_state=42)\n"
      ],
      "metadata": {
        "id": "xJMuk2lWq8v6"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "clf_lgbm = lgb.LGBMClassifier()"
      ],
      "metadata": {
        "id": "KFc0oT1XMHGv"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "clf_gpc = GaussianProcessClassifier(kernel=1.0 * RBF(length_scale=1.0), random_state=42)\n"
      ],
      "metadata": {
        "id": "G_SB4HCTNIWQ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "clf_perceptron = Perceptron(random_state=42)"
      ],
      "metadata": {
        "id": "xQ_4Dt-2N5t8"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost\n",
        "import catboost as cb\n",
        "clf_catboost = cb.CatBoostClassifier(iterations=1000, depth=6, learning_rate=0.1, loss_function='Logloss', verbose=0)\n"
      ],
      "metadata": {
        "id": "9pheOwL8PlYt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48b00079-9d67-42b6-ba21-89e65c8708aa"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.5-cp310-cp310-manylinux2014_x86_64.whl (98.2 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m98.2/98.2 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.25.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.11.4)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.4.1)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define cnn model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "def create_cnn_model(data_np):\n",
        "    model = Sequential([\n",
        "        Input(shape=(data_np.shape[1], 1)),  # Adjusted to match data_np shape\n",
        "        Conv1D(filters=64, kernel_size=1, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Dropout(0.2),\n",
        "        Conv1D(filters=64, kernel_size=1, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Dropout(0.2),\n",
        "        Flatten(),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.1),\n",
        "        Dense(2, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def train_and_evaluate_cnn(df_x, df_y):\n",
        "    # Convert dataframes to numpy arrays\n",
        "    data_np = df_x.values\n",
        "    labels_np = df_y.values\n",
        "\n",
        "    # Reshape data for CNN: (number of samples, height, width, channels)\n",
        "    # Here, height=1, width=number of features, channels=1\n",
        "    data_reshaped = data_np.reshape((data_np.shape[0], data_np.shape[1], 1))\n",
        "\n",
        "    # One-hot encode the labels\n",
        "    labels_encoded = to_categorical(labels_np)\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(data_reshaped, labels_encoded, test_size=0.20, random_state=42)\n",
        "\n",
        "    # Define the CNN model\n",
        "    model = create_cnn_model(data_np)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "    return test_loss, test_accuracy*100\n"
      ],
      "metadata": {
        "id": "llDxjmEzwD4b"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define ann model\n",
        "def create_ann_model(data_scaled):\n",
        "    model = Sequential([\n",
        "        Dense(64, input_dim=data_scaled.shape[1], activation='relu'),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(2, activation='softmax')  # Assuming two classes: 0 and 1\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_and_evaluate_ann(df_x, df_y):\n",
        "    # Convert dataframes to numpy arrays\n",
        "    data_np = df_x.values\n",
        "    labels_np = df_y.values\n",
        "\n",
        "    # Standardize the data\n",
        "    scaler = StandardScaler()\n",
        "    data_scaled = scaler.fit_transform(data_np)\n",
        "\n",
        "    # One-hot encode the labels\n",
        "    labels_encoded = to_categorical(labels_np)\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(data_scaled, labels_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define the CNN model\n",
        "    model = create_ann_model(data_np)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "    return test_loss, test_accuracy*100"
      ],
      "metadata": {
        "id": "WJP4i5h0wF5q"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define lstm model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "def create_lstm_model(data_np):\n",
        "    model = Sequential([\n",
        "    LSTM(20, input_shape=(data_np.shape[1], 1)),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(2, activation='softmax')  # Assuming two classes: 0 and 1\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def train_and_evaluate_lstm(df_x, df_y):\n",
        "    # Convert dataframes to numpy arrays\n",
        "    data_np = df_x.values\n",
        "    labels_np = df_y.values\n",
        "\n",
        "    # Reshape data for LSTM: (samples, time_steps, features)\n",
        "    # Here, each row is treated as a sequence with a single time step\n",
        "    data_reshaped = data_np.reshape((data_np.shape[0], data_np.shape[1], 1))\n",
        "\n",
        "    # One-hot encode the labels\n",
        "    labels_encoded = to_categorical(labels_np)\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(data_reshaped, labels_encoded, test_size=0.2, random_state=42)\n",
        "    # Define the CNN model\n",
        "    model = create_lstm_model(data_np)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "    return test_loss, test_accuracy*100"
      ],
      "metadata": {
        "id": "gTtgqxsewObj"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = []\n",
        "models.append(('SVM', clf_svm))\n",
        "models.append(('k-NN', clf_knn))\n",
        "models.append(('DT', clf_dtree))\n",
        "models.append(('RF', clf_rf))\n",
        "models.append(('NB', clf_nb))\n",
        "models.append(('MLP', clf_mlp))\n",
        "models.append(('AB', clf_adaboost))\n",
        "models.append(('XGB', clf_xgb))"
      ],
      "metadata": {
        "id": "i28DLITPgfgJ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_clf_cv(band, channel, label, clf):\n",
        "  if (band == \"theta\"):\n",
        "    df_x = df_theta\n",
        "  elif (band == \"alpha\"):\n",
        "    df_x = df_alpha\n",
        "  elif (band == \"beta\"):\n",
        "    df_x = df_beta\n",
        "  elif (band == \"gamma\"):\n",
        "    df_x = df_gamma\n",
        "\n",
        "  if (channel == \"frontal\"):\n",
        "    df_x = df_x[frontal]\n",
        "  elif (channel == \"central\"):\n",
        "    df_x = df_x[central]\n",
        "  elif (channel == \"parietal\"):\n",
        "    df_x = df_x[parietal]\n",
        "  elif (channel == \"occipital\"):\n",
        "    df_x = df_x[occipital]\n",
        "\n",
        "  df_y = df_arousal if (label == \"arousal\") else df_valence\n",
        "\n",
        "  # Train-test split\n",
        "  x_train, x_test, y_train, y_test = split_train_test(df_x, df_y)\n",
        "\n",
        "  # Apply CV\n",
        "  x_for_kfold = np.array(x_train)\n",
        "  y_for_kfold = np.array(y_train)\n",
        "  kfold = model_selection.KFold(n_splits=5)\n",
        "\n",
        "  for i, j in kfold.split(x_for_kfold):\n",
        "   x_train2, x_test2 = x_for_kfold[i], x_for_kfold[j]\n",
        "   y_train2, y_test2 = y_for_kfold[i], y_for_kfold[j]\n",
        "\n",
        "  # Feature scaling\n",
        "  x_train2, x_test2 = feature_scaling(x_train2, x_test2)\n",
        "\n",
        "  if (clf == \"svm\"):\n",
        "    clf_svm.fit(x_train2, y_train2)\n",
        "    y_predict = clf_svm.predict(x_test2)\n",
        "  elif (clf == \"knn\"):\n",
        "    clf_knn.fit(x_train2, y_train2)\n",
        "    y_predict = clf_knn.predict(x_test2)\n",
        "  elif (clf == \"dtree\"):\n",
        "    clf_dtree.fit(x_train2, y_train2)\n",
        "    y_predict = clf_dtree.predict(x_test2)\n",
        "  elif (clf == \"rf\"):\n",
        "    clf_rf.fit(x_train2, y_train2)\n",
        "    y_predict = clf_rf.predict(x_test2)\n",
        "  elif (clf == \"nb\"):\n",
        "    clf_nb.fit(x_train2, y_train2)\n",
        "    y_predict = clf_nb.predict(x_test2)\n",
        "  elif (clf == \"mlp\"):\n",
        "    clf_mlp.fit(x_train2, y_train2)\n",
        "    y_predict = clf_mlp.predict(x_test2)\n",
        "  elif (clf == \"ab\"):\n",
        "    clf_adaboost.fit(x_train2, y_train2)\n",
        "    y_predict = clf_adaboost.predict(x_test2)\n",
        "  elif (clf == \"xgb\"):\n",
        "    clf_xgb.fit(x_train2, y_train2)\n",
        "    y_predict = clf_xgb.predict(x_test2)\n",
        "  elif (clf == \"lgbm\"):\n",
        "    clf_lgbm.fit(x_train2, y_train2)\n",
        "    y_predict = clf_lgbm.predict(x_test2)\n",
        "  elif (clf == \"gpc\"):\n",
        "    clf_gpc.fit(x_train2, y_train2)\n",
        "    y_predict = clf_gpc.predict(x_test2)\n",
        "  elif (clf == \"per\"):\n",
        "    clf_perceptron.fit(x_train2, y_train2)\n",
        "    y_predict = clf_perceptron.predict(x_test2)\n",
        "\n",
        "  elif (clf == \"cb\"):\n",
        "    clf_catboost.fit(x_train2, y_train2)\n",
        "    y_predict = clf_catboost.predict(x_test2)\n",
        "\n",
        "  elif(clf ==\"ann\"):\n",
        "    test_loss, test_accuracy = train_and_evaluate_ann(df_x, df_y)\n",
        "    # print(f\"Test Loss: {test_loss}\")\n",
        "    # print(f\"Test Accuracy: {test_accuracy}\")\n",
        "    return \"DL\",test_loss, test_accuracy\n",
        "  elif(clf ==\"lstm\"):\n",
        "    test_loss, test_accuracy = train_and_evaluate_lstm(df_x, df_y)\n",
        "    return \"DL\",test_loss,test_accuracy\n",
        "  elif(clf ==\"cnn\"):\n",
        "    test_loss, test_accuracy = train_and_evaluate_cnn(df_x, df_y)\n",
        "    return \"DL\",test_loss,test_accuracy\n",
        "\n",
        "  return \"ML\",y_test2, y_predict"
      ],
      "metadata": {
        "id": "L_WzJMzsjrL4"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(band, channel, label, clf):\n",
        "  classifier,y_test2, y_predict = run_clf_cv(band, channel, label, clf)\n",
        "  if (classifier == \"DL\"):\n",
        "    return y_predict\n",
        "  return np.round(accuracy_score(y_test2, y_predict)*100,2)"
      ],
      "metadata": {
        "id": "gWN0kvbJjyhB"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "def print_conf(band, channel, label, clf):\n",
        "  y_test2, y_predict = run_clf_cv(band, channel, label, clf)\n",
        "  conf_matrix = confusion_matrix(y_test2, y_predict)\n",
        "  # print(conf_matrix)\n",
        "  plt.figure(figsize=(4, 2))  # Decrease the figure size\n",
        "  plt.title('Confusion Matrix')\n",
        "  sns.heatmap(conf_matrix,\n",
        "              annot=True,\n",
        "              fmt='g',\n",
        "              xticklabels=['0','1'],\n",
        "              yticklabels=['0','1'])\n",
        "\n",
        "  # display matrix\n",
        "  plt.ylabel('True Label',fontsize=12)\n",
        "  plt.xlabel('Predicted Label',fontsize=12)\n",
        "  plt.show()\n",
        "\n",
        "  # printing the classificati# Split the data into training/testing sets\n",
        "# def split_train_test(x, y):\n",
        "#   x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)\n",
        "#   return x_train, x_test, y_train, y_test\n",
        "\n",
        "  class_report = classification_report(y_test2, y_predict, target_names=['0', '1'])\n",
        "  print(\"\\nClassification Report:\")\n",
        "  print(class_report)"
      ],
      "metadata": {
        "id": "cB-1MOCIuv_M"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_accuracy(label, clf):\n",
        "  arr = []\n",
        "  for i in range (len(band_names)):\n",
        "    for j in range (len(channel_names)):\n",
        "      arr.append(get_accuracy(band_names[i], channel_names[j], label, clf))\n",
        "  arr = np.reshape(arr, (4,4))\n",
        "  df = pd.DataFrame(data = arr, index=band_names, columns=channel_names)\n",
        "\n",
        "  #print(\"Top 3 EEG regions with highest scores\")\n",
        "  #print(df.apply(lambda s: s.abs()).max().nlargest(3))\n",
        "  #print()\n",
        "  #print(\"Top 2 bands with highest scores\")\n",
        "  #print(df.apply(lambda s: s.abs()).max(axis=1).nlargest(2))\n",
        "  #print()\n",
        "  #print(\"EEG region with highest scores per each band\")\n",
        "  #print(df.idxmax(axis=1))\n",
        "  #print()\n",
        "  #print(\"Accuracy Scores\")\n",
        "  #print(df.idxmax())\n",
        "  #print()\n",
        "  print(df)"
      ],
      "metadata": {
        "id": "_kBo9bznkFa9"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print_accuracy('arousal', 'svm')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDIaK777kOdU",
        "outputId": "85139731-f44b-45d4-81c2-a04d6fe2f9b1"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    52.94    50.00     54.41      50.98\n",
            "alpha    53.43    50.00     50.00      50.98\n",
            "beta     52.45    50.00     55.39      50.49\n",
            "gamma    50.49    52.94     51.96      49.02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print the confusion matrix which gives the highest accuracy\n",
        "print_conf(\"theta\",\"parietal\",\"arousal\",\"svm\")"
      ],
      "metadata": {
        "id": "oY9uqQzbv46q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "a7edc616-733b-4d65-ea6a-a1889a690e4d"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x200 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.35      0.43       101\n",
            "           1       0.54      0.74      0.62       103\n",
            "\n",
            "    accuracy                           0.54       204\n",
            "   macro avg       0.55      0.54      0.52       204\n",
            "weighted avg       0.55      0.54      0.53       204\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('valence', 'svm')"
      ],
      "metadata": {
        "id": "XHUBype4mFY6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e992fd2-1df4-49b2-c299-eaf56bcbf763"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    55.39    53.92     54.41      55.88\n",
            "alpha    55.39    54.90     55.39      55.39\n",
            "beta     54.41    56.86     56.86      56.37\n",
            "gamma    54.90    58.33     56.37      58.33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_conf(\"gamma\",\"central\",\"valence\",\"svm\")"
      ],
      "metadata": {
        "id": "Bv3oLNIVGCSf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "f4514f49-4af4-4b68-f9ee-3c3bcdd91d48"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x200 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.10      0.17        90\n",
            "           1       0.58      0.96      0.72       114\n",
            "\n",
            "    accuracy                           0.58       204\n",
            "   macro avg       0.63      0.53      0.45       204\n",
            "weighted avg       0.63      0.58      0.48       204\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy(\"arousal\",\"cnn\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIn9Pm32xQL_",
        "outputId": "f67bf74e-8f8e-468f-9b48-4b423438e44a"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "32/32 [==============================] - 6s 43ms/step - loss: 0.8816 - accuracy: 0.4922 - val_loss: 0.6965 - val_accuracy: 0.4922\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7955 - accuracy: 0.5186 - val_loss: 0.7341 - val_accuracy: 0.5000\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.7985 - accuracy: 0.5078 - val_loss: 0.6932 - val_accuracy: 0.4922\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.7675 - accuracy: 0.5059 - val_loss: 0.7184 - val_accuracy: 0.4922\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.7775 - accuracy: 0.5010 - val_loss: 0.7074 - val_accuracy: 0.4922\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 0.7602 - accuracy: 0.5137 - val_loss: 0.6979 - val_accuracy: 0.4922\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 1s 18ms/step - loss: 0.7544 - accuracy: 0.5010 - val_loss: 0.7632 - val_accuracy: 0.4727\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 0.7278 - accuracy: 0.4932 - val_loss: 0.7062 - val_accuracy: 0.4648\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 1s 18ms/step - loss: 0.7480 - accuracy: 0.5068 - val_loss: 0.6991 - val_accuracy: 0.4766\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.7368 - accuracy: 0.4922 - val_loss: 0.6914 - val_accuracy: 0.4766\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7261 - accuracy: 0.4980 - val_loss: 0.7010 - val_accuracy: 0.4883\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.7333 - accuracy: 0.5176 - val_loss: 0.6971 - val_accuracy: 0.4727\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 0.7311 - accuracy: 0.4863 - val_loss: 0.7033 - val_accuracy: 0.4688\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7054 - accuracy: 0.5156 - val_loss: 0.7015 - val_accuracy: 0.4922\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7222 - accuracy: 0.5000 - val_loss: 0.6948 - val_accuracy: 0.4883\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7187 - accuracy: 0.5156 - val_loss: 0.7198 - val_accuracy: 0.4805\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7076 - accuracy: 0.5088 - val_loss: 0.6984 - val_accuracy: 0.4922\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7140 - accuracy: 0.5303 - val_loss: 0.7155 - val_accuracy: 0.4609\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7098 - accuracy: 0.5127 - val_loss: 0.7064 - val_accuracy: 0.4609\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7162 - accuracy: 0.5010 - val_loss: 0.7073 - val_accuracy: 0.4492\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.7073 - accuracy: 0.4492\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 23ms/step - loss: 0.8125 - accuracy: 0.5088 - val_loss: 0.7032 - val_accuracy: 0.4883\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7857 - accuracy: 0.4932 - val_loss: 0.6924 - val_accuracy: 0.5000\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7741 - accuracy: 0.5166 - val_loss: 0.7469 - val_accuracy: 0.4922\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7269 - accuracy: 0.5166 - val_loss: 0.7190 - val_accuracy: 0.4922\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7370 - accuracy: 0.5361 - val_loss: 0.6958 - val_accuracy: 0.4922\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7371 - accuracy: 0.5068 - val_loss: 0.7217 - val_accuracy: 0.4961\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7350 - accuracy: 0.5127 - val_loss: 0.7052 - val_accuracy: 0.4922\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7159 - accuracy: 0.5215 - val_loss: 0.7041 - val_accuracy: 0.5039\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 0.7238 - accuracy: 0.5195 - val_loss: 0.6948 - val_accuracy: 0.4727\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 0.7282 - accuracy: 0.4951 - val_loss: 0.7001 - val_accuracy: 0.4648\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.7364 - accuracy: 0.4814 - val_loss: 0.6950 - val_accuracy: 0.5000\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.7236 - accuracy: 0.4980 - val_loss: 0.6977 - val_accuracy: 0.4766\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.7172 - accuracy: 0.4990 - val_loss: 0.6954 - val_accuracy: 0.5234\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.7112 - accuracy: 0.5166 - val_loss: 0.6940 - val_accuracy: 0.5586\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.7161 - accuracy: 0.5166 - val_loss: 0.6950 - val_accuracy: 0.5547\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7100 - accuracy: 0.5215 - val_loss: 0.6947 - val_accuracy: 0.5352\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 0.7100 - accuracy: 0.5059 - val_loss: 0.6949 - val_accuracy: 0.5859\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 1s 16ms/step - loss: 0.7002 - accuracy: 0.5283 - val_loss: 0.6959 - val_accuracy: 0.5195\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.6937 - accuracy: 0.5371 - val_loss: 0.6964 - val_accuracy: 0.5000\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.6982 - accuracy: 0.5146 - val_loss: 0.6979 - val_accuracy: 0.5000\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6979 - accuracy: 0.5000\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 26ms/step - loss: 0.8436 - accuracy: 0.4980 - val_loss: 0.6894 - val_accuracy: 0.5273\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7454 - accuracy: 0.5205 - val_loss: 0.7056 - val_accuracy: 0.5117\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7638 - accuracy: 0.5361 - val_loss: 0.6849 - val_accuracy: 0.5430\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7443 - accuracy: 0.5107 - val_loss: 0.6833 - val_accuracy: 0.5508\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7324 - accuracy: 0.5098 - val_loss: 0.7013 - val_accuracy: 0.5156\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7302 - accuracy: 0.5332 - val_loss: 0.6867 - val_accuracy: 0.5312\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7171 - accuracy: 0.5156 - val_loss: 0.6846 - val_accuracy: 0.5352\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7179 - accuracy: 0.5215 - val_loss: 0.6873 - val_accuracy: 0.5508\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7196 - accuracy: 0.5195 - val_loss: 0.7146 - val_accuracy: 0.5430\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7214 - accuracy: 0.5254 - val_loss: 0.6750 - val_accuracy: 0.5664\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7161 - accuracy: 0.5049 - val_loss: 0.6818 - val_accuracy: 0.5547\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7071 - accuracy: 0.5508 - val_loss: 0.6747 - val_accuracy: 0.5508\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7031 - accuracy: 0.5234 - val_loss: 0.6766 - val_accuracy: 0.5664\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7073 - accuracy: 0.5234 - val_loss: 0.6891 - val_accuracy: 0.5352\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7033 - accuracy: 0.5381 - val_loss: 0.6849 - val_accuracy: 0.5664\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7107 - accuracy: 0.5225 - val_loss: 0.6919 - val_accuracy: 0.5391\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7203 - accuracy: 0.5098 - val_loss: 0.6869 - val_accuracy: 0.5586\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7088 - accuracy: 0.5361 - val_loss: 0.7055 - val_accuracy: 0.5352\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7048 - accuracy: 0.5107 - val_loss: 0.6979 - val_accuracy: 0.5430\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7038 - accuracy: 0.5225 - val_loss: 0.6883 - val_accuracy: 0.5547\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6883 - accuracy: 0.5547\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 26ms/step - loss: 0.8660 - accuracy: 0.5088 - val_loss: 0.6884 - val_accuracy: 0.5469\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7997 - accuracy: 0.4922 - val_loss: 0.6805 - val_accuracy: 0.5547\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7848 - accuracy: 0.5215 - val_loss: 0.6811 - val_accuracy: 0.5430\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7800 - accuracy: 0.5283 - val_loss: 0.6819 - val_accuracy: 0.5469\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7850 - accuracy: 0.4600 - val_loss: 0.6815 - val_accuracy: 0.5547\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7495 - accuracy: 0.4971 - val_loss: 0.6942 - val_accuracy: 0.5039\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7341 - accuracy: 0.4980 - val_loss: 0.6869 - val_accuracy: 0.5273\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7242 - accuracy: 0.5068 - val_loss: 0.6950 - val_accuracy: 0.5195\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7388 - accuracy: 0.4902 - val_loss: 0.7006 - val_accuracy: 0.5078\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7354 - accuracy: 0.4893 - val_loss: 0.6882 - val_accuracy: 0.5156\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7197 - accuracy: 0.5254 - val_loss: 0.6928 - val_accuracy: 0.5195\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7309 - accuracy: 0.4756 - val_loss: 0.6931 - val_accuracy: 0.5156\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7220 - accuracy: 0.4873 - val_loss: 0.6979 - val_accuracy: 0.5000\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7220 - accuracy: 0.5107 - val_loss: 0.6861 - val_accuracy: 0.5664\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7135 - accuracy: 0.5146 - val_loss: 0.6865 - val_accuracy: 0.5703\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7234 - accuracy: 0.4980 - val_loss: 0.6956 - val_accuracy: 0.4570\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7092 - accuracy: 0.5156 - val_loss: 0.6922 - val_accuracy: 0.4648\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7045 - accuracy: 0.5254 - val_loss: 0.6913 - val_accuracy: 0.5391\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7074 - accuracy: 0.5098 - val_loss: 0.6880 - val_accuracy: 0.5117\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7086 - accuracy: 0.5244 - val_loss: 0.6913 - val_accuracy: 0.5391\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6913 - accuracy: 0.5391\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 14ms/step - loss: 0.8864 - accuracy: 0.5029 - val_loss: 0.6941 - val_accuracy: 0.5039\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.8142 - accuracy: 0.5068 - val_loss: 0.7127 - val_accuracy: 0.4844\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7786 - accuracy: 0.5215 - val_loss: 0.7030 - val_accuracy: 0.5391\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7695 - accuracy: 0.5176 - val_loss: 0.6898 - val_accuracy: 0.4961\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7558 - accuracy: 0.4766 - val_loss: 0.6970 - val_accuracy: 0.5273\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7355 - accuracy: 0.5225 - val_loss: 0.7163 - val_accuracy: 0.5234\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7513 - accuracy: 0.4971 - val_loss: 0.7319 - val_accuracy: 0.5117\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7676 - accuracy: 0.4824 - val_loss: 0.6971 - val_accuracy: 0.5117\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7255 - accuracy: 0.5049 - val_loss: 0.7041 - val_accuracy: 0.5117\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7318 - accuracy: 0.4980 - val_loss: 0.7497 - val_accuracy: 0.5156\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7275 - accuracy: 0.5107 - val_loss: 0.6973 - val_accuracy: 0.5078\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7188 - accuracy: 0.4922 - val_loss: 0.7006 - val_accuracy: 0.5000\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7039 - accuracy: 0.5195 - val_loss: 0.7231 - val_accuracy: 0.5195\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7228 - accuracy: 0.4941 - val_loss: 0.6952 - val_accuracy: 0.5195\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7329 - accuracy: 0.4736 - val_loss: 0.7104 - val_accuracy: 0.5117\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7285 - accuracy: 0.4961 - val_loss: 0.6995 - val_accuracy: 0.5039\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7247 - accuracy: 0.4766 - val_loss: 0.6996 - val_accuracy: 0.4922\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7051 - accuracy: 0.4844 - val_loss: 0.6990 - val_accuracy: 0.4766\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7147 - accuracy: 0.4951 - val_loss: 0.6973 - val_accuracy: 0.4180\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7042 - accuracy: 0.5039 - val_loss: 0.7020 - val_accuracy: 0.4805\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.7020 - accuracy: 0.4805\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 13ms/step - loss: 0.8543 - accuracy: 0.5029 - val_loss: 0.6870 - val_accuracy: 0.5547\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.8003 - accuracy: 0.5127 - val_loss: 0.6919 - val_accuracy: 0.5391\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7686 - accuracy: 0.5273 - val_loss: 0.6938 - val_accuracy: 0.5508\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7465 - accuracy: 0.5244 - val_loss: 0.6946 - val_accuracy: 0.4805\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7434 - accuracy: 0.5146 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7457 - accuracy: 0.5166 - val_loss: 0.6969 - val_accuracy: 0.5352\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7363 - accuracy: 0.4941 - val_loss: 0.7314 - val_accuracy: 0.4844\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7526 - accuracy: 0.4707 - val_loss: 0.6967 - val_accuracy: 0.5195\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7217 - accuracy: 0.5010 - val_loss: 0.6994 - val_accuracy: 0.5234\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7312 - accuracy: 0.4961 - val_loss: 0.6926 - val_accuracy: 0.4961\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7262 - accuracy: 0.4951 - val_loss: 0.6940 - val_accuracy: 0.5156\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7208 - accuracy: 0.5176 - val_loss: 0.7647 - val_accuracy: 0.4961\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7263 - accuracy: 0.5049 - val_loss: 0.7102 - val_accuracy: 0.4453\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7194 - accuracy: 0.4785 - val_loss: 0.6981 - val_accuracy: 0.5742\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7166 - accuracy: 0.4805 - val_loss: 0.6960 - val_accuracy: 0.5391\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7072 - accuracy: 0.5195 - val_loss: 0.6953 - val_accuracy: 0.5117\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7133 - accuracy: 0.4883 - val_loss: 0.6896 - val_accuracy: 0.5469\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7057 - accuracy: 0.5107 - val_loss: 0.6926 - val_accuracy: 0.5469\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7176 - accuracy: 0.5068 - val_loss: 0.6952 - val_accuracy: 0.5312\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7132 - accuracy: 0.4912 - val_loss: 0.7006 - val_accuracy: 0.5117\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.7006 - accuracy: 0.5117\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 13ms/step - loss: 0.9371 - accuracy: 0.4834 - val_loss: 1.0353 - val_accuracy: 0.4961\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.8081 - accuracy: 0.5283 - val_loss: 0.7613 - val_accuracy: 0.4922\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7681 - accuracy: 0.4990 - val_loss: 0.7074 - val_accuracy: 0.5000\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7590 - accuracy: 0.5146 - val_loss: 0.7092 - val_accuracy: 0.5078\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7479 - accuracy: 0.5039 - val_loss: 0.6980 - val_accuracy: 0.5039\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7442 - accuracy: 0.5098 - val_loss: 0.7012 - val_accuracy: 0.5117\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7358 - accuracy: 0.5029 - val_loss: 0.6904 - val_accuracy: 0.5078\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7233 - accuracy: 0.5078 - val_loss: 0.6996 - val_accuracy: 0.4922\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7157 - accuracy: 0.5244 - val_loss: 0.6918 - val_accuracy: 0.5156\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7167 - accuracy: 0.5029 - val_loss: 0.7041 - val_accuracy: 0.4961\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7191 - accuracy: 0.5098 - val_loss: 0.7025 - val_accuracy: 0.4961\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7206 - accuracy: 0.5166 - val_loss: 0.7010 - val_accuracy: 0.4883\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7134 - accuracy: 0.5166 - val_loss: 0.7004 - val_accuracy: 0.4922\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7091 - accuracy: 0.5205 - val_loss: 0.6972 - val_accuracy: 0.5273\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7273 - accuracy: 0.4873 - val_loss: 0.7104 - val_accuracy: 0.5273\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7078 - accuracy: 0.5166 - val_loss: 0.7001 - val_accuracy: 0.5391\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7147 - accuracy: 0.5059 - val_loss: 0.6950 - val_accuracy: 0.5078\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7034 - accuracy: 0.5146 - val_loss: 0.6908 - val_accuracy: 0.5508\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7133 - accuracy: 0.5234 - val_loss: 0.7036 - val_accuracy: 0.5273\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7050 - accuracy: 0.5283 - val_loss: 0.6948 - val_accuracy: 0.5352\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6948 - accuracy: 0.5352\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 14ms/step - loss: 0.8347 - accuracy: 0.5049 - val_loss: 0.7009 - val_accuracy: 0.4844\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.8011 - accuracy: 0.5020 - val_loss: 0.7184 - val_accuracy: 0.4961\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7503 - accuracy: 0.5049 - val_loss: 0.7088 - val_accuracy: 0.4883\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7894 - accuracy: 0.4873 - val_loss: 0.6909 - val_accuracy: 0.5469\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7525 - accuracy: 0.4971 - val_loss: 0.6937 - val_accuracy: 0.5117\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7337 - accuracy: 0.5195 - val_loss: 0.6885 - val_accuracy: 0.5391\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7370 - accuracy: 0.5234 - val_loss: 0.6859 - val_accuracy: 0.5352\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7284 - accuracy: 0.5312 - val_loss: 0.6927 - val_accuracy: 0.5156\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7302 - accuracy: 0.4971 - val_loss: 0.7064 - val_accuracy: 0.5000\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7374 - accuracy: 0.5068 - val_loss: 0.6967 - val_accuracy: 0.5000\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7310 - accuracy: 0.5107 - val_loss: 0.6880 - val_accuracy: 0.5117\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7337 - accuracy: 0.5176 - val_loss: 0.6976 - val_accuracy: 0.5039\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7228 - accuracy: 0.5000 - val_loss: 0.6881 - val_accuracy: 0.5234\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7320 - accuracy: 0.5029 - val_loss: 0.6937 - val_accuracy: 0.5156\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7106 - accuracy: 0.5117 - val_loss: 0.7073 - val_accuracy: 0.5273\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7189 - accuracy: 0.5273 - val_loss: 0.7078 - val_accuracy: 0.5117\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7216 - accuracy: 0.5059 - val_loss: 0.6965 - val_accuracy: 0.4961\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7165 - accuracy: 0.5293 - val_loss: 0.6946 - val_accuracy: 0.5078\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7039 - accuracy: 0.5068 - val_loss: 0.6926 - val_accuracy: 0.5156\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7109 - accuracy: 0.5293 - val_loss: 0.6954 - val_accuracy: 0.5430\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6954 - accuracy: 0.5430\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 13ms/step - loss: 0.8690 - accuracy: 0.4844 - val_loss: 0.8628 - val_accuracy: 0.5039\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7680 - accuracy: 0.5166 - val_loss: 0.7195 - val_accuracy: 0.4922\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7741 - accuracy: 0.4902 - val_loss: 0.6910 - val_accuracy: 0.4922\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7390 - accuracy: 0.5283 - val_loss: 0.6924 - val_accuracy: 0.5508\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7496 - accuracy: 0.5059 - val_loss: 0.7019 - val_accuracy: 0.5312\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7425 - accuracy: 0.5176 - val_loss: 0.6876 - val_accuracy: 0.5352\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7441 - accuracy: 0.4951 - val_loss: 0.6980 - val_accuracy: 0.5312\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7359 - accuracy: 0.4863 - val_loss: 0.6987 - val_accuracy: 0.5234\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7192 - accuracy: 0.4990 - val_loss: 0.7071 - val_accuracy: 0.5195\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7249 - accuracy: 0.5098 - val_loss: 0.7241 - val_accuracy: 0.5391\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7068 - accuracy: 0.5225 - val_loss: 0.7035 - val_accuracy: 0.5195\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7119 - accuracy: 0.4893 - val_loss: 0.6948 - val_accuracy: 0.5586\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7121 - accuracy: 0.5244 - val_loss: 0.6889 - val_accuracy: 0.5156\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6997 - accuracy: 0.5264 - val_loss: 0.6938 - val_accuracy: 0.5859\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7223 - accuracy: 0.4873 - val_loss: 0.6941 - val_accuracy: 0.5195\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7177 - accuracy: 0.5312 - val_loss: 0.6863 - val_accuracy: 0.6055\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7137 - accuracy: 0.4922 - val_loss: 0.6864 - val_accuracy: 0.5664\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7107 - accuracy: 0.5000 - val_loss: 0.6877 - val_accuracy: 0.5547\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7079 - accuracy: 0.5195 - val_loss: 0.6843 - val_accuracy: 0.5469\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7065 - accuracy: 0.5029 - val_loss: 0.6922 - val_accuracy: 0.5742\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.6922 - accuracy: 0.5742\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 4s 14ms/step - loss: 0.8533 - accuracy: 0.5049 - val_loss: 0.7189 - val_accuracy: 0.5078\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7879 - accuracy: 0.4941 - val_loss: 0.6901 - val_accuracy: 0.5430\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7664 - accuracy: 0.5020 - val_loss: 0.6880 - val_accuracy: 0.5547\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7500 - accuracy: 0.5234 - val_loss: 0.6867 - val_accuracy: 0.5430\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7565 - accuracy: 0.4922 - val_loss: 0.6928 - val_accuracy: 0.5664\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7304 - accuracy: 0.5186 - val_loss: 0.6897 - val_accuracy: 0.5859\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7318 - accuracy: 0.5293 - val_loss: 0.7008 - val_accuracy: 0.5664\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7174 - accuracy: 0.5322 - val_loss: 0.6841 - val_accuracy: 0.5898\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7238 - accuracy: 0.5225 - val_loss: 0.6943 - val_accuracy: 0.5664\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7196 - accuracy: 0.5029 - val_loss: 0.6894 - val_accuracy: 0.5273\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7214 - accuracy: 0.5205 - val_loss: 0.6860 - val_accuracy: 0.5625\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7233 - accuracy: 0.4980 - val_loss: 0.6887 - val_accuracy: 0.5469\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7062 - accuracy: 0.5469 - val_loss: 0.6883 - val_accuracy: 0.5234\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7167 - accuracy: 0.5088 - val_loss: 0.6896 - val_accuracy: 0.5430\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7024 - accuracy: 0.5293 - val_loss: 0.6918 - val_accuracy: 0.5469\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 1s 19ms/step - loss: 0.7053 - accuracy: 0.5146 - val_loss: 0.6935 - val_accuracy: 0.5469\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 0.7056 - accuracy: 0.5352 - val_loss: 0.6864 - val_accuracy: 0.5508\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6993 - accuracy: 0.4990 - val_loss: 0.6906 - val_accuracy: 0.5703\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7012 - accuracy: 0.5020 - val_loss: 0.6900 - val_accuracy: 0.5352\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7022 - accuracy: 0.5127 - val_loss: 0.6883 - val_accuracy: 0.5469\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6883 - accuracy: 0.5469\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 14ms/step - loss: 0.8799 - accuracy: 0.5146 - val_loss: 0.6929 - val_accuracy: 0.4922\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.8265 - accuracy: 0.5010 - val_loss: 0.6950 - val_accuracy: 0.4961\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7595 - accuracy: 0.5078 - val_loss: 0.7030 - val_accuracy: 0.4922\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7695 - accuracy: 0.5039 - val_loss: 0.7126 - val_accuracy: 0.5156\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7501 - accuracy: 0.5176 - val_loss: 0.6889 - val_accuracy: 0.5391\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7512 - accuracy: 0.5205 - val_loss: 0.6938 - val_accuracy: 0.5312\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7340 - accuracy: 0.5225 - val_loss: 0.6887 - val_accuracy: 0.5312\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7536 - accuracy: 0.4873 - val_loss: 0.6910 - val_accuracy: 0.5469\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7233 - accuracy: 0.5322 - val_loss: 0.7015 - val_accuracy: 0.5078\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7283 - accuracy: 0.5068 - val_loss: 0.7132 - val_accuracy: 0.5078\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7195 - accuracy: 0.5186 - val_loss: 0.6882 - val_accuracy: 0.5508\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7043 - accuracy: 0.5342 - val_loss: 0.6980 - val_accuracy: 0.5078\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7323 - accuracy: 0.5039 - val_loss: 0.6785 - val_accuracy: 0.5781\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7330 - accuracy: 0.5225 - val_loss: 0.6944 - val_accuracy: 0.5117\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7110 - accuracy: 0.5078 - val_loss: 0.6942 - val_accuracy: 0.5195\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7255 - accuracy: 0.4941 - val_loss: 0.6892 - val_accuracy: 0.5430\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7115 - accuracy: 0.5000 - val_loss: 0.6970 - val_accuracy: 0.5273\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7132 - accuracy: 0.4990 - val_loss: 0.6931 - val_accuracy: 0.5234\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7197 - accuracy: 0.5088 - val_loss: 0.6834 - val_accuracy: 0.5273\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7129 - accuracy: 0.4785 - val_loss: 0.6841 - val_accuracy: 0.5430\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6841 - accuracy: 0.5430\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 15ms/step - loss: 0.8955 - accuracy: 0.4961 - val_loss: 0.7160 - val_accuracy: 0.5078\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.8520 - accuracy: 0.5039 - val_loss: 0.7005 - val_accuracy: 0.5078\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.8205 - accuracy: 0.4785 - val_loss: 0.7473 - val_accuracy: 0.5078\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.8030 - accuracy: 0.4980 - val_loss: 0.6814 - val_accuracy: 0.5664\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7575 - accuracy: 0.5244 - val_loss: 0.6824 - val_accuracy: 0.5547\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7535 - accuracy: 0.5342 - val_loss: 0.6936 - val_accuracy: 0.5234\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7528 - accuracy: 0.5352 - val_loss: 0.7840 - val_accuracy: 0.5195\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7900 - accuracy: 0.5293 - val_loss: 0.6920 - val_accuracy: 0.5195\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7647 - accuracy: 0.5234 - val_loss: 0.7409 - val_accuracy: 0.5391\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7500 - accuracy: 0.5195 - val_loss: 0.7056 - val_accuracy: 0.5078\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7413 - accuracy: 0.5518 - val_loss: 0.6857 - val_accuracy: 0.5430\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7187 - accuracy: 0.5537 - val_loss: 0.6929 - val_accuracy: 0.5000\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7281 - accuracy: 0.5244 - val_loss: 0.7076 - val_accuracy: 0.5078\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7305 - accuracy: 0.5420 - val_loss: 0.6985 - val_accuracy: 0.5078\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7147 - accuracy: 0.5479 - val_loss: 0.7062 - val_accuracy: 0.5273\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7150 - accuracy: 0.5146 - val_loss: 0.6914 - val_accuracy: 0.5586\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7152 - accuracy: 0.5244 - val_loss: 0.6892 - val_accuracy: 0.5469\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7072 - accuracy: 0.5332 - val_loss: 0.7025 - val_accuracy: 0.4961\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7097 - accuracy: 0.5293 - val_loss: 0.7093 - val_accuracy: 0.5352\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7218 - accuracy: 0.5225 - val_loss: 0.7043 - val_accuracy: 0.5117\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.7043 - accuracy: 0.5117\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 14ms/step - loss: 0.8385 - accuracy: 0.5215 - val_loss: 0.6933 - val_accuracy: 0.4922\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7738 - accuracy: 0.5137 - val_loss: 0.7078 - val_accuracy: 0.5391\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7616 - accuracy: 0.4775 - val_loss: 0.6978 - val_accuracy: 0.4961\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7865 - accuracy: 0.4805 - val_loss: 0.6918 - val_accuracy: 0.5078\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7486 - accuracy: 0.4990 - val_loss: 0.7009 - val_accuracy: 0.5156\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7496 - accuracy: 0.5088 - val_loss: 0.6935 - val_accuracy: 0.4688\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7211 - accuracy: 0.5215 - val_loss: 0.6935 - val_accuracy: 0.4805\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7419 - accuracy: 0.4834 - val_loss: 0.6950 - val_accuracy: 0.4766\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7400 - accuracy: 0.4912 - val_loss: 0.6900 - val_accuracy: 0.5117\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7286 - accuracy: 0.5039 - val_loss: 0.6948 - val_accuracy: 0.5039\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7295 - accuracy: 0.4941 - val_loss: 0.7029 - val_accuracy: 0.5078\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7224 - accuracy: 0.5049 - val_loss: 0.6977 - val_accuracy: 0.5195\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7203 - accuracy: 0.5029 - val_loss: 0.6946 - val_accuracy: 0.5195\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7295 - accuracy: 0.4990 - val_loss: 0.6994 - val_accuracy: 0.5195\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7124 - accuracy: 0.5068 - val_loss: 0.7013 - val_accuracy: 0.5117\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6984 - accuracy: 0.5244 - val_loss: 0.6980 - val_accuracy: 0.5195\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7154 - accuracy: 0.5186 - val_loss: 0.6949 - val_accuracy: 0.5156\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6989 - accuracy: 0.5420 - val_loss: 0.6932 - val_accuracy: 0.5078\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7050 - accuracy: 0.5049 - val_loss: 0.6976 - val_accuracy: 0.4883\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7019 - accuracy: 0.5215 - val_loss: 0.7043 - val_accuracy: 0.4922\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.7043 - accuracy: 0.4922\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 14ms/step - loss: 0.8070 - accuracy: 0.5176 - val_loss: 0.6990 - val_accuracy: 0.5273\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7947 - accuracy: 0.5059 - val_loss: 0.6961 - val_accuracy: 0.5352\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7563 - accuracy: 0.5088 - val_loss: 0.6955 - val_accuracy: 0.5469\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7285 - accuracy: 0.5322 - val_loss: 0.7014 - val_accuracy: 0.5391\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7307 - accuracy: 0.5068 - val_loss: 0.6993 - val_accuracy: 0.5352\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7351 - accuracy: 0.4951 - val_loss: 0.6933 - val_accuracy: 0.5391\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7302 - accuracy: 0.5039 - val_loss: 0.6923 - val_accuracy: 0.5312\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7295 - accuracy: 0.5303 - val_loss: 0.7081 - val_accuracy: 0.5000\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7395 - accuracy: 0.4863 - val_loss: 0.6967 - val_accuracy: 0.5234\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7348 - accuracy: 0.5137 - val_loss: 0.7061 - val_accuracy: 0.5078\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7296 - accuracy: 0.5098 - val_loss: 0.6967 - val_accuracy: 0.5156\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7248 - accuracy: 0.5020 - val_loss: 0.7082 - val_accuracy: 0.4961\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7270 - accuracy: 0.5225 - val_loss: 0.6990 - val_accuracy: 0.5469\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7168 - accuracy: 0.5342 - val_loss: 0.7445 - val_accuracy: 0.4609\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7245 - accuracy: 0.5146 - val_loss: 0.7032 - val_accuracy: 0.5234\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7205 - accuracy: 0.4961 - val_loss: 0.7073 - val_accuracy: 0.5078\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7104 - accuracy: 0.5000 - val_loss: 0.7044 - val_accuracy: 0.5000\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7187 - accuracy: 0.5078 - val_loss: 0.6935 - val_accuracy: 0.5234\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6966 - accuracy: 0.5186 - val_loss: 0.7031 - val_accuracy: 0.5078\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7008 - accuracy: 0.5283 - val_loss: 0.6975 - val_accuracy: 0.5195\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6975 - accuracy: 0.5195\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 18ms/step - loss: 0.9892 - accuracy: 0.4717 - val_loss: 0.7240 - val_accuracy: 0.5430\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.8406 - accuracy: 0.4971 - val_loss: 0.6943 - val_accuracy: 0.5430\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7857 - accuracy: 0.4951 - val_loss: 0.6886 - val_accuracy: 0.5352\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7849 - accuracy: 0.5234 - val_loss: 0.6891 - val_accuracy: 0.5156\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7769 - accuracy: 0.4971 - val_loss: 0.7135 - val_accuracy: 0.5469\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7671 - accuracy: 0.5205 - val_loss: 0.7186 - val_accuracy: 0.5430\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7607 - accuracy: 0.5098 - val_loss: 0.7012 - val_accuracy: 0.5156\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7334 - accuracy: 0.5186 - val_loss: 0.7066 - val_accuracy: 0.5156\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7551 - accuracy: 0.5195 - val_loss: 0.6892 - val_accuracy: 0.5352\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7246 - accuracy: 0.5146 - val_loss: 0.7043 - val_accuracy: 0.5430\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7248 - accuracy: 0.5068 - val_loss: 0.6957 - val_accuracy: 0.5586\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7470 - accuracy: 0.4873 - val_loss: 0.7133 - val_accuracy: 0.5156\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7395 - accuracy: 0.5039 - val_loss: 0.7088 - val_accuracy: 0.5195\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7238 - accuracy: 0.5264 - val_loss: 0.7001 - val_accuracy: 0.5312\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7077 - accuracy: 0.5156 - val_loss: 0.7034 - val_accuracy: 0.5312\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7252 - accuracy: 0.5137 - val_loss: 0.6889 - val_accuracy: 0.5703\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7163 - accuracy: 0.5049 - val_loss: 0.7003 - val_accuracy: 0.5352\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7105 - accuracy: 0.5400 - val_loss: 0.6943 - val_accuracy: 0.5391\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7137 - accuracy: 0.5283 - val_loss: 0.6972 - val_accuracy: 0.5508\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7079 - accuracy: 0.5312 - val_loss: 0.6897 - val_accuracy: 0.5430\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6897 - accuracy: 0.5430\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 14ms/step - loss: 0.8269 - accuracy: 0.5020 - val_loss: 0.6955 - val_accuracy: 0.5312\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7803 - accuracy: 0.5176 - val_loss: 0.7172 - val_accuracy: 0.5078\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7702 - accuracy: 0.5234 - val_loss: 0.6915 - val_accuracy: 0.5078\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7698 - accuracy: 0.5244 - val_loss: 0.6896 - val_accuracy: 0.5391\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7597 - accuracy: 0.5303 - val_loss: 0.6921 - val_accuracy: 0.5352\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7402 - accuracy: 0.4980 - val_loss: 0.6910 - val_accuracy: 0.5430\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7101 - accuracy: 0.5234 - val_loss: 0.6975 - val_accuracy: 0.5156\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7244 - accuracy: 0.5273 - val_loss: 0.6948 - val_accuracy: 0.5117\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7192 - accuracy: 0.5303 - val_loss: 0.7013 - val_accuracy: 0.5078\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7454 - accuracy: 0.5225 - val_loss: 0.6988 - val_accuracy: 0.5000\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7258 - accuracy: 0.5195 - val_loss: 0.7122 - val_accuracy: 0.5078\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7159 - accuracy: 0.5244 - val_loss: 0.6997 - val_accuracy: 0.5039\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7156 - accuracy: 0.5264 - val_loss: 0.6960 - val_accuracy: 0.5117\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7250 - accuracy: 0.5127 - val_loss: 0.7076 - val_accuracy: 0.5078\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7012 - accuracy: 0.5254 - val_loss: 0.7053 - val_accuracy: 0.5039\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7183 - accuracy: 0.5166 - val_loss: 0.7054 - val_accuracy: 0.5039\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7123 - accuracy: 0.5107 - val_loss: 0.7020 - val_accuracy: 0.5039\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7212 - accuracy: 0.5039 - val_loss: 0.7056 - val_accuracy: 0.5078\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7054 - accuracy: 0.5254 - val_loss: 0.6988 - val_accuracy: 0.5117\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6926 - accuracy: 0.5498 - val_loss: 0.7162 - val_accuracy: 0.5078\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.7162 - accuracy: 0.5078\n",
            "         frontal    central   parietal  occipital\n",
            "theta  44.921875  50.000000  55.468750  53.906250\n",
            "alpha  48.046875  51.171875  53.515625  54.296875\n",
            "beta   57.421875  54.687500  54.296875  51.171875\n",
            "gamma  49.218750  51.953125  54.296875  50.781250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy(\"valence\",\"cnn\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UVdNx3SxWxT",
        "outputId": "ac4f4dff-2cf8-4a7c-8767-0b0ceb7b5316"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 14ms/step - loss: 0.8868 - accuracy: 0.5049 - val_loss: 0.7863 - val_accuracy: 0.4531\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7809 - accuracy: 0.5078 - val_loss: 0.7168 - val_accuracy: 0.4805\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7392 - accuracy: 0.5293 - val_loss: 0.7170 - val_accuracy: 0.4961\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7491 - accuracy: 0.5225 - val_loss: 0.7221 - val_accuracy: 0.4531\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7515 - accuracy: 0.4805 - val_loss: 0.7312 - val_accuracy: 0.4492\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7495 - accuracy: 0.4893 - val_loss: 0.7339 - val_accuracy: 0.5000\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7336 - accuracy: 0.5068 - val_loss: 0.7358 - val_accuracy: 0.5078\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7160 - accuracy: 0.5107 - val_loss: 0.7001 - val_accuracy: 0.5117\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7226 - accuracy: 0.5244 - val_loss: 0.7084 - val_accuracy: 0.5078\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7209 - accuracy: 0.5127 - val_loss: 0.6891 - val_accuracy: 0.5234\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7264 - accuracy: 0.5195 - val_loss: 0.6876 - val_accuracy: 0.5352\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7269 - accuracy: 0.5029 - val_loss: 0.6997 - val_accuracy: 0.5117\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7005 - accuracy: 0.5352 - val_loss: 0.7200 - val_accuracy: 0.5039\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7150 - accuracy: 0.5127 - val_loss: 0.7092 - val_accuracy: 0.5078\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7115 - accuracy: 0.5059 - val_loss: 0.6981 - val_accuracy: 0.5156\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7104 - accuracy: 0.5215 - val_loss: 0.6922 - val_accuracy: 0.5117\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7200 - accuracy: 0.4990 - val_loss: 0.7024 - val_accuracy: 0.5117\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7023 - accuracy: 0.5244 - val_loss: 0.6945 - val_accuracy: 0.5234\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6986 - accuracy: 0.5293 - val_loss: 0.6943 - val_accuracy: 0.5117\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6950 - accuracy: 0.5361 - val_loss: 0.6993 - val_accuracy: 0.5078\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6993 - accuracy: 0.5078\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 13ms/step - loss: 0.8046 - accuracy: 0.5000 - val_loss: 0.7131 - val_accuracy: 0.5078\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7718 - accuracy: 0.4873 - val_loss: 0.7223 - val_accuracy: 0.4805\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7463 - accuracy: 0.5225 - val_loss: 0.6937 - val_accuracy: 0.4883\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7113 - accuracy: 0.5439 - val_loss: 0.6914 - val_accuracy: 0.4961\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7351 - accuracy: 0.5303 - val_loss: 0.6960 - val_accuracy: 0.4805\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7271 - accuracy: 0.5195 - val_loss: 0.6933 - val_accuracy: 0.5273\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7211 - accuracy: 0.5322 - val_loss: 0.6954 - val_accuracy: 0.4609\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7348 - accuracy: 0.5059 - val_loss: 0.6895 - val_accuracy: 0.5117\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7294 - accuracy: 0.5137 - val_loss: 0.6952 - val_accuracy: 0.5039\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7327 - accuracy: 0.5049 - val_loss: 0.6939 - val_accuracy: 0.5234\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7206 - accuracy: 0.5264 - val_loss: 0.6945 - val_accuracy: 0.4688\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7211 - accuracy: 0.4980 - val_loss: 0.6940 - val_accuracy: 0.4883\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7150 - accuracy: 0.5107 - val_loss: 0.6955 - val_accuracy: 0.5234\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7148 - accuracy: 0.5098 - val_loss: 0.6915 - val_accuracy: 0.5078\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7170 - accuracy: 0.5186 - val_loss: 0.6942 - val_accuracy: 0.4688\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7194 - accuracy: 0.5088 - val_loss: 0.6943 - val_accuracy: 0.5039\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6993 - accuracy: 0.5215 - val_loss: 0.6990 - val_accuracy: 0.4727\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7140 - accuracy: 0.4932 - val_loss: 0.6930 - val_accuracy: 0.5195\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6958 - accuracy: 0.5215 - val_loss: 0.6922 - val_accuracy: 0.5039\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7106 - accuracy: 0.5166 - val_loss: 0.6898 - val_accuracy: 0.5195\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.6898 - accuracy: 0.5195\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 13ms/step - loss: 0.8789 - accuracy: 0.5166 - val_loss: 0.7146 - val_accuracy: 0.5430\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7926 - accuracy: 0.4834 - val_loss: 0.7698 - val_accuracy: 0.5430\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.8000 - accuracy: 0.4902 - val_loss: 0.7149 - val_accuracy: 0.5352\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7589 - accuracy: 0.5098 - val_loss: 0.6880 - val_accuracy: 0.5273\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7506 - accuracy: 0.5293 - val_loss: 0.7028 - val_accuracy: 0.5391\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7398 - accuracy: 0.5137 - val_loss: 0.6882 - val_accuracy: 0.5195\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7261 - accuracy: 0.5264 - val_loss: 0.6884 - val_accuracy: 0.5273\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7264 - accuracy: 0.5361 - val_loss: 0.6864 - val_accuracy: 0.5547\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7278 - accuracy: 0.5283 - val_loss: 0.6898 - val_accuracy: 0.5195\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7495 - accuracy: 0.5195 - val_loss: 0.7251 - val_accuracy: 0.5234\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7218 - accuracy: 0.5137 - val_loss: 0.6877 - val_accuracy: 0.5234\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7140 - accuracy: 0.5430 - val_loss: 0.7240 - val_accuracy: 0.4961\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7156 - accuracy: 0.5137 - val_loss: 0.7009 - val_accuracy: 0.5195\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7216 - accuracy: 0.5264 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7159 - accuracy: 0.5146 - val_loss: 0.6911 - val_accuracy: 0.5156\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7073 - accuracy: 0.5381 - val_loss: 0.7023 - val_accuracy: 0.5039\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7158 - accuracy: 0.5010 - val_loss: 0.6908 - val_accuracy: 0.5156\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7183 - accuracy: 0.5195 - val_loss: 0.7024 - val_accuracy: 0.4727\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7087 - accuracy: 0.5029 - val_loss: 0.6920 - val_accuracy: 0.5352\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7160 - accuracy: 0.5078 - val_loss: 0.6936 - val_accuracy: 0.5078\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6936 - accuracy: 0.5078\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 13ms/step - loss: 0.8628 - accuracy: 0.5107 - val_loss: 0.6938 - val_accuracy: 0.5000\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.8292 - accuracy: 0.4922 - val_loss: 0.7266 - val_accuracy: 0.5000\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7835 - accuracy: 0.5000 - val_loss: 0.6984 - val_accuracy: 0.5430\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7638 - accuracy: 0.5488 - val_loss: 0.6922 - val_accuracy: 0.5195\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7779 - accuracy: 0.5205 - val_loss: 0.6921 - val_accuracy: 0.5312\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7555 - accuracy: 0.5137 - val_loss: 0.7025 - val_accuracy: 0.5430\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7532 - accuracy: 0.4971 - val_loss: 0.6922 - val_accuracy: 0.5430\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7521 - accuracy: 0.5039 - val_loss: 0.6903 - val_accuracy: 0.5430\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7509 - accuracy: 0.4863 - val_loss: 0.7154 - val_accuracy: 0.5547\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7253 - accuracy: 0.5342 - val_loss: 0.6976 - val_accuracy: 0.5547\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7194 - accuracy: 0.5215 - val_loss: 0.6953 - val_accuracy: 0.5547\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7278 - accuracy: 0.5029 - val_loss: 0.6980 - val_accuracy: 0.5391\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7158 - accuracy: 0.5088 - val_loss: 0.6892 - val_accuracy: 0.5469\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7226 - accuracy: 0.4844 - val_loss: 0.6921 - val_accuracy: 0.5195\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7096 - accuracy: 0.5215 - val_loss: 0.6891 - val_accuracy: 0.5391\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7218 - accuracy: 0.4980 - val_loss: 0.6882 - val_accuracy: 0.5391\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7055 - accuracy: 0.5215 - val_loss: 0.6938 - val_accuracy: 0.5391\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7073 - accuracy: 0.5361 - val_loss: 0.7030 - val_accuracy: 0.5469\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7091 - accuracy: 0.5059 - val_loss: 0.7016 - val_accuracy: 0.5391\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7019 - accuracy: 0.5293 - val_loss: 0.6946 - val_accuracy: 0.5430\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6946 - accuracy: 0.5430\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 14ms/step - loss: 0.8158 - accuracy: 0.5010 - val_loss: 0.7741 - val_accuracy: 0.4844\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7875 - accuracy: 0.4805 - val_loss: 0.7563 - val_accuracy: 0.5000\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7623 - accuracy: 0.5410 - val_loss: 0.7383 - val_accuracy: 0.5039\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7280 - accuracy: 0.5225 - val_loss: 0.6912 - val_accuracy: 0.5195\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7375 - accuracy: 0.5225 - val_loss: 0.7012 - val_accuracy: 0.5078\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7261 - accuracy: 0.5107 - val_loss: 0.7148 - val_accuracy: 0.5117\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7318 - accuracy: 0.5215 - val_loss: 0.6872 - val_accuracy: 0.5469\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7246 - accuracy: 0.5137 - val_loss: 0.6967 - val_accuracy: 0.5078\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7103 - accuracy: 0.5156 - val_loss: 0.7160 - val_accuracy: 0.5312\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7103 - accuracy: 0.5381 - val_loss: 0.7169 - val_accuracy: 0.5156\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7199 - accuracy: 0.5225 - val_loss: 0.7275 - val_accuracy: 0.5156\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7161 - accuracy: 0.5283 - val_loss: 0.6990 - val_accuracy: 0.5195\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7148 - accuracy: 0.5117 - val_loss: 0.7172 - val_accuracy: 0.5000\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7040 - accuracy: 0.5303 - val_loss: 0.7230 - val_accuracy: 0.5078\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7099 - accuracy: 0.5273 - val_loss: 0.6960 - val_accuracy: 0.5273\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7073 - accuracy: 0.5303 - val_loss: 0.6935 - val_accuracy: 0.5391\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7212 - accuracy: 0.4941 - val_loss: 0.7010 - val_accuracy: 0.4961\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.7155 - accuracy: 0.5186 - val_loss: 0.6918 - val_accuracy: 0.5273\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 0.7053 - accuracy: 0.5117 - val_loss: 0.7088 - val_accuracy: 0.5078\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7045 - accuracy: 0.5098 - val_loss: 0.6927 - val_accuracy: 0.5156\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6927 - accuracy: 0.5156\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 13ms/step - loss: 0.9167 - accuracy: 0.5039 - val_loss: 0.8513 - val_accuracy: 0.5273\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7897 - accuracy: 0.5176 - val_loss: 0.7241 - val_accuracy: 0.5430\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7686 - accuracy: 0.5283 - val_loss: 0.7310 - val_accuracy: 0.5586\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7560 - accuracy: 0.5205 - val_loss: 0.7125 - val_accuracy: 0.5430\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7596 - accuracy: 0.5078 - val_loss: 0.7193 - val_accuracy: 0.5430\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7550 - accuracy: 0.4980 - val_loss: 0.7038 - val_accuracy: 0.5312\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7074 - accuracy: 0.5205 - val_loss: 0.7041 - val_accuracy: 0.5469\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7299 - accuracy: 0.5264 - val_loss: 0.6987 - val_accuracy: 0.5195\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7213 - accuracy: 0.5156 - val_loss: 0.6946 - val_accuracy: 0.4961\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7285 - accuracy: 0.5088 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7090 - accuracy: 0.5400 - val_loss: 0.6956 - val_accuracy: 0.5039\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7283 - accuracy: 0.5107 - val_loss: 0.6943 - val_accuracy: 0.5000\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7263 - accuracy: 0.4951 - val_loss: 0.6931 - val_accuracy: 0.5352\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7189 - accuracy: 0.5225 - val_loss: 0.6917 - val_accuracy: 0.5234\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7091 - accuracy: 0.5137 - val_loss: 0.6917 - val_accuracy: 0.5039\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6981 - accuracy: 0.5400 - val_loss: 0.6952 - val_accuracy: 0.5039\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7180 - accuracy: 0.5273 - val_loss: 0.6967 - val_accuracy: 0.4922\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7123 - accuracy: 0.5020 - val_loss: 0.6975 - val_accuracy: 0.5078\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6970 - accuracy: 0.5332 - val_loss: 0.6977 - val_accuracy: 0.4609\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7164 - accuracy: 0.5117 - val_loss: 0.6932 - val_accuracy: 0.5078\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5078\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 14ms/step - loss: 0.8455 - accuracy: 0.4990 - val_loss: 0.6878 - val_accuracy: 0.5469\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.8025 - accuracy: 0.4668 - val_loss: 0.7223 - val_accuracy: 0.5469\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7810 - accuracy: 0.4980 - val_loss: 0.6887 - val_accuracy: 0.5469\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7368 - accuracy: 0.5361 - val_loss: 0.7042 - val_accuracy: 0.4922\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7527 - accuracy: 0.5264 - val_loss: 0.6962 - val_accuracy: 0.5117\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7317 - accuracy: 0.5029 - val_loss: 0.6871 - val_accuracy: 0.5469\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7544 - accuracy: 0.4961 - val_loss: 0.6903 - val_accuracy: 0.5273\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7178 - accuracy: 0.5508 - val_loss: 0.6902 - val_accuracy: 0.5312\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7370 - accuracy: 0.5098 - val_loss: 0.6911 - val_accuracy: 0.5312\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7329 - accuracy: 0.5049 - val_loss: 0.6904 - val_accuracy: 0.5469\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7228 - accuracy: 0.5195 - val_loss: 0.6868 - val_accuracy: 0.5469\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7071 - accuracy: 0.5264 - val_loss: 0.6897 - val_accuracy: 0.5430\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7182 - accuracy: 0.5137 - val_loss: 0.6885 - val_accuracy: 0.5469\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7231 - accuracy: 0.5137 - val_loss: 0.6889 - val_accuracy: 0.5430\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7066 - accuracy: 0.5078 - val_loss: 0.6940 - val_accuracy: 0.5000\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7055 - accuracy: 0.5391 - val_loss: 0.6930 - val_accuracy: 0.5078\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7009 - accuracy: 0.5332 - val_loss: 0.6891 - val_accuracy: 0.5391\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7025 - accuracy: 0.5400 - val_loss: 0.7092 - val_accuracy: 0.4844\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7167 - accuracy: 0.5059 - val_loss: 0.6960 - val_accuracy: 0.5117\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7065 - accuracy: 0.5068 - val_loss: 0.7123 - val_accuracy: 0.4805\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.7123 - accuracy: 0.4805\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 14ms/step - loss: 0.8293 - accuracy: 0.5156 - val_loss: 0.7386 - val_accuracy: 0.5430\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7627 - accuracy: 0.5322 - val_loss: 0.7657 - val_accuracy: 0.5234\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7891 - accuracy: 0.5225 - val_loss: 0.6940 - val_accuracy: 0.5195\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7556 - accuracy: 0.5166 - val_loss: 0.7191 - val_accuracy: 0.5273\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7439 - accuracy: 0.5186 - val_loss: 0.7486 - val_accuracy: 0.5352\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7736 - accuracy: 0.5029 - val_loss: 0.7443 - val_accuracy: 0.5234\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7557 - accuracy: 0.5283 - val_loss: 0.7483 - val_accuracy: 0.5352\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7429 - accuracy: 0.5234 - val_loss: 0.7188 - val_accuracy: 0.5352\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7334 - accuracy: 0.5176 - val_loss: 0.7026 - val_accuracy: 0.5391\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 1s 22ms/step - loss: 0.7271 - accuracy: 0.5215 - val_loss: 0.6982 - val_accuracy: 0.5234\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 1s 32ms/step - loss: 0.7278 - accuracy: 0.5195 - val_loss: 0.7111 - val_accuracy: 0.5312\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 1s 20ms/step - loss: 0.7236 - accuracy: 0.5283 - val_loss: 0.6994 - val_accuracy: 0.5547\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7264 - accuracy: 0.5107 - val_loss: 0.6911 - val_accuracy: 0.5352\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7207 - accuracy: 0.5156 - val_loss: 0.6881 - val_accuracy: 0.5312\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7174 - accuracy: 0.5156 - val_loss: 0.6891 - val_accuracy: 0.5352\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7067 - accuracy: 0.5322 - val_loss: 0.7067 - val_accuracy: 0.5469\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7075 - accuracy: 0.5049 - val_loss: 0.6999 - val_accuracy: 0.5508\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7176 - accuracy: 0.5283 - val_loss: 0.6922 - val_accuracy: 0.5547\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7181 - accuracy: 0.5156 - val_loss: 0.7037 - val_accuracy: 0.5508\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7092 - accuracy: 0.5195 - val_loss: 0.6977 - val_accuracy: 0.5312\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6977 - accuracy: 0.5312\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 14ms/step - loss: 0.8929 - accuracy: 0.5010 - val_loss: 0.6939 - val_accuracy: 0.4961\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7843 - accuracy: 0.5303 - val_loss: 0.7103 - val_accuracy: 0.4922\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7981 - accuracy: 0.5186 - val_loss: 0.7380 - val_accuracy: 0.4922\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7456 - accuracy: 0.5166 - val_loss: 0.7737 - val_accuracy: 0.4961\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7843 - accuracy: 0.4834 - val_loss: 0.7139 - val_accuracy: 0.4922\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7586 - accuracy: 0.4990 - val_loss: 0.7229 - val_accuracy: 0.4922\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7258 - accuracy: 0.5312 - val_loss: 0.7113 - val_accuracy: 0.4961\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7381 - accuracy: 0.4941 - val_loss: 0.6989 - val_accuracy: 0.4922\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7242 - accuracy: 0.5039 - val_loss: 0.6909 - val_accuracy: 0.5039\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7150 - accuracy: 0.5205 - val_loss: 0.6901 - val_accuracy: 0.5312\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7353 - accuracy: 0.4834 - val_loss: 0.7011 - val_accuracy: 0.4961\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7098 - accuracy: 0.5400 - val_loss: 0.6926 - val_accuracy: 0.4805\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7193 - accuracy: 0.5127 - val_loss: 0.6905 - val_accuracy: 0.5000\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7103 - accuracy: 0.5195 - val_loss: 0.6939 - val_accuracy: 0.4961\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7182 - accuracy: 0.5049 - val_loss: 0.6965 - val_accuracy: 0.4883\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7082 - accuracy: 0.5332 - val_loss: 0.6975 - val_accuracy: 0.4883\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7114 - accuracy: 0.5049 - val_loss: 0.7054 - val_accuracy: 0.4961\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7120 - accuracy: 0.4990 - val_loss: 0.7029 - val_accuracy: 0.4961\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7184 - accuracy: 0.5195 - val_loss: 0.7034 - val_accuracy: 0.5000\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7130 - accuracy: 0.5215 - val_loss: 0.6932 - val_accuracy: 0.4961\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.4961\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 26ms/step - loss: 0.8680 - accuracy: 0.4941 - val_loss: 0.7118 - val_accuracy: 0.4961\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7784 - accuracy: 0.5303 - val_loss: 0.6996 - val_accuracy: 0.4805\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7862 - accuracy: 0.5059 - val_loss: 0.6975 - val_accuracy: 0.4766\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7816 - accuracy: 0.5010 - val_loss: 0.6927 - val_accuracy: 0.5039\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7530 - accuracy: 0.5195 - val_loss: 0.7004 - val_accuracy: 0.5117\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7541 - accuracy: 0.5107 - val_loss: 0.6957 - val_accuracy: 0.5000\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7213 - accuracy: 0.5107 - val_loss: 0.6942 - val_accuracy: 0.5234\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7579 - accuracy: 0.5078 - val_loss: 0.6930 - val_accuracy: 0.5234\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7399 - accuracy: 0.5137 - val_loss: 0.6942 - val_accuracy: 0.5039\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7456 - accuracy: 0.5225 - val_loss: 0.7164 - val_accuracy: 0.5117\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7278 - accuracy: 0.5303 - val_loss: 0.6961 - val_accuracy: 0.5117\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7379 - accuracy: 0.5146 - val_loss: 0.6990 - val_accuracy: 0.5234\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7186 - accuracy: 0.5322 - val_loss: 0.6979 - val_accuracy: 0.5117\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7259 - accuracy: 0.5215 - val_loss: 0.7011 - val_accuracy: 0.5156\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7238 - accuracy: 0.5127 - val_loss: 0.6947 - val_accuracy: 0.5234\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7307 - accuracy: 0.5215 - val_loss: 0.6945 - val_accuracy: 0.5156\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7231 - accuracy: 0.5186 - val_loss: 0.6961 - val_accuracy: 0.5039\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7022 - accuracy: 0.5420 - val_loss: 0.7018 - val_accuracy: 0.5078\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7365 - accuracy: 0.4902 - val_loss: 0.6991 - val_accuracy: 0.5156\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7108 - accuracy: 0.5059 - val_loss: 0.7040 - val_accuracy: 0.5156\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 0.7040 - accuracy: 0.5156\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 13ms/step - loss: 0.8356 - accuracy: 0.4951 - val_loss: 0.7439 - val_accuracy: 0.4492\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7728 - accuracy: 0.4912 - val_loss: 0.7227 - val_accuracy: 0.4375\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7631 - accuracy: 0.5137 - val_loss: 0.7097 - val_accuracy: 0.4570\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7414 - accuracy: 0.5205 - val_loss: 0.7110 - val_accuracy: 0.4648\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7195 - accuracy: 0.5273 - val_loss: 0.6941 - val_accuracy: 0.5312\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7347 - accuracy: 0.5156 - val_loss: 0.7023 - val_accuracy: 0.4922\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7215 - accuracy: 0.5293 - val_loss: 0.6960 - val_accuracy: 0.4844\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7252 - accuracy: 0.5029 - val_loss: 0.6970 - val_accuracy: 0.4844\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7336 - accuracy: 0.5391 - val_loss: 0.6942 - val_accuracy: 0.4531\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7157 - accuracy: 0.5176 - val_loss: 0.7056 - val_accuracy: 0.4766\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7112 - accuracy: 0.5176 - val_loss: 0.6964 - val_accuracy: 0.4375\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7080 - accuracy: 0.5273 - val_loss: 0.7044 - val_accuracy: 0.4727\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6953 - accuracy: 0.5400 - val_loss: 0.7000 - val_accuracy: 0.4844\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7092 - accuracy: 0.5029 - val_loss: 0.7016 - val_accuracy: 0.4766\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7102 - accuracy: 0.5254 - val_loss: 0.6979 - val_accuracy: 0.4805\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7014 - accuracy: 0.5449 - val_loss: 0.6926 - val_accuracy: 0.5039\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7040 - accuracy: 0.5234 - val_loss: 0.7065 - val_accuracy: 0.4805\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7007 - accuracy: 0.5488 - val_loss: 0.6966 - val_accuracy: 0.5117\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7077 - accuracy: 0.5205 - val_loss: 0.7008 - val_accuracy: 0.4961\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6966 - accuracy: 0.5332 - val_loss: 0.6937 - val_accuracy: 0.4961\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6937 - accuracy: 0.4961\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 14ms/step - loss: 0.8488 - accuracy: 0.4951 - val_loss: 0.7653 - val_accuracy: 0.4805\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.8085 - accuracy: 0.5088 - val_loss: 0.7041 - val_accuracy: 0.4531\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7985 - accuracy: 0.4727 - val_loss: 0.7055 - val_accuracy: 0.4688\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7530 - accuracy: 0.5020 - val_loss: 0.6946 - val_accuracy: 0.5273\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7204 - accuracy: 0.5078 - val_loss: 0.7052 - val_accuracy: 0.5391\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7271 - accuracy: 0.5166 - val_loss: 0.6954 - val_accuracy: 0.5469\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7407 - accuracy: 0.5078 - val_loss: 0.6939 - val_accuracy: 0.5586\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7243 - accuracy: 0.5146 - val_loss: 0.6927 - val_accuracy: 0.5352\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7149 - accuracy: 0.5420 - val_loss: 0.6893 - val_accuracy: 0.5391\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7253 - accuracy: 0.4863 - val_loss: 0.6909 - val_accuracy: 0.5547\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7282 - accuracy: 0.4717 - val_loss: 0.6909 - val_accuracy: 0.5352\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7243 - accuracy: 0.5098 - val_loss: 0.6987 - val_accuracy: 0.5508\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7254 - accuracy: 0.5039 - val_loss: 0.6943 - val_accuracy: 0.5469\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7222 - accuracy: 0.4893 - val_loss: 0.6932 - val_accuracy: 0.5508\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7062 - accuracy: 0.5195 - val_loss: 0.6908 - val_accuracy: 0.5586\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7129 - accuracy: 0.5059 - val_loss: 0.6907 - val_accuracy: 0.5430\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7077 - accuracy: 0.5039 - val_loss: 0.6883 - val_accuracy: 0.5664\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7059 - accuracy: 0.5078 - val_loss: 0.6952 - val_accuracy: 0.4844\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7054 - accuracy: 0.4961 - val_loss: 0.6883 - val_accuracy: 0.5391\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7085 - accuracy: 0.4902 - val_loss: 0.6898 - val_accuracy: 0.5469\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6898 - accuracy: 0.5469\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 15ms/step - loss: 0.8067 - accuracy: 0.5088 - val_loss: 0.7019 - val_accuracy: 0.4688\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7634 - accuracy: 0.5254 - val_loss: 0.7026 - val_accuracy: 0.4648\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7273 - accuracy: 0.5430 - val_loss: 0.6971 - val_accuracy: 0.5078\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7499 - accuracy: 0.5029 - val_loss: 0.7030 - val_accuracy: 0.5117\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7321 - accuracy: 0.4951 - val_loss: 0.6928 - val_accuracy: 0.5234\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7228 - accuracy: 0.5254 - val_loss: 0.6929 - val_accuracy: 0.5391\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7285 - accuracy: 0.5078 - val_loss: 0.6980 - val_accuracy: 0.5273\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7236 - accuracy: 0.5244 - val_loss: 0.6978 - val_accuracy: 0.5195\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7170 - accuracy: 0.5469 - val_loss: 0.6904 - val_accuracy: 0.5312\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7283 - accuracy: 0.5010 - val_loss: 0.6918 - val_accuracy: 0.5430\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7032 - accuracy: 0.5381 - val_loss: 0.6922 - val_accuracy: 0.5156\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7177 - accuracy: 0.5225 - val_loss: 0.6976 - val_accuracy: 0.5273\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7033 - accuracy: 0.5312 - val_loss: 0.6967 - val_accuracy: 0.5234\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7135 - accuracy: 0.5088 - val_loss: 0.6903 - val_accuracy: 0.5352\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6999 - accuracy: 0.5371 - val_loss: 0.6937 - val_accuracy: 0.5273\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7014 - accuracy: 0.5303 - val_loss: 0.6941 - val_accuracy: 0.5234\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7107 - accuracy: 0.5107 - val_loss: 0.6913 - val_accuracy: 0.5352\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7044 - accuracy: 0.5322 - val_loss: 0.6933 - val_accuracy: 0.5234\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6988 - accuracy: 0.5605 - val_loss: 0.6891 - val_accuracy: 0.5352\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7059 - accuracy: 0.5264 - val_loss: 0.6910 - val_accuracy: 0.5391\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6910 - accuracy: 0.5391\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 15ms/step - loss: 0.9011 - accuracy: 0.4893 - val_loss: 0.8027 - val_accuracy: 0.4414\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7939 - accuracy: 0.4951 - val_loss: 0.7111 - val_accuracy: 0.4609\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7669 - accuracy: 0.5049 - val_loss: 0.7050 - val_accuracy: 0.4961\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7443 - accuracy: 0.5400 - val_loss: 0.7057 - val_accuracy: 0.5000\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7632 - accuracy: 0.5176 - val_loss: 0.7048 - val_accuracy: 0.5000\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7397 - accuracy: 0.5234 - val_loss: 0.6941 - val_accuracy: 0.5391\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7529 - accuracy: 0.5098 - val_loss: 0.6935 - val_accuracy: 0.5430\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7297 - accuracy: 0.5312 - val_loss: 0.6935 - val_accuracy: 0.5352\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7390 - accuracy: 0.5137 - val_loss: 0.6947 - val_accuracy: 0.5508\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7213 - accuracy: 0.4990 - val_loss: 0.6930 - val_accuracy: 0.5352\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7190 - accuracy: 0.5137 - val_loss: 0.6942 - val_accuracy: 0.5273\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7284 - accuracy: 0.4990 - val_loss: 0.6952 - val_accuracy: 0.5234\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7149 - accuracy: 0.5205 - val_loss: 0.6963 - val_accuracy: 0.4766\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7096 - accuracy: 0.5146 - val_loss: 0.6919 - val_accuracy: 0.5234\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7209 - accuracy: 0.5215 - val_loss: 0.6963 - val_accuracy: 0.5117\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7174 - accuracy: 0.5068 - val_loss: 0.6926 - val_accuracy: 0.5352\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7004 - accuracy: 0.5273 - val_loss: 0.6927 - val_accuracy: 0.5469\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7057 - accuracy: 0.5322 - val_loss: 0.6966 - val_accuracy: 0.5391\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7102 - accuracy: 0.5088 - val_loss: 0.6962 - val_accuracy: 0.5625\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7132 - accuracy: 0.5059 - val_loss: 0.6978 - val_accuracy: 0.5273\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6978 - accuracy: 0.5273\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 20ms/step - loss: 0.8939 - accuracy: 0.5000 - val_loss: 0.8074 - val_accuracy: 0.5469\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.8069 - accuracy: 0.5166 - val_loss: 0.7131 - val_accuracy: 0.5117\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.8079 - accuracy: 0.4824 - val_loss: 0.7226 - val_accuracy: 0.5469\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7579 - accuracy: 0.5020 - val_loss: 0.6975 - val_accuracy: 0.5469\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7885 - accuracy: 0.4902 - val_loss: 0.6957 - val_accuracy: 0.5469\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7813 - accuracy: 0.4922 - val_loss: 0.6869 - val_accuracy: 0.5469\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7744 - accuracy: 0.4951 - val_loss: 0.6890 - val_accuracy: 0.5469\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7492 - accuracy: 0.5078 - val_loss: 0.7066 - val_accuracy: 0.5000\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7530 - accuracy: 0.5088 - val_loss: 0.6879 - val_accuracy: 0.5547\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7458 - accuracy: 0.4854 - val_loss: 0.7070 - val_accuracy: 0.4805\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7579 - accuracy: 0.4697 - val_loss: 0.6929 - val_accuracy: 0.5273\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7353 - accuracy: 0.5010 - val_loss: 0.6957 - val_accuracy: 0.5234\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7474 - accuracy: 0.4922 - val_loss: 0.6950 - val_accuracy: 0.5234\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7270 - accuracy: 0.5137 - val_loss: 0.6957 - val_accuracy: 0.5195\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7287 - accuracy: 0.5127 - val_loss: 0.6953 - val_accuracy: 0.5156\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7208 - accuracy: 0.4961 - val_loss: 0.6966 - val_accuracy: 0.5234\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7207 - accuracy: 0.4922 - val_loss: 0.7005 - val_accuracy: 0.4883\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7130 - accuracy: 0.5088 - val_loss: 0.6886 - val_accuracy: 0.5469\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7051 - accuracy: 0.5303 - val_loss: 0.6970 - val_accuracy: 0.4883\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7094 - accuracy: 0.5107 - val_loss: 0.6949 - val_accuracy: 0.5039\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.6949 - accuracy: 0.5039\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 13ms/step - loss: 0.8178 - accuracy: 0.4883 - val_loss: 0.7040 - val_accuracy: 0.5469\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7711 - accuracy: 0.5098 - val_loss: 0.6915 - val_accuracy: 0.5430\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7515 - accuracy: 0.5205 - val_loss: 0.6942 - val_accuracy: 0.5664\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7414 - accuracy: 0.5059 - val_loss: 0.6891 - val_accuracy: 0.5625\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7375 - accuracy: 0.5195 - val_loss: 0.6908 - val_accuracy: 0.5469\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7445 - accuracy: 0.5146 - val_loss: 0.6923 - val_accuracy: 0.5547\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7305 - accuracy: 0.5000 - val_loss: 0.6954 - val_accuracy: 0.5547\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7166 - accuracy: 0.5283 - val_loss: 0.7003 - val_accuracy: 0.5508\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7195 - accuracy: 0.4834 - val_loss: 0.6913 - val_accuracy: 0.5391\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7151 - accuracy: 0.5186 - val_loss: 0.6913 - val_accuracy: 0.5430\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7070 - accuracy: 0.5010 - val_loss: 0.6951 - val_accuracy: 0.5508\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7156 - accuracy: 0.5146 - val_loss: 0.6905 - val_accuracy: 0.5508\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7093 - accuracy: 0.5215 - val_loss: 0.6900 - val_accuracy: 0.5586\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7167 - accuracy: 0.5088 - val_loss: 0.6878 - val_accuracy: 0.5625\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7123 - accuracy: 0.5176 - val_loss: 0.6908 - val_accuracy: 0.5508\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7228 - accuracy: 0.4746 - val_loss: 0.6893 - val_accuracy: 0.5547\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7045 - accuracy: 0.5020 - val_loss: 0.6919 - val_accuracy: 0.5508\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7066 - accuracy: 0.5117 - val_loss: 0.6941 - val_accuracy: 0.5508\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7071 - accuracy: 0.5078 - val_loss: 0.6894 - val_accuracy: 0.5469\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7036 - accuracy: 0.5371 - val_loss: 0.6915 - val_accuracy: 0.5547\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 0.6915 - accuracy: 0.5547\n",
            "         frontal    central   parietal  occipital\n",
            "theta  50.781250  51.953125  50.781250  54.296875\n",
            "alpha  51.562500  50.781250  48.046875  53.125000\n",
            "beta   49.609375  51.562500  49.609375  54.687500\n",
            "gamma  53.906250  52.734375  50.390625  55.468750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('arousal', 'ab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoKgtLNEpU-v",
        "outputId": "f0ef7f1b-5c2a-40b6-95bd-d7a56f4ebc07"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    60.78    59.31     57.84      60.29\n",
            "alpha    61.27    59.80     53.43      56.86\n",
            "beta     64.22    56.37     58.33      56.86\n",
            "gamma    60.29    55.39     57.84      65.69\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_conf(\"gamma\",\"occipital\",\"arousal\",\"ab\")"
      ],
      "metadata": {
        "id": "BcqO8QBLHR7q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "b9221dee-ad39-47ea-f033-ee8438b4b896"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x200 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.69      0.67       101\n",
            "           1       0.67      0.62      0.65       103\n",
            "\n",
            "    accuracy                           0.66       204\n",
            "   macro avg       0.66      0.66      0.66       204\n",
            "weighted avg       0.66      0.66      0.66       204\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('valence', 'ab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1za3TrVphgH",
        "outputId": "59c2f5ca-08b6-4829-e928-91fc9cc7774e"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    50.49    55.39     56.86      55.88\n",
            "alpha    53.92    56.37     57.84      54.90\n",
            "beta     56.86    56.37     56.86      57.84\n",
            "gamma    56.37    55.88     50.98      55.88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('arousal', 'xgb')"
      ],
      "metadata": {
        "id": "1DPjYGhArgcr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cf99f3d-2119-4090-b36c-d47c11053935"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    61.27    59.31     56.86      59.31\n",
            "alpha    62.75    59.80     63.24      57.84\n",
            "beta     62.25    56.86     61.76      57.35\n",
            "gamma    63.73    59.80     55.39      61.27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('valence', 'xgb')"
      ],
      "metadata": {
        "id": "wYrfmtLwrmES",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52d96d08-24b3-497c-cc38-d65e6466c5d6"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    54.90    56.37     56.86      57.84\n",
            "alpha    55.39    64.22     62.25      57.35\n",
            "beta     58.82    65.69     58.82      59.80\n",
            "gamma    57.84    58.33     55.88      59.80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_conf(\"beta\",\"central\",\"valence\",\"xgb\")"
      ],
      "metadata": {
        "id": "6_wzHU40YhWY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "8cd89d34-99d7-44cc-9aa0-a96a20f5625d"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x200 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Not Spam       0.62      0.58      0.60        90\n",
            "        Spam       0.68      0.72      0.70       114\n",
            "\n",
            "    accuracy                           0.66       204\n",
            "   macro avg       0.65      0.65      0.65       204\n",
            "weighted avg       0.65      0.66      0.66       204\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('arousal', 'knn')"
      ],
      "metadata": {
        "id": "r3qH1vs2J5rC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dd0533f-d09a-49cd-919e-6e7ba1a32680"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    62.75    60.78     60.29      62.25\n",
            "alpha    65.69    64.22     64.22      59.31\n",
            "beta     61.76    56.86     63.24      55.39\n",
            "gamma    62.25    61.27     60.29      56.37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('valence', 'knn')"
      ],
      "metadata": {
        "id": "x84JhQCIKciv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dea2d607-ca96-43a3-bdde-0420db345a49"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    57.84    58.33     51.47      58.33\n",
            "alpha    55.39    57.84     59.31      53.92\n",
            "beta     58.82    57.84     61.76      55.39\n",
            "gamma    64.22    56.86     61.76      54.41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('arousal', 'dtree')"
      ],
      "metadata": {
        "id": "pXS8oiqJKknT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "807a4b8e-521d-462b-abbd-34822b87ab6a"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    56.86    58.33     57.35      59.31\n",
            "alpha    60.29    59.31     60.29      54.90\n",
            "beta     56.86    54.41     51.47      59.80\n",
            "gamma    60.29    57.84     58.33      55.88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('valence', 'dtree')"
      ],
      "metadata": {
        "id": "zPn1ljyNKtr4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7deee551-5d60-4b88-83f3-b7f645c0bec0"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    48.04    54.90     54.41      50.00\n",
            "alpha    56.86    52.94     57.84      57.35\n",
            "beta     50.00    51.47     55.88      59.80\n",
            "gamma    59.31    57.35     57.84      54.41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('arousal', 'rf')"
      ],
      "metadata": {
        "id": "zS_EF8dRKyLm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6594fba0-5d57-445d-d6b3-ef9de8f5caed"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    59.31    60.29     58.33      63.73\n",
            "alpha    64.71    63.24     60.29      61.76\n",
            "beta     59.80    55.39     63.73      56.37\n",
            "gamma    66.18    60.78     58.82      61.76\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('valence', 'rf')"
      ],
      "metadata": {
        "id": "GNTGFHQOK14l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6369dd44-3538-49bb-f213-c166fbe51c3d"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    53.92    55.39     59.80      60.78\n",
            "alpha    57.84    62.25     62.75      58.82\n",
            "beta     59.80    61.27     63.24      62.25\n",
            "gamma    60.78    60.29     56.86      62.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('arousal', 'nb')"
      ],
      "metadata": {
        "id": "MkmagqFfK54a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9918d7ab-b7e9-4bac-eaf7-9f5bc8be0fd2"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    47.55    52.94     50.00      51.47\n",
            "alpha    49.02    52.94     50.98      51.47\n",
            "beta     48.04    50.98     54.41      47.55\n",
            "gamma    49.02    52.45     51.47      47.55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('valence', 'nb')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uV3gnYCKK-cA",
        "outputId": "c74bf097-6a13-4061-9a30-62611314f903"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    56.86    48.04     55.88      56.37\n",
            "alpha    55.39    47.55     48.53      57.35\n",
            "beta     54.90    49.51     47.55      55.39\n",
            "gamma    56.37    56.86     54.90      57.84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('arousal', 'mlp')"
      ],
      "metadata": {
        "id": "02_ynnQ1LEoz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd400f5e-a8cf-463a-e38d-800643e7919f"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    51.47    57.84     50.49      52.94\n",
            "alpha    52.94    50.49     50.49      53.43\n",
            "beta     51.47    50.00     54.41      54.90\n",
            "gamma    56.37    56.37     50.00      55.88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('valence', 'mlp')"
      ],
      "metadata": {
        "id": "tuwCpDS1LHAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3494fb6-ad85-44d0-ac5e-72f25f9d37b9"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    54.90    56.86     56.86      57.35\n",
            "alpha    53.92    56.37     53.92      57.35\n",
            "beta     54.41    55.88     55.39      56.86\n",
            "gamma    54.90    52.45     52.45      55.39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('arousal', 'lgbm')"
      ],
      "metadata": {
        "id": "tAvwS8OmOkX6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "000e2159-8efb-4671-8693-dd7a004d00c5"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 407, number of negative: 413\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000171 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496341 -> initscore=-0.014634\n",
            "[LightGBM] [Info] Start training from score -0.014634\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 407, number of negative: 413\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000090 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1785\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 7\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496341 -> initscore=-0.014634\n",
            "[LightGBM] [Info] Start training from score -0.014634\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 407, number of negative: 413\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000117 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496341 -> initscore=-0.014634\n",
            "[LightGBM] [Info] Start training from score -0.014634\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 407, number of negative: 413\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000106 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496341 -> initscore=-0.014634\n",
            "[LightGBM] [Info] Start training from score -0.014634\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 407, number of negative: 413\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000127 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496341 -> initscore=-0.014634\n",
            "[LightGBM] [Info] Start training from score -0.014634\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 407, number of negative: 413\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000137 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1785\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 7\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496341 -> initscore=-0.014634\n",
            "[LightGBM] [Info] Start training from score -0.014634\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 407, number of negative: 413\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000107 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496341 -> initscore=-0.014634\n",
            "[LightGBM] [Info] Start training from score -0.014634\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 407, number of negative: 413\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000114 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496341 -> initscore=-0.014634\n",
            "[LightGBM] [Info] Start training from score -0.014634\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 407, number of negative: 413\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000108 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496341 -> initscore=-0.014634\n",
            "[LightGBM] [Info] Start training from score -0.014634\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 407, number of negative: 413\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000131 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1785\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 7\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496341 -> initscore=-0.014634\n",
            "[LightGBM] [Info] Start training from score -0.014634\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 407, number of negative: 413\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000066 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496341 -> initscore=-0.014634\n",
            "[LightGBM] [Info] Start training from score -0.014634\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 407, number of negative: 413\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000112 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496341 -> initscore=-0.014634\n",
            "[LightGBM] [Info] Start training from score -0.014634\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 407, number of negative: 413\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000106 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496341 -> initscore=-0.014634\n",
            "[LightGBM] [Info] Start training from score -0.014634\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 407, number of negative: 413\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000143 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1785\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 7\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496341 -> initscore=-0.014634\n",
            "[LightGBM] [Info] Start training from score -0.014634\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 407, number of negative: 413\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000142 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496341 -> initscore=-0.014634\n",
            "[LightGBM] [Info] Start training from score -0.014634\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 407, number of negative: 413\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000071 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496341 -> initscore=-0.014634\n",
            "[LightGBM] [Info] Start training from score -0.014634\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "       frontal  central  parietal  occipital\n",
            "theta    60.29    60.78     54.41      60.78\n",
            "alpha    63.24    60.29     62.25      60.29\n",
            "beta     61.27    57.35     57.84      58.33\n",
            "gamma    59.31    61.76     58.82      62.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('valence', 'lgbm')"
      ],
      "metadata": {
        "id": "hIwlFJ7nOyFX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83c5f86b-c1b4-4b5a-c1be-1c19885da7aa"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 426, number of negative: 394\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000165 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.519512 -> initscore=0.078088\n",
            "[LightGBM] [Info] Start training from score 0.078088\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 426, number of negative: 394\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000136 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1785\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 7\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.519512 -> initscore=0.078088\n",
            "[LightGBM] [Info] Start training from score 0.078088\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 426, number of negative: 394\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000104 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.519512 -> initscore=0.078088\n",
            "[LightGBM] [Info] Start training from score 0.078088\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 426, number of negative: 394\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000103 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.519512 -> initscore=0.078088\n",
            "[LightGBM] [Info] Start training from score 0.078088\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 426, number of negative: 394\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000106 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.519512 -> initscore=0.078088\n",
            "[LightGBM] [Info] Start training from score 0.078088\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 426, number of negative: 394\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000155 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1785\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 7\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.519512 -> initscore=0.078088\n",
            "[LightGBM] [Info] Start training from score 0.078088\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 426, number of negative: 394\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000129 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.519512 -> initscore=0.078088\n",
            "[LightGBM] [Info] Start training from score 0.078088\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 426, number of negative: 394\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000122 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.519512 -> initscore=0.078088\n",
            "[LightGBM] [Info] Start training from score 0.078088\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 426, number of negative: 394\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000108 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.519512 -> initscore=0.078088\n",
            "[LightGBM] [Info] Start training from score 0.078088\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 426, number of negative: 394\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000144 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1785\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 7\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.519512 -> initscore=0.078088\n",
            "[LightGBM] [Info] Start training from score 0.078088\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 426, number of negative: 394\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000114 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.519512 -> initscore=0.078088\n",
            "[LightGBM] [Info] Start training from score 0.078088\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 426, number of negative: 394\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000116 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.519512 -> initscore=0.078088\n",
            "[LightGBM] [Info] Start training from score 0.078088\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 426, number of negative: 394\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000120 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.519512 -> initscore=0.078088\n",
            "[LightGBM] [Info] Start training from score 0.078088\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 426, number of negative: 394\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000154 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1785\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 7\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.519512 -> initscore=0.078088\n",
            "[LightGBM] [Info] Start training from score 0.078088\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 426, number of negative: 394\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000122 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.519512 -> initscore=0.078088\n",
            "[LightGBM] [Info] Start training from score 0.078088\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 426, number of negative: 394\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000108 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.519512 -> initscore=0.078088\n",
            "[LightGBM] [Info] Start training from score 0.078088\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "       frontal  central  parietal  occipital\n",
            "theta    55.88    58.33     56.86      60.29\n",
            "alpha    58.33    57.35     59.31      55.39\n",
            "beta     56.37    58.82     57.35      58.33\n",
            "gamma    63.73    59.31     53.92      59.31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('arousal', 'gpc')"
      ],
      "metadata": {
        "id": "QBw1c8r9O6s5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3230df88-a37e-4470-e407-eb99b5b246ad"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    57.84    52.45     61.76      52.45\n",
            "alpha    65.20    51.47     65.20      53.43\n",
            "beta     60.29    58.33     62.25      55.88\n",
            "gamma    63.73    55.88     59.31      63.73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('valence', 'gpc')"
      ],
      "metadata": {
        "id": "_mBzYOJwO-Tc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c148edf4-777f-43ff-b876-ed3f027baffe"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    55.88    55.88     54.41      54.41\n",
            "alpha    56.86    56.37     61.76      55.88\n",
            "beta     56.37    61.76     63.24      56.86\n",
            "gamma    56.86    63.73     52.94      57.35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('arousal', 'per')"
      ],
      "metadata": {
        "id": "J_tYtr1vPEwA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83be5f4b-9d93-4a78-cc88-43ccc6490410"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    52.45    48.53     53.43      50.00\n",
            "alpha    49.02    50.00     50.49      50.49\n",
            "beta     50.98    50.49     51.47      50.49\n",
            "gamma    50.49    57.84     52.45      50.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('valence', 'per')"
      ],
      "metadata": {
        "id": "Yz0L94FJPKCx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87c55b41-3935-40c4-96ee-de3e18f76d94"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    43.63    47.06     48.53      43.14\n",
            "alpha    43.14    49.02     48.53      44.12\n",
            "beta     54.41    47.55     44.12      43.63\n",
            "gamma    44.12    51.47     45.10      46.08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('arousal', 'cb')"
      ],
      "metadata": {
        "id": "OPkBIcPtPTXf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4584115d-7bde-4ffd-9549-5ad75cd96238"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    60.78    59.31     52.94      62.25\n",
            "alpha    65.20    63.73     57.84      62.25\n",
            "beta     60.29    54.90     56.86      55.88\n",
            "gamma    66.67    59.31     60.29      64.71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_conf(\"gamma\",\"frontal\",\"arousal\",\"cb\")"
      ],
      "metadata": {
        "id": "al5LA0spYBqU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "9941a79a-e8a5-4de6-f417-623dce826644"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x200 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Not Spam       0.65      0.71      0.68       101\n",
            "        Spam       0.69      0.62      0.65       103\n",
            "\n",
            "    accuracy                           0.67       204\n",
            "   macro avg       0.67      0.67      0.67       204\n",
            "weighted avg       0.67      0.67      0.67       204\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('valence', 'cb')"
      ],
      "metadata": {
        "id": "7LB1NnMQQcTD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54987fc9-3784-42eb-b2ad-bae4750e8a2c"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    54.90    56.37     55.88      61.27\n",
            "alpha    55.88    58.33     56.37      55.88\n",
            "beta     60.78    61.27     59.31      62.75\n",
            "gamma    58.82    58.33     57.84      59.80\n"
          ]
        }
      ]
    }
  ]
}