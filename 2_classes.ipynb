{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RabinSharma25/emotion-classification-using-eeg-signals/blob/main/2_classes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pwwbhMy0pssx"
      },
      "outputs": [],
      "source": [
        "#import the required libraries\n",
        "import os\n",
        "import time\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "#for feature extraction\n",
        "from scipy.signal import welch\n",
        "from scipy.integrate import simps\n",
        "\n",
        "#classifier libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "from sklearn import model_selection\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import itertools\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_recall_curve"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(filename):\n",
        "    x = pickle._Unpickler(open(filename, 'rb'))\n",
        "    x.encoding = 'latin1'\n",
        "    p = x.load()\n",
        "    return p"
      ],
      "metadata": {
        "id": "7ZUAXilSp_n9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating the file names of the dataset to load it\n",
        "files = []\n",
        "for n in range (1, 33):\n",
        "    s = 's'\n",
        "    if n < 10:\n",
        "        s += '0'\n",
        "    s += str(n)\n",
        "    s+=str(\".dat\")\n",
        "    files.append(s)\n",
        "print(files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYK5hgFMqFNy",
        "outputId": "e95b40a6-9434-478c-fe81-5738988a9e2b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['s01.dat', 's02.dat', 's03.dat', 's04.dat', 's05.dat', 's06.dat', 's07.dat', 's08.dat', 's09.dat', 's10.dat', 's11.dat', 's12.dat', 's13.dat', 's14.dat', 's15.dat', 's16.dat', 's17.dat', 's18.dat', 's19.dat', 's20.dat', 's21.dat', 's22.dat', 's23.dat', 's24.dat', 's25.dat', 's26.dat', 's27.dat', 's28.dat', 's29.dat', 's30.dat', 's31.dat', 's32.dat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 32x40 = 1280 trials for 32 participants\n",
        "labels = []\n",
        "data = []\n",
        "drive.mount('/content/drive')\n",
        "for i in files:\n",
        "  filename = \"/content/drive/My Drive/major-project/major-project-dataset/\" + i\n",
        "  trial = read_data(filename)\n",
        "  labels.append(trial['labels'])\n",
        "  data.append(trial['data'])\n"
      ],
      "metadata": {
        "id": "2Ru2cxT3qrtI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53bca10c-f267-401a-fb61-7d9829618af6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets see the shapes of the raw data\n",
        "labels = np.array(labels)\n",
        "data = np.array(data)\n",
        "print(\"Labels: \", labels.shape) # participants x videos x labels\n",
        "print(\"Data: \", data.shape) # participants x videos x channels x data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwIUirXzuY_3",
        "outputId": "e7984568-4891-48a4-c985-7394985910d4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels:  (32, 40, 4)\n",
            "Data:  (32, 40, 40, 8064)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-shape arrays into desired shapes\n",
        "labels = labels.flatten()\n",
        "labels = labels.reshape(1280, 4)\n",
        "\n",
        "data = data.flatten()\n",
        "data = data.reshape(1280, 40, 8064)"
      ],
      "metadata": {
        "id": "6JhtYReeudzh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Double-check the new arrays\n",
        "#Here trial = participants x vidoes = 32 x 40 = 1280\n",
        "\n",
        "print(\"Labels: \", labels.shape) # trial x label\n",
        "print(\"Data: \", data.shape) # trial x channel x data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pw4kk4WasX14",
        "outputId": "5ed8883e-712e-4048-a579-6f6abd1b1db7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels:  (1280, 4)\n",
            "Data:  (1280, 40, 8064)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating the dataframes for the labels\n",
        "labelsDf = pd.DataFrame(labels)\n",
        "print(\"printing the labels dataframe\\n\")\n",
        "print(labelsDf)\n",
        "print(\"\\n\\nDescribing the labels dataframe\")\n",
        "labelsDf.describe()"
      ],
      "metadata": {
        "id": "puNQxQcOx0Gq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "outputId": "2f112463-d08d-4c1e-d580-56e12e41faef"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "printing the labels dataframe\n",
            "\n",
            "         0     1     2     3\n",
            "0     7.71  7.60  6.90  7.83\n",
            "1     8.10  7.31  7.28  8.47\n",
            "2     8.58  7.54  9.00  7.08\n",
            "3     4.94  6.01  6.12  8.06\n",
            "4     6.96  3.92  7.19  6.05\n",
            "...    ...   ...   ...   ...\n",
            "1275  3.91  6.96  5.82  3.12\n",
            "1276  2.81  6.13  6.06  1.04\n",
            "1277  3.05  7.01  5.10  1.10\n",
            "1278  3.99  7.17  4.85  1.00\n",
            "1279  7.15  4.03  9.00  1.88\n",
            "\n",
            "[1280 rows x 4 columns]\n",
            "\n",
            "\n",
            "Describing the labels dataframe\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 0            1            2            3\n",
              "count  1280.000000  1280.000000  1280.000000  1280.000000\n",
              "mean      5.254313     5.156711     5.382750     5.518133\n",
              "std       2.130816     2.020499     2.096321     2.282780\n",
              "min       1.000000     1.000000     1.000000     1.000000\n",
              "25%       3.867500     3.762500     3.932500     3.960000\n",
              "50%       5.040000     5.230000     5.240000     6.050000\n",
              "75%       7.050000     6.950000     7.040000     7.090000\n",
              "max       9.000000     9.000000     9.000000     9.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-467de04b-123d-47bf-aeb7-acc61c68e29c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1280.000000</td>\n",
              "      <td>1280.000000</td>\n",
              "      <td>1280.000000</td>\n",
              "      <td>1280.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>5.254313</td>\n",
              "      <td>5.156711</td>\n",
              "      <td>5.382750</td>\n",
              "      <td>5.518133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.130816</td>\n",
              "      <td>2.020499</td>\n",
              "      <td>2.096321</td>\n",
              "      <td>2.282780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>3.867500</td>\n",
              "      <td>3.762500</td>\n",
              "      <td>3.932500</td>\n",
              "      <td>3.960000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>5.040000</td>\n",
              "      <td>5.230000</td>\n",
              "      <td>5.240000</td>\n",
              "      <td>6.050000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>7.050000</td>\n",
              "      <td>6.950000</td>\n",
              "      <td>7.040000</td>\n",
              "      <td>7.090000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-467de04b-123d-47bf-aeb7-acc61c68e29c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-467de04b-123d-47bf-aeb7-acc61c68e29c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-467de04b-123d-47bf-aeb7-acc61c68e29c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c76af997-cde2-4206-b5f2-15995116f7cd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c76af997-cde2-4206-b5f2-15995116f7cd')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c76af997-cde2-4206-b5f2-15995116f7cd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"labelsDf\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 450.87147400594165,\n        \"min\": 1.0,\n        \"max\": 1280.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          5.2543125,\n          5.04,\n          1280.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 1,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 450.88279505775586,\n        \"min\": 1.0,\n        \"max\": 1280.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          5.1567109375,\n          5.23,\n          1280.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 2,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 450.85389766830843,\n        \"min\": 1.0,\n        \"max\": 1280.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          5.38275,\n          5.24,\n          1280.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 3,\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 450.79289858765816,\n        \"min\": 1.0,\n        \"max\": 1280.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          5.5181328125,\n          6.05,\n          1280.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#giving names to the label columns\n",
        "labelsDf= pd.DataFrame({'Valence': labels[:,0], 'Arousal': labels[:,1], 'Dominance': labels[:,2], 'Liking': labels[:,3]})\n",
        "print(labelsDf.describe())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SGS2ghM3uYC",
        "outputId": "21983b75-e5d7-4075-ae38-78c121ada006"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Valence      Arousal    Dominance       Liking\n",
            "count  1280.000000  1280.000000  1280.000000  1280.000000\n",
            "mean      5.254313     5.156711     5.382750     5.518133\n",
            "std       2.130816     2.020499     2.096321     2.282780\n",
            "min       1.000000     1.000000     1.000000     1.000000\n",
            "25%       3.867500     3.762500     3.932500     3.960000\n",
            "50%       5.040000     5.230000     5.240000     6.050000\n",
            "75%       7.050000     6.950000     7.040000     7.090000\n",
            "max       9.000000     9.000000     9.000000     9.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dropping the Dominance and Liking columns\n",
        "labelsDf=labelsDf.drop('Dominance',axis=1)\n",
        "labelsDf=labelsDf.drop('Liking',axis=1)\n",
        "print(labelsDf.describe())\n",
        "# df = df.drop('B', axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3boVQ1FE4var",
        "outputId": "412a3925-0879-4e8f-f1cd-e3eeef5a94ce"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Valence      Arousal\n",
            "count  1280.000000  1280.000000\n",
            "mean      5.254313     5.156711\n",
            "std       2.130816     2.020499\n",
            "min       1.000000     1.000000\n",
            "25%       3.867500     3.762500\n",
            "50%       5.040000     5.230000\n",
            "75%       7.050000     6.950000\n",
            "max       9.000000     9.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset with only Valence column\n",
        "df_val = labelsDf['Valence']\n",
        "# Dataset with only Arousal column\n",
        "df_aro = labelsDf['Arousal']"
      ],
      "metadata": {
        "id": "Pjhzu_pQlWDv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to check if each trial has positive or negative valence\n",
        "def positive_valence(trial):\n",
        "    return 1 if labels[trial,0] >= np.median(labels[:,0]) else 0\n",
        "# Function to check if each trial has high or low arousal\n",
        "def high_arousal(trial):\n",
        "    return 1 if labels[trial,1] >= np.median(labels[:,1]) else 0"
      ],
      "metadata": {
        "id": "u8H4KOWLll-C"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all ratings to boolean values\n",
        "labels_encoded = []\n",
        "for i in range (len(labels)):\n",
        "    labels_encoded.append([positive_valence(i), high_arousal(i)])\n",
        "labels_encoded = np.reshape(labels_encoded, (1280, 2))\n",
        "df_labels = pd.DataFrame(data=labels_encoded, columns=[\"High Valence\", \"High Arousal\"])\n",
        "print(df_labels.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37Gsz-Z_lp_2",
        "outputId": "a4a6110c-9490-4559-b555-bff35ab2ccc0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       High Valence  High Arousal\n",
            "count   1280.000000   1280.000000\n",
            "mean       0.531250      0.500000\n",
            "std        0.499218      0.500195\n",
            "min        0.000000      0.000000\n",
            "25%        0.000000      0.000000\n",
            "50%        1.000000      0.500000\n",
            "75%        1.000000      1.000000\n",
            "max        1.000000      1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset with only Valence column\n",
        "df_valence = df_labels['High Valence']\n",
        "# Dataset with only Arousal column\n",
        "df_arousal = df_labels['High Arousal']"
      ],
      "metadata": {
        "id": "4BrMAjqult5R"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FEATURE EXTRACTION USING WELCH'S METHOD"
      ],
      "metadata": {
        "id": "Gh0IevZcnKtX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eeg_channels = np.array([\"Fp1\", \"AF3\", \"F3\", \"F7\", \"FC5\", \"FC1\", \"C3\", \"T7\", \"CP5\", \"CP1\", \"P3\", \"P7\", \"PO3\", \"O1\", \"Oz\", \"Pz\", \"Fp2\", \"AF4\", \"Fz\", \"F4\", \"F8\", \"FC6\", \"FC2\", \"Cz\", \"C4\", \"T8\", \"CP6\", \"CP2\", \"P4\", \"P8\", \"PO4\", \"O2\"])\n",
        "peripheral_channels = np.array([\"hEOG\", \"vEOG\", \"zEMG\", \"tEMG\", \"GSR\", \"Respiration belt\", \"Plethysmograph\", \"Temperature\"])"
      ],
      "metadata": {
        "id": "QGtC7nRipvA8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eeg_data = []\n",
        "for i in range (len(data)):\n",
        "  for j in range (len(eeg_channels)):\n",
        "    eeg_data.append(data[i,j])\n",
        "eeg_data = np.reshape(eeg_data, (len(data), len(eeg_channels), len(data[0,0])))\n",
        "print(eeg_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyC_7IWLp93t",
        "outputId": "086522c6-65ec-48b3-b73e-2d5b632546dc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1280, 32, 8064)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "peripheral_data = []\n",
        "for i in range (len(data)):\n",
        "  for j in range (32,len(data[0])):\n",
        "    peripheral_data.append(data[i,j])\n",
        "peripheral_data = np.reshape(peripheral_data, (len(data), len(peripheral_channels), len(data[0,0])))\n",
        "print(peripheral_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5P3G0PWqEsM",
        "outputId": "ed22df23-9893-4afe-c441-7afcf73bcc23"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1280, 8, 8064)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def bandpower(data, sf, band, window_sec=None, relative=False):\n",
        "    band = np.asarray(band)\n",
        "    low, high = band\n",
        "\n",
        "    # Define window length\n",
        "    if window_sec is not None:\n",
        "        nperseg = window_sec * sf\n",
        "    else:\n",
        "        nperseg = (2 / low) * sf\n",
        "\n",
        "    # Compute the modified periodogram (Welch)\n",
        "    freqs, psd = welch(data, sf, nperseg=nperseg)\n",
        "\n",
        "    # Frequency resolution\n",
        "    freq_res = freqs[1] - freqs[0]\n",
        "\n",
        "    # Find closest indices of band in frequency vector\n",
        "    idx_band = np.logical_and(freqs >= low, freqs <= high)\n",
        "\n",
        "    # Integral approximation of the spectrum using Simpson's rule.\n",
        "    bp = simps(psd[idx_band], dx=freq_res)\n",
        "\n",
        "    if relative:\n",
        "        bp /= simps(psd, dx=freq_res)\n",
        "    return bp"
      ],
      "metadata": {
        "id": "Xc9rTRGHnYKt"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_band_power(trial, channel, band):\n",
        "  bd = (0,0)\n",
        "\n",
        "  if (band == \"theta\"): # drownsiness, emotional connection, intuition, creativity\n",
        "    bd = (4,8)\n",
        "  elif (band == \"alpha\"): # reflection, relaxation\n",
        "    bd = (8,12)\n",
        "  elif (band == \"beta\"): # concentration, problem solving, memory\n",
        "    bd = (12,30)\n",
        "  elif (band == \"gamma\"): # cognition, perception, learning, multi-tasking\n",
        "    bd = (30,64)\n",
        "\n",
        "  return bandpower(eeg_data[trial,channel], 128, bd)\n"
      ],
      "metadata": {
        "id": "mIxttAuQnrel"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_band_power(0,31,\"theta\"))\n",
        "print(get_band_power(0,31,\"alpha\"))\n",
        "print(get_band_power(0,31,\"beta\"))\n",
        "print(get_band_power(0,31,\"gamma\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpNyS4s8n9Q3",
        "outputId": "6a42713d-652c-4023-a0a2-46ca02607399"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.434119660168186\n",
            "5.369595513295193\n",
            "6.286556266834863\n",
            "0.9879159580139809\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process new datasets with 6 EEG regions and 4 band power values"
      ],
      "metadata": {
        "id": "8psQHMB9xqpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform 1280x 32 x 8064 => 1280 x 128\n",
        "eeg_band_arr = []\n",
        "for i in range (len(eeg_data)):\n",
        "  for j in range (len(eeg_data[0])):\n",
        "    eeg_band_arr.append(get_band_power(i,j,\"theta\"))\n",
        "    eeg_band_arr.append(get_band_power(i,j,\"alpha\"))\n",
        "    eeg_band_arr.append(get_band_power(i,j,\"beta\"))\n",
        "    eeg_band_arr.append(get_band_power(i,j,\"gamma\"))\n",
        "eeg_band_arr = np.reshape(eeg_band_arr, (1280, 128))"
      ],
      "metadata": {
        "id": "r0y_hNHIxoxi"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frontal = np.array([\"F3\", \"FC1\", \"Fz\", \"F4\", \"FC2\"])\n",
        "parietal = np.array([\"P3\", \"P7\", \"Pz\", \"P4\", \"P8\"])\n",
        "occipital = np.array([\"O1\", \"Oz\", \"O2\", \"PO3\", \"PO4\"])\n",
        "central = np.array([\"CP5\", \"CP1\", \"Cz\", \"C4\", \"C3\", \"CP6\", \"CP2\"])"
      ],
      "metadata": {
        "id": "HVksA4V2ypUD"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eeg_theta = []\n",
        "for i in range (len(eeg_data)):\n",
        "  for j in range (len(eeg_data[0])):\n",
        "    eeg_theta.append(get_band_power(i,j,\"theta\"))\n",
        "eeg_theta = np.reshape(eeg_theta, (1280, 32))\n",
        "\n",
        "df_theta = pd.DataFrame(data = eeg_theta, columns=eeg_channels)\n",
        "print(df_theta.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YQbtKTRzmdG",
        "outputId": "e49b44fa-88c2-444d-ae8a-6997bc5c3eb0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                Fp1           AF3            F3            F7           FC5  \\\n",
            "count   1280.000000   1280.000000   1280.000000   1280.000000   1280.000000   \n",
            "mean     517.431002    974.493856    795.093910   1515.004071    746.435950   \n",
            "std     1165.295163   3147.555132   3020.711666   4990.544508   2188.101734   \n",
            "min        2.698928      1.995398      1.820656      3.283107      1.311200   \n",
            "25%       23.306027     17.117475     18.392335     30.827191     14.023436   \n",
            "50%       65.468048     82.102529     60.241953     91.266539     48.704968   \n",
            "75%      331.636624    304.949248    175.394835    248.308696    218.358248   \n",
            "max    15524.135098  38122.870846  39431.320394  49272.793208  20182.668545   \n",
            "\n",
            "               FC1            C3           T7           CP5          CP1  ...  \\\n",
            "count  1280.000000   1280.000000  1280.000000   1280.000000  1280.000000  ...   \n",
            "mean    345.936665    486.095113   354.004358    664.656754   276.667812  ...   \n",
            "std     717.328768   1497.636647   641.432373   2146.857585   498.823733  ...   \n",
            "min       2.025214      1.200156     3.418167      1.857737     1.340340  ...   \n",
            "25%      17.383454     18.273844    43.838157     19.178089    28.281757  ...   \n",
            "50%      55.561232     46.451432   128.629588     61.998792    60.330974  ...   \n",
            "75%     281.828747    234.342275   348.249862    312.653366   237.040645  ...   \n",
            "max    8542.244175  26021.167025  7023.985761  23255.512271  5972.589469  ...   \n",
            "\n",
            "                FC2            Cz            C4            T8           CP6  \\\n",
            "count   1280.000000   1280.000000   1280.000000   1280.000000   1280.000000   \n",
            "mean     669.953166    518.975872    471.905917    763.990397    544.785103   \n",
            "std     1714.340609   1407.210210   1896.584105   2236.655420   1563.878765   \n",
            "min        2.809501      1.637028      1.713283      2.133474      1.496299   \n",
            "25%       14.927239     23.440039      9.724978     25.143155     18.202863   \n",
            "50%       66.078472     67.044674     24.318235     90.110040     77.517763   \n",
            "75%      214.293557    305.766182    109.715681    386.232875    297.742897   \n",
            "max    18617.218099  16408.471958  24826.365142  28038.714329  19908.437378   \n",
            "\n",
            "               CP2            P4           P8           PO4            O2  \n",
            "count  1280.000000   1280.000000  1280.000000   1280.000000   1280.000000  \n",
            "mean    303.158814    527.491149   222.759444    780.700914    327.242856  \n",
            "std     650.093432   1429.084909   451.119818   1802.083002   1080.707954  \n",
            "min       1.439077      1.791296     1.641482      1.995230      3.681268  \n",
            "25%      26.754006     18.110041    18.064264     23.491991     19.333926  \n",
            "50%      88.083088     62.269206    90.914662     69.772647     49.630345  \n",
            "75%     257.959593    246.195582   225.463786    495.202699    113.117403  \n",
            "max    7210.746793  17568.065100  5487.311705  17167.017710  12314.030522  \n",
            "\n",
            "[8 rows x 32 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform 880 x 32 x 8064 => 880 x 32\n",
        "eeg_alpha = []\n",
        "for i in range (len(eeg_data)):\n",
        "  for j in range (len(eeg_data[0])):\n",
        "    eeg_alpha.append(get_band_power(i,j,\"alpha\"))\n",
        "eeg_alpha = np.reshape(eeg_alpha, (1280, 32))\n",
        "\n",
        "df_alpha = pd.DataFrame(data = eeg_alpha, columns=eeg_channels)\n",
        "print(df_alpha.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1uT6ZlnzqL-",
        "outputId": "6af5a181-ab0c-40a9-903c-8d2972601739"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               Fp1           AF3            F3            F7          FC5  \\\n",
            "count  1280.000000   1280.000000   1280.000000   1280.000000  1280.000000   \n",
            "mean    173.544174    333.184883    285.545436    573.725974   249.793720   \n",
            "std     378.371554   1058.758333   1036.330438   1943.692827   702.436934   \n",
            "min       2.770151      1.793012      1.724442      2.793554     0.975527   \n",
            "25%      14.082681     10.737435     10.012318     15.888935    10.074042   \n",
            "50%      33.713172     34.992442     30.026795     38.178299    20.079100   \n",
            "75%     125.508396    114.334883     73.488279     93.987498    74.040939   \n",
            "max    5627.906982  12380.702125  12764.724842  20843.070851  6575.781434   \n",
            "\n",
            "               FC1           C3           T7          CP5          CP1  ...  \\\n",
            "count  1280.000000  1280.000000  1280.000000  1280.000000  1280.000000  ...   \n",
            "mean    138.011594   170.423962   124.101183   232.750895    97.610644  ...   \n",
            "std     275.218934   497.507620   200.171342   735.717912   166.430024  ...   \n",
            "min       1.682977     1.963403     2.545447     2.251935     1.682386  ...   \n",
            "25%       9.228670    10.197058    21.452775    11.461012    13.367379  ...   \n",
            "50%      24.929907    22.318468    51.473641    25.364340    26.229818  ...   \n",
            "75%     117.988147    83.518174   129.890552   110.822931    91.941255  ...   \n",
            "max    3030.077791  9215.549575  2142.437275  7894.273428  2148.109415  ...   \n",
            "\n",
            "               FC2           Cz            C4           T8          CP6  \\\n",
            "count  1280.000000  1280.000000   1280.000000  1280.000000  1280.000000   \n",
            "mean    228.331178   199.941306    193.380344   276.638039   213.620700   \n",
            "std     569.202821   578.947819    812.031964   782.782127   661.135030   \n",
            "min       2.390011     1.315395      1.227024     1.636114     2.181082   \n",
            "25%       8.715210    12.127290      6.665166    15.980086    11.300805   \n",
            "50%      26.880000    29.827367     14.169702    34.252038    31.722499   \n",
            "75%      95.964685   102.480314     40.800135   142.704220   106.673844   \n",
            "max    6038.451512  6881.486185  10501.570698  9022.269308  8394.200372   \n",
            "\n",
            "               CP2           P4           P8          PO4           O2  \n",
            "count  1280.000000  1280.000000  1280.000000  1280.000000  1280.000000  \n",
            "mean    112.138023   181.507936    89.003586   269.572920   126.053116  \n",
            "std     236.260122   482.163785   186.841170   597.330120   400.465505  \n",
            "min       1.520363     2.158950     1.212432     2.002769     3.617116  \n",
            "25%      15.797490    11.564300    10.971965    13.489443    10.548845  \n",
            "50%      37.807211    28.494994    38.758864    29.247807    23.906708  \n",
            "75%      86.070815    92.261617    88.107272   162.117585    45.294454  \n",
            "max    2368.537880  5666.733316  2323.499853  5542.263376  3966.714864  \n",
            "\n",
            "[8 rows x 32 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform 880 x 32 x 8064 => 880 x 32\n",
        "eeg_beta = []\n",
        "for i in range (len(eeg_data)):\n",
        "  for j in range (len(eeg_data[0])):\n",
        "    eeg_beta.append(get_band_power(i,j,\"beta\"))\n",
        "eeg_beta = np.reshape(eeg_beta, (1280, 32))\n",
        "\n",
        "df_beta = pd.DataFrame(data = eeg_beta, columns=eeg_channels)\n",
        "print(df_beta.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUNGxdRLzvAW",
        "outputId": "cecd2440-4f9c-4d01-d6ba-b510acb601be"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               Fp1          AF3           F3            F7          FC5  \\\n",
            "count  1280.000000  1280.000000  1280.000000   1280.000000  1280.000000   \n",
            "mean     92.200266   200.684361   193.372441    341.951096   140.642394   \n",
            "std     176.162562   668.027842   668.771074   1274.679748   386.740178   \n",
            "min       4.486703     2.650970     2.784376      3.363422     2.788531   \n",
            "25%      17.999409    13.190613    10.170223     17.951892    11.341571   \n",
            "50%      34.413828    29.155242    28.120517     33.617573    22.402198   \n",
            "75%      80.774739   107.619088    59.807546     62.087123    53.162211   \n",
            "max    3528.485435  5784.160193  5841.475721  14848.700463  3094.687504   \n",
            "\n",
            "               FC1           C3           T7          CP5          CP1  ...  \\\n",
            "count  1280.000000  1280.000000  1280.000000  1280.000000  1280.000000  ...   \n",
            "mean     99.397187    96.901859    77.540922   137.387625    55.047411  ...   \n",
            "std     197.556641   247.621839    90.107877   409.366974    85.846400  ...   \n",
            "min       3.145262     2.991550     2.477519     2.886994     2.237445  ...   \n",
            "25%       8.919796     9.331138    23.668656    11.097811     9.370318  ...   \n",
            "50%      17.625226    19.698019    44.350493    22.622621    18.748879  ...   \n",
            "75%      79.066898    46.494215    93.087474    58.273844    51.308093  ...   \n",
            "max    1702.542675  5187.441594   634.645675  4094.116788  1238.024108  ...   \n",
            "\n",
            "               FC2           Cz           C4           T8          CP6  \\\n",
            "count  1280.000000  1280.000000  1280.000000  1280.000000  1280.000000   \n",
            "mean    130.756212   131.961699   136.536661   178.534190   144.021213   \n",
            "std     338.843374   409.481195   603.902680   506.405387   484.806320   \n",
            "min       3.037555     1.816169     2.079646     2.429178     2.524762   \n",
            "25%       9.115906    10.803851     7.350435    17.466794    11.400277   \n",
            "50%      17.479204    25.300435    12.926731    30.157902    23.679679   \n",
            "75%      75.259937    54.745901    25.609433    93.320360    64.338413   \n",
            "max    2786.622358  4842.537927  7489.480637  4064.418656  5955.637009   \n",
            "\n",
            "               CP2           P4           P8          PO4           O2  \n",
            "count  1280.000000  1280.000000  1280.000000  1280.000000  1280.000000  \n",
            "mean     74.751315   103.971557    62.850280   148.868580    90.837375  \n",
            "std     165.803855   288.941591   134.177873   329.650542   275.150372  \n",
            "min       2.272088     2.914214     3.059450     2.461123     5.084168  \n",
            "25%      13.014515    11.560334    12.312069    13.732279    11.291516  \n",
            "50%      24.705624    20.439224    27.763129    23.940855    19.688820  \n",
            "75%      53.062550    67.593128    64.187473    99.947093    36.076683  \n",
            "max    1265.234960  2533.794744  1653.743590  2516.334382  2454.759737  \n",
            "\n",
            "[8 rows x 32 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform 880 x 32 x 8064 => 880 x 32\n",
        "eeg_gamma = []\n",
        "for i in range (len(eeg_data)):\n",
        "  for j in range (len(eeg_data[0])):\n",
        "    eeg_gamma.append(get_band_power(i,j,\"gamma\"))\n",
        "eeg_gamma = np.reshape(eeg_gamma, (1280, 32))\n",
        "\n",
        "df_gamma = pd.DataFrame(data = eeg_gamma, columns=eeg_channels)\n",
        "print(df_gamma.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJP58i-60AEV",
        "outputId": "0e61527e-e46d-4561-e069-8b0fa827ed11"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               Fp1          AF3           F3           F7          FC5  \\\n",
            "count  1280.000000  1280.000000  1280.000000  1280.000000  1280.000000   \n",
            "mean     50.663247   150.492372   149.210824   122.775294    91.633571   \n",
            "std     135.591365   645.635199   632.049548   496.744509   340.562025   \n",
            "min       1.069233     1.119910     0.791859     1.177512     0.712624   \n",
            "25%       8.292302     5.605668     4.352724     7.591098     5.172920   \n",
            "50%      18.329100    14.069330    11.828027    15.919877    12.648067   \n",
            "75%      39.995543    43.454610    28.586812    36.456420    38.094737   \n",
            "max    3213.255538  6060.539788  5758.756812  6348.177208  3472.515858   \n",
            "\n",
            "               FC1           C3           T7          CP5          CP1  ...  \\\n",
            "count  1280.000000  1280.000000  1280.000000  1280.000000  1280.000000  ...   \n",
            "mean     60.487715    56.236468    43.792285    77.048369    29.250057  ...   \n",
            "std     139.230040   180.285647    62.089299   267.809883    68.503827  ...   \n",
            "min       0.694928     0.684210     0.562676     0.788359     0.509259  ...   \n",
            "25%       2.666459     3.534375    10.492723     3.837095     3.156311  ...   \n",
            "50%       5.313502     9.056340    23.540152     9.532963     7.735783  ...   \n",
            "75%      41.027482    29.838450    54.382720    22.302862    18.604669  ...   \n",
            "max    1413.985901  4376.356658   809.935644  2294.561961  1011.985310  ...   \n",
            "\n",
            "               FC2           Cz           C4           T8          CP6  \\\n",
            "count  1280.000000  1280.000000  1280.000000  1280.000000  1280.000000   \n",
            "mean     79.191679    56.547283    55.989289   117.745424    64.340498   \n",
            "std     306.464243   167.968314   239.143748   428.381018   199.387966   \n",
            "min       0.692350     0.395680     0.697486     1.027260     0.609425   \n",
            "25%       2.893590     3.800780     2.286229     7.221083     3.850387   \n",
            "50%       7.535055     8.493068     4.640151    17.921722    10.563661   \n",
            "75%      26.896601    20.882885    13.538524    39.981006    30.943717   \n",
            "max    2829.052404  1892.813293  3054.309237  3954.519861  2405.614840   \n",
            "\n",
            "               CP2           P4           P8          PO4           O2  \n",
            "count  1280.000000  1280.000000  1280.000000  1280.000000  1280.000000  \n",
            "mean     46.943309    68.100629    29.805883    82.197819    55.821892  \n",
            "std     142.501802   265.168885    57.843154   272.880509   203.288109  \n",
            "min       0.478152     0.761387     0.804002     0.838747     0.877576  \n",
            "25%       3.849956     3.690054     4.513405     4.620006     4.380690  \n",
            "50%       8.646393     7.300040    10.911662    10.430337     6.994829  \n",
            "75%      22.614200    28.908609    26.489674    42.960182    14.567389  \n",
            "max    1426.462453  2446.765512   647.533599  2505.826127  1794.801694  \n",
            "\n",
            "[8 rows x 32 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training/testing sets\n",
        "def split_train_test(x, y):\n",
        "  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)\n",
        "  return x_train, x_test, y_train, y_test"
      ],
      "metadata": {
        "id": "kmTpi6Qc02Bu"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature scaling\n",
        "def feature_scaling(train, test):\n",
        "  sc = StandardScaler()\n",
        "  train = sc.fit_transform(train)\n",
        "  test = sc.transform(test)\n",
        "  return train, test"
      ],
      "metadata": {
        "id": "cOCvaWDba3pa"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "band_names = np.array([\"theta\", \"alpha\", \"beta\", \"gamma\"])\n",
        "channel_names = np.array([\"frontal\",  \"central\", \"parietal\", \"occipital\"])\n",
        "label_names = np.array([\"valence\", \"arousal\"])"
      ],
      "metadata": {
        "id": "EHrmKN1nf8ys"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing different kernels (linear, sigmoid, rbf, poly) to select the most optimal one\n",
        "clf_svm = SVC(kernel = 'rbf', random_state = 42, probability=True)"
      ],
      "metadata": {
        "id": "tYVZwPvFgATk"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing different k (odd) numbers, algorithm (auto, ball_tree, kd_tree) and weight (uniform, distance) to select the most optimal one\n",
        "clf_knn = KNeighborsClassifier(n_neighbors=5, weights='distance', algorithm='auto')"
      ],
      "metadata": {
        "id": "t4pznSOZgHmK"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_dtree=DecisionTreeClassifier(max_depth=20,min_samples_split=4)"
      ],
      "metadata": {
        "id": "7gi_aMFOgL08"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_rf=RandomForestClassifier(n_estimators=50,max_depth=20,min_samples_split=5)"
      ],
      "metadata": {
        "id": "T9hH4fktgPOX"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_nb= GaussianNB()"
      ],
      "metadata": {
        "id": "7lJ9wwn0gUmH"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing different learning rate (alpha), solver (adam, sgd, lbfgs) and activation (relu, tanh, logistic) to select the most optimal one\n",
        "clf_mlp = MLPClassifier(solver='adam', activation='tanh', alpha=0.3, max_iter=400)"
      ],
      "metadata": {
        "id": "h-f5ay_0gYey"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_adaboost = AdaBoostClassifier(n_estimators=50, random_state=42)\n"
      ],
      "metadata": {
        "id": "vOCq_XttoT4e"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_xgb = xgb.XGBClassifier(learning_rate=0.1, max_depth=3, n_estimators=100, random_state=42)\n"
      ],
      "metadata": {
        "id": "xJMuk2lWq8v6"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "clf_lgbm = lgb.LGBMClassifier()"
      ],
      "metadata": {
        "id": "KFc0oT1XMHGv"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "clf_gpc = GaussianProcessClassifier(kernel=1.0 * RBF(length_scale=1.0), random_state=42)\n"
      ],
      "metadata": {
        "id": "G_SB4HCTNIWQ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Perceptron\n",
        "clf_perceptron = Perceptron(random_state=42)"
      ],
      "metadata": {
        "id": "xQ_4Dt-2N5t8"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost\n",
        "import catboost as cb\n",
        "clf_catboost = cb.CatBoostClassifier(iterations=1000, depth=6, learning_rate=0.1, loss_function='Logloss', verbose=0)\n"
      ],
      "metadata": {
        "id": "9pheOwL8PlYt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1cb6102-b595-46b2-880e-83a4d62f0294"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.5-cp310-cp310-manylinux2014_x86_64.whl (98.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.25.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.11.4)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.4.1)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define cnn model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "def create_cnn_model(data_np):\n",
        "    model = Sequential([\n",
        "        Input(shape=(data_np.shape[1], 1)),  # Adjusted to match data_np shape\n",
        "        Conv1D(filters=64, kernel_size=1, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Dropout(0.2),\n",
        "        Conv1D(filters=64, kernel_size=1, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Dropout(0.2),\n",
        "        Flatten(),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.1),\n",
        "        Dense(2, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def train_and_evaluate_cnn(df_x, df_y):\n",
        "    # Convert dataframes to numpy arrays\n",
        "    data_np = df_x.values\n",
        "    labels_np = df_y.values\n",
        "\n",
        "    # Reshape data for CNN: (number of samples, height, width, channels)\n",
        "    # Here, height=1, width=number of features, channels=1\n",
        "    data_reshaped = data_np.reshape((data_np.shape[0], data_np.shape[1], 1))\n",
        "\n",
        "    # One-hot encode the labels\n",
        "    labels_encoded = to_categorical(labels_np)\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(data_reshaped, labels_encoded, test_size=0.20, random_state=42)\n",
        "\n",
        "    # Define the CNN model\n",
        "    model = create_cnn_model(data_np)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    # test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "    # Predict on the test data\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "\n",
        "    return y_true,y_pred_classes\n"
      ],
      "metadata": {
        "id": "llDxjmEzwD4b"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define ann model\n",
        "def create_ann_model(data_scaled):\n",
        "    model = Sequential([\n",
        "        Dense(64, input_dim=data_scaled.shape[1], activation='relu'),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(2, activation='softmax')  # Assuming two classes: 0 and 1\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_and_evaluate_ann(df_x, df_y):\n",
        "    # Convert dataframes to numpy arrays\n",
        "    data_np = df_x.values\n",
        "    labels_np = df_y.values\n",
        "\n",
        "    # Standardize the data\n",
        "    scaler = StandardScaler()\n",
        "    data_scaled = scaler.fit_transform(data_np)\n",
        "\n",
        "    # One-hot encode the labels\n",
        "    labels_encoded = to_categorical(labels_np)\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(data_scaled, labels_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define the CNN model\n",
        "    model = create_ann_model(data_np)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    # test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "\n",
        "    # Predict on the test data\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "    return y_true,y_pred_classes\n",
        "\n"
      ],
      "metadata": {
        "id": "WJP4i5h0wF5q"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define lstm model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "def create_lstm_model(data_np):\n",
        "    model = Sequential([\n",
        "    LSTM(20, input_shape=(data_np.shape[1], 1)),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(2, activation='softmax')  # Assuming two classes: 0 and 1\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def train_and_evaluate_lstm(df_x, df_y):\n",
        "    # Convert dataframes to numpy arrays\n",
        "    data_np = df_x.values\n",
        "    labels_np = df_y.values\n",
        "\n",
        "    # Reshape data for LSTM: (samples, time_steps, features)\n",
        "    # Here, each row is treated as a sequence with a single time step\n",
        "    data_reshaped = data_np.reshape((data_np.shape[0], data_np.shape[1], 1))\n",
        "\n",
        "    # One-hot encode the labels\n",
        "    labels_encoded = to_categorical(labels_np)\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(data_reshaped, labels_encoded, test_size=0.2, random_state=42)\n",
        "    # Define the CNN model\n",
        "    model = create_lstm_model(data_np)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    # test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "    # Predict on the test data\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "\n",
        "    return y_true,y_pred_classes"
      ],
      "metadata": {
        "id": "gTtgqxsewObj"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = []\n",
        "models.append(('SVM', clf_svm))\n",
        "models.append(('k-NN', clf_knn))\n",
        "models.append(('DT', clf_dtree))\n",
        "models.append(('RF', clf_rf))\n",
        "models.append(('NB', clf_nb))\n",
        "models.append(('MLP', clf_mlp))\n",
        "models.append(('AB', clf_adaboost))\n",
        "models.append(('XGB', clf_xgb))"
      ],
      "metadata": {
        "id": "i28DLITPgfgJ"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_clf_cv(band, channel, label, clf):\n",
        "  if (band == \"theta\"):\n",
        "    df_x = df_theta\n",
        "  elif (band == \"alpha\"):\n",
        "    df_x = df_alpha\n",
        "  elif (band == \"beta\"):\n",
        "    df_x = df_beta\n",
        "  elif (band == \"gamma\"):\n",
        "    df_x = df_gamma\n",
        "\n",
        "  if (channel == \"frontal\"):\n",
        "    df_x = df_x[frontal]\n",
        "  elif (channel == \"central\"):\n",
        "    df_x = df_x[central]\n",
        "  elif (channel == \"parietal\"):\n",
        "    df_x = df_x[parietal]\n",
        "  elif (channel == \"occipital\"):\n",
        "    df_x = df_x[occipital]\n",
        "\n",
        "  df_y = df_arousal if (label == \"arousal\") else df_valence\n",
        "\n",
        "  # Train-test split\n",
        "  x_train, x_test, y_train, y_test = split_train_test(df_x, df_y)\n",
        "\n",
        "  # Apply CV\n",
        "  x_for_kfold = np.array(x_train)\n",
        "  y_for_kfold = np.array(y_train)\n",
        "  kfold = model_selection.KFold(n_splits=5)\n",
        "\n",
        "  for i, j in kfold.split(x_for_kfold):\n",
        "   x_train2, x_test2 = x_for_kfold[i], x_for_kfold[j]\n",
        "   y_train2, y_test2 = y_for_kfold[i], y_for_kfold[j]\n",
        "\n",
        "  # Feature scaling\n",
        "  x_train2, x_test2 = feature_scaling(x_train2, x_test2)\n",
        "\n",
        "  if (clf == \"svm\"):\n",
        "    clf_svm.fit(x_train2, y_train2)\n",
        "    y_predict = clf_svm.predict(x_test2)\n",
        "  elif (clf == \"knn\"):\n",
        "    clf_knn.fit(x_train2, y_train2)\n",
        "    y_predict = clf_knn.predict(x_test2)\n",
        "  elif (clf == \"dtree\"):\n",
        "    clf_dtree.fit(x_train2, y_train2)\n",
        "    y_predict = clf_dtree.predict(x_test2)\n",
        "  elif (clf == \"rf\"):\n",
        "    clf_rf.fit(x_train2, y_train2)\n",
        "    y_predict = clf_rf.predict(x_test2)\n",
        "  elif (clf == \"nb\"):\n",
        "    clf_nb.fit(x_train2, y_train2)\n",
        "    y_predict = clf_nb.predict(x_test2)\n",
        "  elif (clf == \"mlp\"):\n",
        "    clf_mlp.fit(x_train2, y_train2)\n",
        "    y_predict = clf_mlp.predict(x_test2)\n",
        "  elif (clf == \"ab\"):\n",
        "    clf_adaboost.fit(x_train2, y_train2)\n",
        "    y_predict = clf_adaboost.predict(x_test2)\n",
        "  elif (clf == \"xgb\"):\n",
        "    clf_xgb.fit(x_train2, y_train2)\n",
        "    y_predict = clf_xgb.predict(x_test2)\n",
        "  elif (clf == \"lgbm\"):\n",
        "    clf_lgbm.fit(x_train2, y_train2)\n",
        "    y_predict = clf_lgbm.predict(x_test2)\n",
        "  elif (clf == \"gpc\"):\n",
        "    clf_gpc.fit(x_train2, y_train2)\n",
        "    y_predict = clf_gpc.predict(x_test2)\n",
        "  elif (clf == \"per\"):\n",
        "    clf_perceptron.fit(x_train2, y_train2)\n",
        "    y_predict = clf_perceptron.predict(x_test2)\n",
        "\n",
        "  elif (clf == \"cb\"):\n",
        "    clf_catboost.fit(x_train2, y_train2)\n",
        "    y_predict = clf_catboost.predict(x_test2)\n",
        "\n",
        "  elif (clf == \"cnn\"):\n",
        "    # Evaluate the model on the test set\n",
        "    test,predict = train_and_evaluate_cnn(df_x, df_y)\n",
        "    # print(f\"Test Loss: {test_loss}\")\n",
        "    # print(f\"Test Accuracy: {test_accuracy}\")\n",
        "    return test,predict\n",
        "\n",
        "  elif(clf ==\"ann\"):\n",
        "    test, predict= train_and_evaluate_ann(df_x, df_y)\n",
        "    # print(f\"Test Loss: {test_loss}\")\n",
        "    # print(f\"Test Accuracy: {test_accuracy}\")\n",
        "    return test,predict\n",
        "  elif(clf ==\"lstm\"):\n",
        "    test,predict  = train_and_evaluate_lstm(df_x, df_y)\n",
        "    return test,predict\n",
        "  return y_test2, y_predict"
      ],
      "metadata": {
        "id": "L_WzJMzsjrL4"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(band, channel, label, clf):\n",
        "  y_test2, y_predict = run_clf_cv(band, channel, label, clf)\n",
        "  return np.round(accuracy_score(y_test2, y_predict)*100,2)"
      ],
      "metadata": {
        "id": "gWN0kvbJjyhB"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "def print_conf(band, channel, label, clf):\n",
        "  y_test2, y_predict = run_clf_cv(band, channel, label, clf)\n",
        "  conf_matrix = confusion_matrix(y_test2, y_predict)\n",
        "  # print(conf_matrix)\n",
        "  plt.figure(figsize=(4, 2))  # Decrease the figure size\n",
        "  plt.title('Confusion Matrix')\n",
        "  sns.heatmap(conf_matrix,\n",
        "              annot=True,\n",
        "              fmt='g',\n",
        "              xticklabels=['0','1'],\n",
        "              yticklabels=['0','1'])\n",
        "\n",
        "  # display matrix\n",
        "  plt.ylabel('True Label',fontsize=12)\n",
        "  plt.xlabel('Predicted Label',fontsize=12)\n",
        "  plt.show()\n",
        "\n",
        "  # printing the classificati# Split the data into training/testing sets\n",
        "# def split_train_test(x, y):\n",
        "#   x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)\n",
        "#   return x_train, x_test, y_train, y_test\n",
        "\n",
        "  class_report = classification_report(y_test2, y_predict, target_names=['0', '1'])\n",
        "  print(\"\\nClassification Report:\")\n",
        "  print(class_report)"
      ],
      "metadata": {
        "id": "cB-1MOCIuv_M"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_accuracy(label, clf):\n",
        "  arr = []\n",
        "  for i in range (len(band_names)):\n",
        "    for j in range (len(channel_names)):\n",
        "      arr.append(get_accuracy(band_names[i], channel_names[j], label, clf))\n",
        "  arr = np.reshape(arr, (4,4))\n",
        "  df = pd.DataFrame(data = arr, index=band_names, columns=channel_names)\n",
        "\n",
        "  #print(\"Top 3 EEG regions with highest scores\")\n",
        "  #print(df.apply(lambda s: s.abs()).max().nlargest(3))\n",
        "  #print()\n",
        "  #print(\"Top 2 bands with highest scores\")\n",
        "  #print(df.apply(lambda s: s.abs()).max(axis=1).nlargest(2))\n",
        "  #print()\n",
        "  #print(\"EEG region with highest scores per each band\")\n",
        "  #print(df.idxmax(axis=1))\n",
        "  #print()\n",
        "  #print(\"Accuracy Scores\")\n",
        "  #print(df.idxmax())\n",
        "  #print()\n",
        "  print(df)"
      ],
      "metadata": {
        "id": "_kBo9bznkFa9"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print_accuracy('arousal', 'svm')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDIaK777kOdU",
        "outputId": "73e84062-fc28-4067-9ac0-586d03ad6835"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    52.94    50.00     54.41      50.98\n",
            "alpha    53.43    50.00     50.00      50.98\n",
            "beta     52.45    50.00     55.39      50.49\n",
            "gamma    50.49    52.94     51.96      49.02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('valence', 'svm')"
      ],
      "metadata": {
        "id": "XHUBype4mFY6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8b4ad30-3465-4598-c978-10cfe23fd5f9"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    55.39    53.92     54.41      55.88\n",
            "alpha    55.39    54.90     55.39      55.39\n",
            "beta     54.41    56.86     56.86      56.37\n",
            "gamma    54.90    58.33     56.37      58.33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_conf(\"beta\",\"parietal\",\"arousal\",\"svm\")"
      ],
      "metadata": {
        "id": "EZfqPFKQrSZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_conf(\"gamma\",\"central\",\"valence\",\"svm\")"
      ],
      "metadata": {
        "id": "Bv3oLNIVGCSf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "9cb9a7d1-b0b4-4e24-e0b0-390009c86c38"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x200 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.10      0.17        90\n",
            "           1       0.58      0.96      0.72       114\n",
            "\n",
            "    accuracy                           0.58       204\n",
            "   macro avg       0.63      0.53      0.45       204\n",
            "weighted avg       0.63      0.58      0.48       204\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('arousal', 'knn')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoKgtLNEpU-v",
        "outputId": "5fba615a-dcb5-4f19-caa6-5cdc9c1b9487"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    62.75    60.78     60.29      62.25\n",
            "alpha    65.69    64.22     64.22      59.31\n",
            "beta     61.76    56.86     63.24      55.39\n",
            "gamma    62.25    61.27     60.29      56.37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('valence', 'knn')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gvh7GFZNmhPm",
        "outputId": "c54c5010-a660-469c-fb2d-7ef38b750d3f"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    57.84    58.33     51.47      58.33\n",
            "alpha    55.39    57.84     59.31      53.92\n",
            "beta     58.82    57.84     61.76      55.39\n",
            "gamma    64.22    56.86     61.76      54.41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_conf(\"gamma\",\"occipital\",\"arousal\",\"knn\")"
      ],
      "metadata": {
        "id": "ZmVLpd2urdd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_conf(\"beta\",\"occipital\",\"valence\",\"knn\")"
      ],
      "metadata": {
        "id": "nv1bzJw1riy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('arousal', 'dtree')"
      ],
      "metadata": {
        "id": "pXS8oiqJKknT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbc193bc-b9d8-40af-d344-a07373e8731c"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    59.31    60.78     52.45      61.27\n",
            "alpha    59.31    59.31     56.86      51.47\n",
            "beta     58.33    53.92     51.47      60.29\n",
            "gamma    58.33    56.86     56.86      59.31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('valence', 'dtree')"
      ],
      "metadata": {
        "id": "zPn1ljyNKtr4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30fe00e4-54d5-4dfe-fcda-6b299e074f8c"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    49.02    57.35     52.94      52.45\n",
            "alpha    57.35    54.90     57.84      57.35\n",
            "beta     50.49    52.94     55.88      59.31\n",
            "gamma    59.31    57.35     58.82      55.39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_conf(\"alpha\",\"central\",\"arousal\",\"dtree\")"
      ],
      "metadata": {
        "id": "agQ2RhXIrr07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_conf(\"alpha\",\"frontal\",\"valence\",\"dtree\")"
      ],
      "metadata": {
        "id": "vK5dRNhRrx9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('arousal', 'rf')"
      ],
      "metadata": {
        "id": "zS_EF8dRKyLm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6577a411-38e0-44ab-f040-6b7db209e1a2"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    61.76    60.78     59.31      62.75\n",
            "alpha    61.76    61.76     58.82      58.82\n",
            "beta     58.82    61.27     63.73      59.31\n",
            "gamma    63.73    65.20     61.76      63.73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('valence', 'rf')"
      ],
      "metadata": {
        "id": "GNTGFHQOK14l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12a594d2-3f70-4ddc-b004-b2ca68e51859"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    58.82    59.31     57.35      55.88\n",
            "alpha    56.86    59.80     63.73      55.39\n",
            "beta     57.35    63.24     61.76      62.25\n",
            "gamma    63.73    57.84     58.82      62.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_conf(\"gamma\",\"frontal\",\"arousal\",\"rf\")"
      ],
      "metadata": {
        "id": "RlyoQzNNr6IX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_conf(\"gamma\",\"frontal\",\"valence\",\"rf\")"
      ],
      "metadata": {
        "id": "B5ztZl6yr_-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('arousal', 'nb')"
      ],
      "metadata": {
        "id": "MkmagqFfK54a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2584c41b-2858-4bcb-fd31-9913fd1c3c82"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    47.55    52.94     50.00      51.47\n",
            "alpha    49.02    52.94     50.98      51.47\n",
            "beta     48.04    50.98     54.41      47.55\n",
            "gamma    49.02    52.45     51.47      47.55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('valence', 'nb')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uV3gnYCKK-cA",
        "outputId": "a4186d54-4875-49ca-9583-5354d1830dab"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    56.86    48.04     55.88      56.37\n",
            "alpha    55.39    47.55     48.53      57.35\n",
            "beta     54.90    49.51     47.55      55.39\n",
            "gamma    56.37    56.86     54.90      57.84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_conf(\"alpha\",\"central\",\"arousal\",\"nb\")"
      ],
      "metadata": {
        "id": "i-sKlcGIsE8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_conf(\"gamma\",\"occipital\",\"valence\",\"nb\")"
      ],
      "metadata": {
        "id": "kpTwFKuDsPkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('arousal', 'mlp')"
      ],
      "metadata": {
        "id": "02_ynnQ1LEoz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11fa734f-8d35-4b4c-97a8-dfa7e1e755ba"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    51.96    53.92     51.47      53.43\n",
            "alpha    52.94    50.98     49.51      54.90\n",
            "beta     52.45    49.51     49.51      54.41\n",
            "gamma    53.92    50.00     49.51      57.84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('valence', 'mlp')"
      ],
      "metadata": {
        "id": "tuwCpDS1LHAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a01b94b7-5661-43fe-ac0b-a243c2454872"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    55.39    55.39     57.84      57.84\n",
            "alpha    54.90    55.39     55.88      57.35\n",
            "beta     54.41    54.41     55.88      56.86\n",
            "gamma    52.94    55.88     56.37      56.37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_conf(\"beta\",\"occipital\",\"arousal\",\"mlp\")"
      ],
      "metadata": {
        "id": "2_NYxBBpsaIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_conf(\"beta\",\"occipital\",\"valence\",\"mlp\")"
      ],
      "metadata": {
        "id": "o_3A_6_qsiXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('arousal', 'ab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6m5VzRLmx03",
        "outputId": "06c1f974-8c0b-4ed3-de01-d63385c94be1"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    60.78    59.31     57.84      60.29\n",
            "alpha    61.27    59.80     53.43      56.86\n",
            "beta     64.22    56.37     58.33      56.86\n",
            "gamma    60.29    55.39     57.84      65.69\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('valence', 'ab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLRvg_tSm1LS",
        "outputId": "47d1d990-6d16-4728-eb9f-49d1deef229e"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    50.49    55.39     56.86      55.88\n",
            "alpha    53.92    56.37     57.84      54.90\n",
            "beta     56.86    56.37     56.86      57.84\n",
            "gamma    56.37    55.88     50.98      55.88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_conf(\"gamma\",\"occipital\",\"arousal\",\"ab\")"
      ],
      "metadata": {
        "id": "s6dcAwL9tlyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_conf(\"beta\",\"occipital\",\"valence\",\"ab\")"
      ],
      "metadata": {
        "id": "UyGv7Rdftuf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('arousal', 'xgb')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYyDGTbHm3xs",
        "outputId": "5a817a21-b104-47ae-c5ec-44639f3fe275"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    61.27    59.31     56.86      59.31\n",
            "alpha    62.75    59.80     63.24      57.84\n",
            "beta     62.25    56.86     61.76      57.35\n",
            "gamma    63.73    59.80     55.39      61.27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('valence', 'xgb')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_3vjLgDm5sQ",
        "outputId": "b581ad9a-a40a-498c-aee6-c668072a3523"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    54.90    56.37     56.86      57.84\n",
            "alpha    55.39    64.22     62.25      57.35\n",
            "beta     58.82    65.69     58.82      59.80\n",
            "gamma    57.84    58.33     55.88      59.80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_conf(\"gamma\",\"frontal\",\"arousal\",\"xgb\")"
      ],
      "metadata": {
        "id": "C-i2fQCCt1p7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_conf(\"beta\",\"central\",\"valence\",\"xgb\")"
      ],
      "metadata": {
        "id": "HOO4BhUxt7RD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('arousal', 'lgbm')"
      ],
      "metadata": {
        "id": "tAvwS8OmOkX6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7e21cb7-78a8-4e42-ffaf-5aefb71e8c30"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 407, number of negative: 413\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000159 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496341 -> initscore=-0.014634\n",
            "[LightGBM] [Info] Start training from score -0.014634\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 407, number of negative: 413\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000132 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1785\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 7\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496341 -> initscore=-0.014634\n",
            "[LightGBM] [Info] Start training from score -0.014634\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 407, number of negative: 413\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000100 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496341 -> initscore=-0.014634\n",
            "[LightGBM] [Info] Start training from score -0.014634\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 407, number of negative: 413\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000101 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496341 -> initscore=-0.014634\n",
            "[LightGBM] [Info] Start training from score -0.014634\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 407, number of negative: 413\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000101 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496341 -> initscore=-0.014634\n",
            "[LightGBM] [Info] Start training from score -0.014634\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 407, number of negative: 413\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000147 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1785\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 7\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496341 -> initscore=-0.014634\n",
            "[LightGBM] [Info] Start training from score -0.014634\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 407, number of negative: 413\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000111 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496341 -> initscore=-0.014634\n",
            "[LightGBM] [Info] Start training from score -0.014634\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 407, number of negative: 413\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000115 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496341 -> initscore=-0.014634\n",
            "[LightGBM] [Info] Start training from score -0.014634\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 407, number of negative: 413\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000101 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496341 -> initscore=-0.014634\n",
            "[LightGBM] [Info] Start training from score -0.014634\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 407, number of negative: 413\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000141 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1785\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 7\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496341 -> initscore=-0.014634\n",
            "[LightGBM] [Info] Start training from score -0.014634\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 407, number of negative: 413\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000107 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496341 -> initscore=-0.014634\n",
            "[LightGBM] [Info] Start training from score -0.014634\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 407, number of negative: 413\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000105 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496341 -> initscore=-0.014634\n",
            "[LightGBM] [Info] Start training from score -0.014634\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 407, number of negative: 413\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000102 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496341 -> initscore=-0.014634\n",
            "[LightGBM] [Info] Start training from score -0.014634\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 407, number of negative: 413\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000153 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1785\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 7\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496341 -> initscore=-0.014634\n",
            "[LightGBM] [Info] Start training from score -0.014634\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 407, number of negative: 413\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000116 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496341 -> initscore=-0.014634\n",
            "[LightGBM] [Info] Start training from score -0.014634\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 407, number of negative: 413\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000111 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496341 -> initscore=-0.014634\n",
            "[LightGBM] [Info] Start training from score -0.014634\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "       frontal  central  parietal  occipital\n",
            "theta    60.29    60.78     54.41      60.78\n",
            "alpha    63.24    60.29     62.25      60.29\n",
            "beta     61.27    57.35     57.84      58.33\n",
            "gamma    59.31    61.76     58.82      62.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('valence', 'lgbm')"
      ],
      "metadata": {
        "id": "hIwlFJ7nOyFX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b5c1767-5af3-4637-d904-ce1d44d28dc9"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 426, number of negative: 394\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000104 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.519512 -> initscore=0.078088\n",
            "[LightGBM] [Info] Start training from score 0.078088\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 426, number of negative: 394\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000137 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1785\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 7\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.519512 -> initscore=0.078088\n",
            "[LightGBM] [Info] Start training from score 0.078088\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 426, number of negative: 394\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000104 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.519512 -> initscore=0.078088\n",
            "[LightGBM] [Info] Start training from score 0.078088\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 426, number of negative: 394\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000101 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.519512 -> initscore=0.078088\n",
            "[LightGBM] [Info] Start training from score 0.078088\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 426, number of negative: 394\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000104 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.519512 -> initscore=0.078088\n",
            "[LightGBM] [Info] Start training from score 0.078088\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 426, number of negative: 394\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000138 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1785\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 7\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.519512 -> initscore=0.078088\n",
            "[LightGBM] [Info] Start training from score 0.078088\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 426, number of negative: 394\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000101 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.519512 -> initscore=0.078088\n",
            "[LightGBM] [Info] Start training from score 0.078088\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 426, number of negative: 394\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000115 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.519512 -> initscore=0.078088\n",
            "[LightGBM] [Info] Start training from score 0.078088\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 426, number of negative: 394\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000108 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.519512 -> initscore=0.078088\n",
            "[LightGBM] [Info] Start training from score 0.078088\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 426, number of negative: 394\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000132 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1785\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 7\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.519512 -> initscore=0.078088\n",
            "[LightGBM] [Info] Start training from score 0.078088\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 426, number of negative: 394\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000107 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.519512 -> initscore=0.078088\n",
            "[LightGBM] [Info] Start training from score 0.078088\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 426, number of negative: 394\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000109 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.519512 -> initscore=0.078088\n",
            "[LightGBM] [Info] Start training from score 0.078088\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 426, number of negative: 394\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000105 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.519512 -> initscore=0.078088\n",
            "[LightGBM] [Info] Start training from score 0.078088\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 426, number of negative: 394\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000152 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1785\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 7\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.519512 -> initscore=0.078088\n",
            "[LightGBM] [Info] Start training from score 0.078088\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 426, number of negative: 394\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000100 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.519512 -> initscore=0.078088\n",
            "[LightGBM] [Info] Start training from score 0.078088\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 426, number of negative: 394\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000105 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1275\n",
            "[LightGBM] [Info] Number of data points in the train set: 820, number of used features: 5\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.519512 -> initscore=0.078088\n",
            "[LightGBM] [Info] Start training from score 0.078088\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "       frontal  central  parietal  occipital\n",
            "theta    55.88    58.33     56.86      60.29\n",
            "alpha    58.33    57.35     59.31      55.39\n",
            "beta     56.37    58.82     57.35      58.33\n",
            "gamma    63.73    59.31     53.92      59.31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_conf(\"alpha\",\"frontal\",\"arousal\",\"lgbm\")"
      ],
      "metadata": {
        "id": "BmyvglkuuDvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_conf(\"gamma\",\"frontal\",\"valence\",\"lgbm\")"
      ],
      "metadata": {
        "id": "jwfTcdf6uIpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('arousal', 'gpc')"
      ],
      "metadata": {
        "id": "QBw1c8r9O6s5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86bec518-fd20-4f8d-806e-26122cb59052"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    57.84    52.45     61.76      52.45\n",
            "alpha    65.20    51.47     65.20      53.43\n",
            "beta     60.29    58.33     62.25      55.88\n",
            "gamma    63.73    55.88     59.31      63.73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('valence', 'gpc')"
      ],
      "metadata": {
        "id": "_mBzYOJwO-Tc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4980fd70-156f-4645-b33d-0fd060c92a13"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    55.88    55.88     54.41      54.41\n",
            "alpha    56.86    56.37     61.76      55.88\n",
            "beta     56.37    61.76     63.24      56.86\n",
            "gamma    56.86    63.73     52.94      57.35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_conf(\"alpha\",\"frontal\",\"arousal\",\"gpc\")"
      ],
      "metadata": {
        "id": "cZ9Vp4TquTvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_conf(\"gamma\",\"central\",\"valence\",\"gpc\")"
      ],
      "metadata": {
        "id": "OSCvLe01ubBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('arousal', 'per')"
      ],
      "metadata": {
        "id": "J_tYtr1vPEwA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4745a21d-f8fc-495b-d2b2-ba0a5d812b07"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    52.45    48.53     53.43      50.00\n",
            "alpha    49.02    50.00     50.49      50.49\n",
            "beta     50.98    50.49     51.47      50.49\n",
            "gamma    50.49    57.84     52.45      50.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('valence', 'per')"
      ],
      "metadata": {
        "id": "Yz0L94FJPKCx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9929ff4-cfbf-41d7-895b-d9fe07aa01d8"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    43.63    47.06     48.53      43.14\n",
            "alpha    43.14    49.02     48.53      44.12\n",
            "beta     54.41    47.55     44.12      43.63\n",
            "gamma    44.12    51.47     45.10      46.08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_conf(\"gamma\",\"central\",\"arousal\",\"per\")"
      ],
      "metadata": {
        "id": "cyIi_JvQuhpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_conf(\"beta\",\"frontal\",\"valence\",\"per\")"
      ],
      "metadata": {
        "id": "Yc7X2g5yuoox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('arousal', 'cb')"
      ],
      "metadata": {
        "id": "OPkBIcPtPTXf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac83a060-a001-48d9-8849-3eb2a4fb2d90"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    60.78    59.31     52.94      62.25\n",
            "alpha    65.20    63.73     57.84      62.25\n",
            "beta     60.29    54.90     56.86      55.88\n",
            "gamma    66.67    59.31     60.29      64.71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy('valence', 'cb')"
      ],
      "metadata": {
        "id": "7LB1NnMQQcTD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e869548-10c3-4b11-fb69-047b4f09e0dd"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       frontal  central  parietal  occipital\n",
            "theta    54.90    56.37     55.88      61.27\n",
            "alpha    55.88    58.33     56.37      55.88\n",
            "beta     60.78    61.27     59.31      62.75\n",
            "gamma    58.82    58.33     57.84      59.80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_conf(\"gamma\",\"frontal\",\"arousal\",\"cb\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "hj-4ynmYnH5u",
        "outputId": "ca7f152f-4f06-439d-8343-af6c839b2c58"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x200 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.71      0.68       101\n",
            "           1       0.69      0.62      0.65       103\n",
            "\n",
            "    accuracy                           0.67       204\n",
            "   macro avg       0.67      0.67      0.67       204\n",
            "weighted avg       0.67      0.67      0.67       204\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_conf(\"beta\",\"occipital\",\"valence\",\"cb\")"
      ],
      "metadata": {
        "id": "h8eEMKLpu1f6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy(\"arousal\",\"cnn\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGRCBmwTnIsx",
        "outputId": "8a4bb8eb-a093-459d-c9cb-b7370a08d18f"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 15ms/step - loss: 0.8550 - accuracy: 0.5244 - val_loss: 0.7185 - val_accuracy: 0.4922\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7565 - accuracy: 0.5479 - val_loss: 0.6907 - val_accuracy: 0.4922\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7793 - accuracy: 0.5068 - val_loss: 0.7074 - val_accuracy: 0.5039\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7793 - accuracy: 0.4951 - val_loss: 0.6988 - val_accuracy: 0.5039\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7382 - accuracy: 0.5137 - val_loss: 0.7092 - val_accuracy: 0.4922\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7259 - accuracy: 0.5176 - val_loss: 0.7100 - val_accuracy: 0.5039\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7326 - accuracy: 0.5244 - val_loss: 0.6972 - val_accuracy: 0.4805\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7496 - accuracy: 0.4893 - val_loss: 0.7036 - val_accuracy: 0.4883\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7289 - accuracy: 0.5010 - val_loss: 0.6987 - val_accuracy: 0.4805\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7248 - accuracy: 0.5156 - val_loss: 0.6905 - val_accuracy: 0.4648\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7313 - accuracy: 0.4951 - val_loss: 0.6955 - val_accuracy: 0.5000\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7390 - accuracy: 0.4893 - val_loss: 0.6961 - val_accuracy: 0.5156\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7235 - accuracy: 0.4902 - val_loss: 0.6948 - val_accuracy: 0.4727\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7209 - accuracy: 0.4844 - val_loss: 0.6938 - val_accuracy: 0.4883\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7126 - accuracy: 0.4951 - val_loss: 0.7081 - val_accuracy: 0.4258\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7148 - accuracy: 0.5107 - val_loss: 0.7031 - val_accuracy: 0.4844\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7192 - accuracy: 0.5098 - val_loss: 0.6896 - val_accuracy: 0.4883\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7169 - accuracy: 0.4941 - val_loss: 0.6916 - val_accuracy: 0.5273\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7111 - accuracy: 0.5107 - val_loss: 0.6988 - val_accuracy: 0.4961\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6992 - accuracy: 0.5059 - val_loss: 0.7004 - val_accuracy: 0.4844\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 22ms/step - loss: 1.0565 - accuracy: 0.5020 - val_loss: 1.0129 - val_accuracy: 0.4922\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7733 - accuracy: 0.4922 - val_loss: 0.7766 - val_accuracy: 0.4922\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7721 - accuracy: 0.5078 - val_loss: 0.7510 - val_accuracy: 0.4922\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7275 - accuracy: 0.5176 - val_loss: 0.7319 - val_accuracy: 0.4922\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7278 - accuracy: 0.5127 - val_loss: 0.7180 - val_accuracy: 0.4922\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7323 - accuracy: 0.4990 - val_loss: 0.7081 - val_accuracy: 0.4922\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6988 - accuracy: 0.5557 - val_loss: 0.7203 - val_accuracy: 0.4922\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7108 - accuracy: 0.5234 - val_loss: 0.7017 - val_accuracy: 0.4922\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7181 - accuracy: 0.5088 - val_loss: 0.6976 - val_accuracy: 0.4922\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7164 - accuracy: 0.5029 - val_loss: 0.7001 - val_accuracy: 0.4922\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7039 - accuracy: 0.5059 - val_loss: 0.7004 - val_accuracy: 0.4961\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7159 - accuracy: 0.5186 - val_loss: 0.7074 - val_accuracy: 0.4688\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6990 - accuracy: 0.5107 - val_loss: 0.7052 - val_accuracy: 0.4922\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7178 - accuracy: 0.4912 - val_loss: 0.7005 - val_accuracy: 0.4883\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7095 - accuracy: 0.5176 - val_loss: 0.6953 - val_accuracy: 0.5234\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7089 - accuracy: 0.5352 - val_loss: 0.6950 - val_accuracy: 0.5234\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6986 - accuracy: 0.5264 - val_loss: 0.6970 - val_accuracy: 0.5273\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6908 - accuracy: 0.5332 - val_loss: 0.6988 - val_accuracy: 0.4805\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7050 - accuracy: 0.5049 - val_loss: 0.6995 - val_accuracy: 0.5078\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7037 - accuracy: 0.5215 - val_loss: 0.6976 - val_accuracy: 0.5117\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 14ms/step - loss: 0.8372 - accuracy: 0.4834 - val_loss: 0.7208 - val_accuracy: 0.4375\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7841 - accuracy: 0.5127 - val_loss: 0.7446 - val_accuracy: 0.4805\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7619 - accuracy: 0.5049 - val_loss: 0.6836 - val_accuracy: 0.5508\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7384 - accuracy: 0.5205 - val_loss: 0.6823 - val_accuracy: 0.5273\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7331 - accuracy: 0.4990 - val_loss: 0.6879 - val_accuracy: 0.5117\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7487 - accuracy: 0.5020 - val_loss: 0.6857 - val_accuracy: 0.5430\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7270 - accuracy: 0.5430 - val_loss: 0.7027 - val_accuracy: 0.5273\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7171 - accuracy: 0.5449 - val_loss: 0.7440 - val_accuracy: 0.5547\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7416 - accuracy: 0.5029 - val_loss: 0.6833 - val_accuracy: 0.5508\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7276 - accuracy: 0.4941 - val_loss: 0.7020 - val_accuracy: 0.5195\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7069 - accuracy: 0.5244 - val_loss: 0.6917 - val_accuracy: 0.5312\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7144 - accuracy: 0.5332 - val_loss: 0.6912 - val_accuracy: 0.5391\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7191 - accuracy: 0.5254 - val_loss: 0.6838 - val_accuracy: 0.5469\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7239 - accuracy: 0.4951 - val_loss: 0.6892 - val_accuracy: 0.5391\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7101 - accuracy: 0.5195 - val_loss: 0.6784 - val_accuracy: 0.5938\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7054 - accuracy: 0.5293 - val_loss: 0.6762 - val_accuracy: 0.5547\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7076 - accuracy: 0.5293 - val_loss: 0.7082 - val_accuracy: 0.5234\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6949 - accuracy: 0.5127 - val_loss: 0.6917 - val_accuracy: 0.5469\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7100 - accuracy: 0.4990 - val_loss: 0.6868 - val_accuracy: 0.5547\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7058 - accuracy: 0.5186 - val_loss: 0.6919 - val_accuracy: 0.5352\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 21ms/step - loss: 0.8072 - accuracy: 0.4980 - val_loss: 0.6843 - val_accuracy: 0.5273\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7684 - accuracy: 0.5195 - val_loss: 0.6855 - val_accuracy: 0.5156\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7615 - accuracy: 0.5127 - val_loss: 0.6829 - val_accuracy: 0.5195\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7610 - accuracy: 0.4756 - val_loss: 0.6891 - val_accuracy: 0.5039\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7271 - accuracy: 0.5137 - val_loss: 0.6832 - val_accuracy: 0.5547\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7258 - accuracy: 0.5176 - val_loss: 0.7113 - val_accuracy: 0.4922\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7546 - accuracy: 0.4717 - val_loss: 0.6892 - val_accuracy: 0.4961\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7404 - accuracy: 0.4902 - val_loss: 0.6831 - val_accuracy: 0.5117\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7229 - accuracy: 0.4941 - val_loss: 0.6918 - val_accuracy: 0.4727\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7233 - accuracy: 0.5254 - val_loss: 0.6854 - val_accuracy: 0.5156\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7301 - accuracy: 0.5059 - val_loss: 0.6992 - val_accuracy: 0.4727\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7070 - accuracy: 0.5000 - val_loss: 0.6917 - val_accuracy: 0.4961\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7189 - accuracy: 0.5166 - val_loss: 0.7118 - val_accuracy: 0.4961\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7261 - accuracy: 0.4922 - val_loss: 0.6912 - val_accuracy: 0.5312\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7076 - accuracy: 0.5303 - val_loss: 0.7071 - val_accuracy: 0.5469\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7123 - accuracy: 0.4932 - val_loss: 0.6922 - val_accuracy: 0.5078\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7094 - accuracy: 0.4941 - val_loss: 0.7061 - val_accuracy: 0.4922\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7079 - accuracy: 0.5049 - val_loss: 0.7016 - val_accuracy: 0.4648\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7071 - accuracy: 0.5117 - val_loss: 0.6913 - val_accuracy: 0.5352\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7024 - accuracy: 0.5225 - val_loss: 0.6970 - val_accuracy: 0.5469\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 4s 16ms/step - loss: 0.8211 - accuracy: 0.5010 - val_loss: 0.6927 - val_accuracy: 0.4961\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7974 - accuracy: 0.4707 - val_loss: 0.7407 - val_accuracy: 0.5078\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7412 - accuracy: 0.5020 - val_loss: 0.7177 - val_accuracy: 0.5078\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7289 - accuracy: 0.5088 - val_loss: 0.7524 - val_accuracy: 0.5039\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7398 - accuracy: 0.5146 - val_loss: 0.6928 - val_accuracy: 0.5039\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7543 - accuracy: 0.4971 - val_loss: 0.6903 - val_accuracy: 0.4727\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7251 - accuracy: 0.5010 - val_loss: 0.6897 - val_accuracy: 0.5039\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7573 - accuracy: 0.4902 - val_loss: 0.7502 - val_accuracy: 0.5195\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7363 - accuracy: 0.5049 - val_loss: 0.7203 - val_accuracy: 0.4883\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7189 - accuracy: 0.5078 - val_loss: 0.6957 - val_accuracy: 0.4883\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7251 - accuracy: 0.5000 - val_loss: 0.6911 - val_accuracy: 0.5195\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7104 - accuracy: 0.5186 - val_loss: 0.7010 - val_accuracy: 0.4844\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7332 - accuracy: 0.4902 - val_loss: 0.6881 - val_accuracy: 0.5156\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7076 - accuracy: 0.5137 - val_loss: 0.6949 - val_accuracy: 0.5156\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7090 - accuracy: 0.5029 - val_loss: 0.7067 - val_accuracy: 0.4883\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7055 - accuracy: 0.5195 - val_loss: 0.6938 - val_accuracy: 0.4922\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7126 - accuracy: 0.5000 - val_loss: 0.7051 - val_accuracy: 0.4922\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7135 - accuracy: 0.5176 - val_loss: 0.6984 - val_accuracy: 0.4375\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7262 - accuracy: 0.5107 - val_loss: 0.6958 - val_accuracy: 0.4648\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7099 - accuracy: 0.4834 - val_loss: 0.6981 - val_accuracy: 0.4766\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 14ms/step - loss: 0.8885 - accuracy: 0.4854 - val_loss: 0.7638 - val_accuracy: 0.5039\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.8192 - accuracy: 0.5000 - val_loss: 0.7241 - val_accuracy: 0.5352\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7730 - accuracy: 0.5186 - val_loss: 0.6945 - val_accuracy: 0.5391\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7774 - accuracy: 0.5254 - val_loss: 0.6957 - val_accuracy: 0.5352\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7639 - accuracy: 0.4912 - val_loss: 0.6970 - val_accuracy: 0.5430\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7628 - accuracy: 0.4990 - val_loss: 0.7051 - val_accuracy: 0.5156\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7571 - accuracy: 0.4814 - val_loss: 0.7030 - val_accuracy: 0.5312\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7295 - accuracy: 0.5166 - val_loss: 0.7032 - val_accuracy: 0.5273\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7243 - accuracy: 0.5059 - val_loss: 0.7032 - val_accuracy: 0.5039\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7506 - accuracy: 0.4834 - val_loss: 0.7038 - val_accuracy: 0.5312\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7233 - accuracy: 0.5176 - val_loss: 0.6991 - val_accuracy: 0.5156\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7163 - accuracy: 0.5273 - val_loss: 0.7046 - val_accuracy: 0.5117\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7341 - accuracy: 0.4854 - val_loss: 0.7217 - val_accuracy: 0.5547\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7185 - accuracy: 0.5195 - val_loss: 0.7112 - val_accuracy: 0.5195\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7145 - accuracy: 0.5088 - val_loss: 0.7036 - val_accuracy: 0.5391\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7072 - accuracy: 0.5244 - val_loss: 0.7084 - val_accuracy: 0.5273\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7196 - accuracy: 0.5068 - val_loss: 0.7111 - val_accuracy: 0.5234\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7133 - accuracy: 0.5254 - val_loss: 0.7049 - val_accuracy: 0.5391\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7108 - accuracy: 0.5039 - val_loss: 0.6995 - val_accuracy: 0.5273\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7024 - accuracy: 0.5215 - val_loss: 0.7019 - val_accuracy: 0.5352\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 14ms/step - loss: 0.8465 - accuracy: 0.5049 - val_loss: 0.8064 - val_accuracy: 0.5078\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.8197 - accuracy: 0.4971 - val_loss: 0.7106 - val_accuracy: 0.5078\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7628 - accuracy: 0.5068 - val_loss: 0.6945 - val_accuracy: 0.4844\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7671 - accuracy: 0.5020 - val_loss: 0.6856 - val_accuracy: 0.5156\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7601 - accuracy: 0.5029 - val_loss: 0.6822 - val_accuracy: 0.5156\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7402 - accuracy: 0.5010 - val_loss: 0.6891 - val_accuracy: 0.5547\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7147 - accuracy: 0.5283 - val_loss: 0.6786 - val_accuracy: 0.5469\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7164 - accuracy: 0.5068 - val_loss: 0.6805 - val_accuracy: 0.5547\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7234 - accuracy: 0.5137 - val_loss: 0.6787 - val_accuracy: 0.5898\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7303 - accuracy: 0.5068 - val_loss: 0.6789 - val_accuracy: 0.5469\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7057 - accuracy: 0.5205 - val_loss: 0.6818 - val_accuracy: 0.5586\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7160 - accuracy: 0.5088 - val_loss: 0.7008 - val_accuracy: 0.5352\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7175 - accuracy: 0.4961 - val_loss: 0.6956 - val_accuracy: 0.5273\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7213 - accuracy: 0.4912 - val_loss: 0.6846 - val_accuracy: 0.5312\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7183 - accuracy: 0.5088 - val_loss: 0.6854 - val_accuracy: 0.5352\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6984 - accuracy: 0.5596 - val_loss: 0.6931 - val_accuracy: 0.5312\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7109 - accuracy: 0.5039 - val_loss: 0.6825 - val_accuracy: 0.5547\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7000 - accuracy: 0.5195 - val_loss: 0.6887 - val_accuracy: 0.5586\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7001 - accuracy: 0.5273 - val_loss: 0.6832 - val_accuracy: 0.5352\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7048 - accuracy: 0.5215 - val_loss: 0.6850 - val_accuracy: 0.5430\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 20ms/step - loss: 0.8443 - accuracy: 0.4961 - val_loss: 0.7042 - val_accuracy: 0.5391\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.8555 - accuracy: 0.4736 - val_loss: 0.6849 - val_accuracy: 0.5430\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.8175 - accuracy: 0.5127 - val_loss: 0.6886 - val_accuracy: 0.5508\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7896 - accuracy: 0.5146 - val_loss: 0.6853 - val_accuracy: 0.5312\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7511 - accuracy: 0.5146 - val_loss: 0.6888 - val_accuracy: 0.5586\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7656 - accuracy: 0.4990 - val_loss: 0.6900 - val_accuracy: 0.5508\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7555 - accuracy: 0.5088 - val_loss: 0.6876 - val_accuracy: 0.5352\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7591 - accuracy: 0.5205 - val_loss: 0.6860 - val_accuracy: 0.5312\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7749 - accuracy: 0.5078 - val_loss: 0.6880 - val_accuracy: 0.5117\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7415 - accuracy: 0.5186 - val_loss: 0.6897 - val_accuracy: 0.5156\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7303 - accuracy: 0.5059 - val_loss: 0.6935 - val_accuracy: 0.5195\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7196 - accuracy: 0.5107 - val_loss: 0.6923 - val_accuracy: 0.5039\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7193 - accuracy: 0.5283 - val_loss: 0.6917 - val_accuracy: 0.5078\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7145 - accuracy: 0.5195 - val_loss: 0.6991 - val_accuracy: 0.5000\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7227 - accuracy: 0.5049 - val_loss: 0.6968 - val_accuracy: 0.5117\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7139 - accuracy: 0.5166 - val_loss: 0.6939 - val_accuracy: 0.5117\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7222 - accuracy: 0.5068 - val_loss: 0.6944 - val_accuracy: 0.5078\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7030 - accuracy: 0.5322 - val_loss: 0.6961 - val_accuracy: 0.5156\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7158 - accuracy: 0.5117 - val_loss: 0.6916 - val_accuracy: 0.5156\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7015 - accuracy: 0.5195 - val_loss: 0.6983 - val_accuracy: 0.5078\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 13ms/step - loss: 0.8436 - accuracy: 0.5020 - val_loss: 0.8658 - val_accuracy: 0.5078\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7835 - accuracy: 0.5156 - val_loss: 0.7318 - val_accuracy: 0.5078\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7893 - accuracy: 0.4756 - val_loss: 0.7122 - val_accuracy: 0.5078\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7378 - accuracy: 0.5088 - val_loss: 0.6927 - val_accuracy: 0.5117\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7404 - accuracy: 0.5352 - val_loss: 0.7008 - val_accuracy: 0.5312\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7447 - accuracy: 0.5068 - val_loss: 0.6903 - val_accuracy: 0.5195\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7391 - accuracy: 0.5186 - val_loss: 0.6963 - val_accuracy: 0.5039\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7237 - accuracy: 0.4873 - val_loss: 0.6885 - val_accuracy: 0.4961\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7374 - accuracy: 0.4912 - val_loss: 0.6939 - val_accuracy: 0.5469\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7271 - accuracy: 0.4951 - val_loss: 0.6876 - val_accuracy: 0.5000\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7233 - accuracy: 0.4912 - val_loss: 0.6926 - val_accuracy: 0.5039\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7024 - accuracy: 0.5479 - val_loss: 0.7046 - val_accuracy: 0.5469\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7141 - accuracy: 0.5020 - val_loss: 0.6932 - val_accuracy: 0.5039\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7141 - accuracy: 0.5059 - val_loss: 0.6920 - val_accuracy: 0.5195\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7122 - accuracy: 0.5127 - val_loss: 0.7052 - val_accuracy: 0.5469\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7046 - accuracy: 0.5225 - val_loss: 0.7038 - val_accuracy: 0.5039\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7052 - accuracy: 0.5186 - val_loss: 0.6938 - val_accuracy: 0.5430\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6979 - accuracy: 0.5107 - val_loss: 0.7073 - val_accuracy: 0.5547\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7247 - accuracy: 0.4932 - val_loss: 0.6911 - val_accuracy: 0.5703\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7071 - accuracy: 0.4873 - val_loss: 0.6892 - val_accuracy: 0.5547\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 14ms/step - loss: 0.7931 - accuracy: 0.5127 - val_loss: 0.6985 - val_accuracy: 0.4727\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7887 - accuracy: 0.4805 - val_loss: 0.7131 - val_accuracy: 0.4922\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7693 - accuracy: 0.4922 - val_loss: 0.7185 - val_accuracy: 0.4922\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7621 - accuracy: 0.5195 - val_loss: 0.6934 - val_accuracy: 0.4961\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7260 - accuracy: 0.5361 - val_loss: 0.6968 - val_accuracy: 0.4922\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7662 - accuracy: 0.5029 - val_loss: 0.7010 - val_accuracy: 0.4922\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7226 - accuracy: 0.5127 - val_loss: 0.7061 - val_accuracy: 0.4922\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7213 - accuracy: 0.4873 - val_loss: 0.6939 - val_accuracy: 0.5000\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7330 - accuracy: 0.5059 - val_loss: 0.6934 - val_accuracy: 0.5117\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7183 - accuracy: 0.5195 - val_loss: 0.6905 - val_accuracy: 0.5156\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7305 - accuracy: 0.4697 - val_loss: 0.7069 - val_accuracy: 0.5156\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7187 - accuracy: 0.5039 - val_loss: 0.6904 - val_accuracy: 0.5156\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7140 - accuracy: 0.4951 - val_loss: 0.6885 - val_accuracy: 0.5195\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7109 - accuracy: 0.5166 - val_loss: 0.6924 - val_accuracy: 0.4961\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7019 - accuracy: 0.5068 - val_loss: 0.6929 - val_accuracy: 0.5547\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6997 - accuracy: 0.5078 - val_loss: 0.6925 - val_accuracy: 0.5352\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7084 - accuracy: 0.5264 - val_loss: 0.6903 - val_accuracy: 0.5391\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6946 - accuracy: 0.5459 - val_loss: 0.6903 - val_accuracy: 0.5000\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7095 - accuracy: 0.5117 - val_loss: 0.6923 - val_accuracy: 0.5430\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6930 - accuracy: 0.5352 - val_loss: 0.6905 - val_accuracy: 0.5039\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 14ms/step - loss: 0.8359 - accuracy: 0.4854 - val_loss: 0.7660 - val_accuracy: 0.4570\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7788 - accuracy: 0.4902 - val_loss: 0.6914 - val_accuracy: 0.5391\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7376 - accuracy: 0.5166 - val_loss: 0.6949 - val_accuracy: 0.5117\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7444 - accuracy: 0.4766 - val_loss: 0.7028 - val_accuracy: 0.5234\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7518 - accuracy: 0.5088 - val_loss: 0.7040 - val_accuracy: 0.5156\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7384 - accuracy: 0.4893 - val_loss: 0.7127 - val_accuracy: 0.5000\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7231 - accuracy: 0.4990 - val_loss: 0.6931 - val_accuracy: 0.5273\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7088 - accuracy: 0.5029 - val_loss: 0.6874 - val_accuracy: 0.5234\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7109 - accuracy: 0.5234 - val_loss: 0.6844 - val_accuracy: 0.5469\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7154 - accuracy: 0.5078 - val_loss: 0.6960 - val_accuracy: 0.5273\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7084 - accuracy: 0.5107 - val_loss: 0.6960 - val_accuracy: 0.5469\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7161 - accuracy: 0.4961 - val_loss: 0.6966 - val_accuracy: 0.5430\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7134 - accuracy: 0.5254 - val_loss: 0.6851 - val_accuracy: 0.5391\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7091 - accuracy: 0.5176 - val_loss: 0.6884 - val_accuracy: 0.5312\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7063 - accuracy: 0.5000 - val_loss: 0.6924 - val_accuracy: 0.5430\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6978 - accuracy: 0.5195 - val_loss: 0.6807 - val_accuracy: 0.5859\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7043 - accuracy: 0.5176 - val_loss: 0.6844 - val_accuracy: 0.5742\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6981 - accuracy: 0.5146 - val_loss: 0.6873 - val_accuracy: 0.5586\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6999 - accuracy: 0.5205 - val_loss: 0.6899 - val_accuracy: 0.5781\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6989 - accuracy: 0.4961 - val_loss: 0.6869 - val_accuracy: 0.5391\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 13ms/step - loss: 0.8637 - accuracy: 0.5107 - val_loss: 0.6883 - val_accuracy: 0.5664\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7536 - accuracy: 0.5117 - val_loss: 0.7214 - val_accuracy: 0.5117\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7706 - accuracy: 0.5059 - val_loss: 0.7044 - val_accuracy: 0.5273\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7674 - accuracy: 0.5264 - val_loss: 0.7865 - val_accuracy: 0.5078\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7660 - accuracy: 0.5312 - val_loss: 0.7706 - val_accuracy: 0.5078\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7433 - accuracy: 0.5342 - val_loss: 0.7068 - val_accuracy: 0.5078\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7195 - accuracy: 0.5449 - val_loss: 0.7296 - val_accuracy: 0.5039\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7286 - accuracy: 0.5254 - val_loss: 0.6942 - val_accuracy: 0.5117\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7459 - accuracy: 0.5332 - val_loss: 0.6936 - val_accuracy: 0.5156\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7295 - accuracy: 0.5264 - val_loss: 0.7032 - val_accuracy: 0.5000\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7333 - accuracy: 0.5117 - val_loss: 0.6949 - val_accuracy: 0.5039\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7172 - accuracy: 0.5332 - val_loss: 0.7031 - val_accuracy: 0.5078\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7195 - accuracy: 0.5430 - val_loss: 0.7257 - val_accuracy: 0.5039\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7142 - accuracy: 0.5459 - val_loss: 0.6848 - val_accuracy: 0.5625\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7036 - accuracy: 0.5527 - val_loss: 0.6963 - val_accuracy: 0.5508\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7072 - accuracy: 0.5332 - val_loss: 0.6927 - val_accuracy: 0.5703\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7224 - accuracy: 0.5137 - val_loss: 0.6905 - val_accuracy: 0.5547\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7171 - accuracy: 0.5479 - val_loss: 0.6958 - val_accuracy: 0.5312\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6917 - accuracy: 0.5625 - val_loss: 0.7288 - val_accuracy: 0.5039\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7050 - accuracy: 0.5332 - val_loss: 0.7109 - val_accuracy: 0.5391\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 14ms/step - loss: 0.8434 - accuracy: 0.5098 - val_loss: 0.6971 - val_accuracy: 0.4453\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7943 - accuracy: 0.4961 - val_loss: 0.7005 - val_accuracy: 0.4961\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7795 - accuracy: 0.4971 - val_loss: 0.7062 - val_accuracy: 0.5000\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7616 - accuracy: 0.4980 - val_loss: 0.6926 - val_accuracy: 0.5000\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7390 - accuracy: 0.5303 - val_loss: 0.6983 - val_accuracy: 0.4766\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7435 - accuracy: 0.4971 - val_loss: 0.6942 - val_accuracy: 0.4766\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7399 - accuracy: 0.5088 - val_loss: 0.6972 - val_accuracy: 0.4805\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7210 - accuracy: 0.5127 - val_loss: 0.6892 - val_accuracy: 0.5234\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7458 - accuracy: 0.4688 - val_loss: 0.6949 - val_accuracy: 0.5391\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7290 - accuracy: 0.4902 - val_loss: 0.6953 - val_accuracy: 0.5234\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7247 - accuracy: 0.5166 - val_loss: 0.6957 - val_accuracy: 0.5078\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7199 - accuracy: 0.5117 - val_loss: 0.7070 - val_accuracy: 0.4844\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7272 - accuracy: 0.4893 - val_loss: 0.6966 - val_accuracy: 0.5000\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7062 - accuracy: 0.5322 - val_loss: 0.7074 - val_accuracy: 0.4961\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7229 - accuracy: 0.4873 - val_loss: 0.6997 - val_accuracy: 0.5078\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7036 - accuracy: 0.5225 - val_loss: 0.7000 - val_accuracy: 0.4883\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7172 - accuracy: 0.5039 - val_loss: 0.6990 - val_accuracy: 0.5039\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7172 - accuracy: 0.4971 - val_loss: 0.7061 - val_accuracy: 0.4727\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7007 - accuracy: 0.5312 - val_loss: 0.7118 - val_accuracy: 0.4805\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7116 - accuracy: 0.5049 - val_loss: 0.7031 - val_accuracy: 0.4727\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 15ms/step - loss: 0.8517 - accuracy: 0.5088 - val_loss: 0.6943 - val_accuracy: 0.4609\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7785 - accuracy: 0.5039 - val_loss: 0.7044 - val_accuracy: 0.5078\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7762 - accuracy: 0.5029 - val_loss: 0.7236 - val_accuracy: 0.5078\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7602 - accuracy: 0.5176 - val_loss: 0.6905 - val_accuracy: 0.5273\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7547 - accuracy: 0.4814 - val_loss: 0.6947 - val_accuracy: 0.4922\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7497 - accuracy: 0.5078 - val_loss: 0.7039 - val_accuracy: 0.5117\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7507 - accuracy: 0.4746 - val_loss: 0.7050 - val_accuracy: 0.5039\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7326 - accuracy: 0.5156 - val_loss: 0.6961 - val_accuracy: 0.5078\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7263 - accuracy: 0.5010 - val_loss: 0.7099 - val_accuracy: 0.5039\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7169 - accuracy: 0.5195 - val_loss: 0.7035 - val_accuracy: 0.4922\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7346 - accuracy: 0.5039 - val_loss: 0.6999 - val_accuracy: 0.4961\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7169 - accuracy: 0.5146 - val_loss: 0.7122 - val_accuracy: 0.5039\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7111 - accuracy: 0.5234 - val_loss: 0.6996 - val_accuracy: 0.5312\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7136 - accuracy: 0.4961 - val_loss: 0.6994 - val_accuracy: 0.5156\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7184 - accuracy: 0.5088 - val_loss: 0.6934 - val_accuracy: 0.5352\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7235 - accuracy: 0.4971 - val_loss: 0.6984 - val_accuracy: 0.5312\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7078 - accuracy: 0.5332 - val_loss: 0.6955 - val_accuracy: 0.5430\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7120 - accuracy: 0.5088 - val_loss: 0.6966 - val_accuracy: 0.5234\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7128 - accuracy: 0.5176 - val_loss: 0.7111 - val_accuracy: 0.4961\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7013 - accuracy: 0.5361 - val_loss: 0.6946 - val_accuracy: 0.5352\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 14ms/step - loss: 0.8678 - accuracy: 0.4834 - val_loss: 0.6979 - val_accuracy: 0.5078\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7885 - accuracy: 0.5225 - val_loss: 0.7450 - val_accuracy: 0.5078\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7635 - accuracy: 0.5215 - val_loss: 0.6973 - val_accuracy: 0.5000\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7705 - accuracy: 0.5146 - val_loss: 0.7025 - val_accuracy: 0.5078\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7546 - accuracy: 0.5020 - val_loss: 0.7557 - val_accuracy: 0.5078\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7364 - accuracy: 0.5059 - val_loss: 0.7093 - val_accuracy: 0.5078\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7463 - accuracy: 0.4932 - val_loss: 0.7119 - val_accuracy: 0.5117\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7137 - accuracy: 0.5332 - val_loss: 0.6951 - val_accuracy: 0.5195\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7416 - accuracy: 0.5010 - val_loss: 0.7021 - val_accuracy: 0.5156\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7102 - accuracy: 0.5527 - val_loss: 0.6945 - val_accuracy: 0.5156\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7297 - accuracy: 0.4971 - val_loss: 0.6901 - val_accuracy: 0.5273\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7146 - accuracy: 0.5273 - val_loss: 0.6928 - val_accuracy: 0.5078\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7147 - accuracy: 0.5234 - val_loss: 0.6990 - val_accuracy: 0.5156\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7111 - accuracy: 0.5537 - val_loss: 0.6903 - val_accuracy: 0.5312\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7135 - accuracy: 0.5332 - val_loss: 0.7002 - val_accuracy: 0.5234\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7105 - accuracy: 0.5273 - val_loss: 0.7026 - val_accuracy: 0.5078\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7208 - accuracy: 0.5156 - val_loss: 0.7053 - val_accuracy: 0.5078\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7107 - accuracy: 0.5156 - val_loss: 0.7003 - val_accuracy: 0.5078\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7050 - accuracy: 0.5332 - val_loss: 0.7016 - val_accuracy: 0.5078\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6885 - accuracy: 0.5488 - val_loss: 0.7105 - val_accuracy: 0.5078\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 14ms/step - loss: 0.8269 - accuracy: 0.5371 - val_loss: 0.7017 - val_accuracy: 0.4961\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7541 - accuracy: 0.5430 - val_loss: 0.6926 - val_accuracy: 0.5156\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7268 - accuracy: 0.5518 - val_loss: 0.6913 - val_accuracy: 0.5156\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7931 - accuracy: 0.5010 - val_loss: 0.6982 - val_accuracy: 0.5273\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7327 - accuracy: 0.5205 - val_loss: 0.6914 - val_accuracy: 0.5352\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7412 - accuracy: 0.5117 - val_loss: 0.7022 - val_accuracy: 0.4883\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7262 - accuracy: 0.5439 - val_loss: 0.7033 - val_accuracy: 0.4883\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7202 - accuracy: 0.5361 - val_loss: 0.7130 - val_accuracy: 0.5000\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7264 - accuracy: 0.5303 - val_loss: 0.7145 - val_accuracy: 0.5156\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7400 - accuracy: 0.5088 - val_loss: 0.7046 - val_accuracy: 0.5117\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7199 - accuracy: 0.5088 - val_loss: 0.7057 - val_accuracy: 0.5156\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7072 - accuracy: 0.5508 - val_loss: 0.7061 - val_accuracy: 0.5078\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6993 - accuracy: 0.5352 - val_loss: 0.7160 - val_accuracy: 0.5039\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7153 - accuracy: 0.5078 - val_loss: 0.7009 - val_accuracy: 0.5117\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7151 - accuracy: 0.5254 - val_loss: 0.7115 - val_accuracy: 0.5078\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7188 - accuracy: 0.5244 - val_loss: 0.7037 - val_accuracy: 0.5117\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7152 - accuracy: 0.5029 - val_loss: 0.7030 - val_accuracy: 0.5039\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6958 - accuracy: 0.5234 - val_loss: 0.7082 - val_accuracy: 0.5078\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7064 - accuracy: 0.5488 - val_loss: 0.6980 - val_accuracy: 0.5312\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7104 - accuracy: 0.5059 - val_loss: 0.7085 - val_accuracy: 0.5117\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "       frontal  central  parietal  occipital\n",
            "theta    48.44    51.17     53.52      54.69\n",
            "alpha    47.66    53.52     54.30      50.78\n",
            "beta     55.47    50.39     53.91      53.91\n",
            "gamma    47.27    53.52     50.78      51.17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy(\"valence\",\"cnn\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0SCvBTunqke",
        "outputId": "8be7f8ce-1cce-4d0b-9acb-aec1617ef028"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 14ms/step - loss: 0.8728 - accuracy: 0.4873 - val_loss: 0.6878 - val_accuracy: 0.5469\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.8116 - accuracy: 0.5098 - val_loss: 0.7299 - val_accuracy: 0.5078\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7652 - accuracy: 0.5176 - val_loss: 0.7066 - val_accuracy: 0.5234\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7824 - accuracy: 0.4873 - val_loss: 0.7099 - val_accuracy: 0.5234\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7422 - accuracy: 0.5400 - val_loss: 0.7215 - val_accuracy: 0.5234\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7376 - accuracy: 0.5000 - val_loss: 0.6911 - val_accuracy: 0.5469\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7352 - accuracy: 0.5225 - val_loss: 0.6977 - val_accuracy: 0.5234\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7262 - accuracy: 0.4990 - val_loss: 0.6977 - val_accuracy: 0.5312\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7221 - accuracy: 0.5186 - val_loss: 0.6920 - val_accuracy: 0.5391\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7186 - accuracy: 0.5078 - val_loss: 0.7051 - val_accuracy: 0.5273\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7242 - accuracy: 0.5098 - val_loss: 0.6933 - val_accuracy: 0.5391\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7171 - accuracy: 0.5205 - val_loss: 0.6911 - val_accuracy: 0.5391\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7169 - accuracy: 0.5176 - val_loss: 0.6863 - val_accuracy: 0.5547\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7325 - accuracy: 0.4805 - val_loss: 0.6911 - val_accuracy: 0.5352\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7101 - accuracy: 0.5225 - val_loss: 0.7013 - val_accuracy: 0.5156\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7103 - accuracy: 0.5020 - val_loss: 0.6889 - val_accuracy: 0.5312\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7046 - accuracy: 0.5225 - val_loss: 0.7055 - val_accuracy: 0.5117\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7191 - accuracy: 0.5205 - val_loss: 0.6886 - val_accuracy: 0.5312\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7035 - accuracy: 0.5020 - val_loss: 0.6925 - val_accuracy: 0.5312\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7052 - accuracy: 0.5215 - val_loss: 0.6908 - val_accuracy: 0.5273\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 52ms/step - loss: 0.8517 - accuracy: 0.5029 - val_loss: 0.7398 - val_accuracy: 0.5273\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.8002 - accuracy: 0.5146 - val_loss: 0.6976 - val_accuracy: 0.5430\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.8083 - accuracy: 0.5078 - val_loss: 0.6876 - val_accuracy: 0.5508\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7544 - accuracy: 0.4980 - val_loss: 0.6960 - val_accuracy: 0.5352\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7440 - accuracy: 0.5137 - val_loss: 0.6886 - val_accuracy: 0.5273\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7391 - accuracy: 0.5137 - val_loss: 0.6971 - val_accuracy: 0.5469\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7318 - accuracy: 0.5088 - val_loss: 0.6880 - val_accuracy: 0.5117\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7149 - accuracy: 0.5186 - val_loss: 0.6973 - val_accuracy: 0.5000\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7364 - accuracy: 0.5254 - val_loss: 0.7016 - val_accuracy: 0.5039\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7249 - accuracy: 0.5234 - val_loss: 0.6971 - val_accuracy: 0.5078\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7203 - accuracy: 0.5176 - val_loss: 0.6921 - val_accuracy: 0.5078\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7144 - accuracy: 0.5137 - val_loss: 0.6914 - val_accuracy: 0.5000\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7054 - accuracy: 0.5342 - val_loss: 0.6911 - val_accuracy: 0.5195\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7246 - accuracy: 0.5059 - val_loss: 0.6940 - val_accuracy: 0.4961\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7133 - accuracy: 0.5322 - val_loss: 0.6896 - val_accuracy: 0.4961\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7044 - accuracy: 0.4961 - val_loss: 0.6892 - val_accuracy: 0.5352\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7126 - accuracy: 0.5137 - val_loss: 0.6952 - val_accuracy: 0.5234\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7152 - accuracy: 0.5146 - val_loss: 0.6930 - val_accuracy: 0.5312\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7009 - accuracy: 0.5400 - val_loss: 0.6888 - val_accuracy: 0.5195\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7060 - accuracy: 0.5146 - val_loss: 0.6888 - val_accuracy: 0.5156\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 20ms/step - loss: 0.9092 - accuracy: 0.5117 - val_loss: 0.9052 - val_accuracy: 0.4531\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7791 - accuracy: 0.5244 - val_loss: 0.7390 - val_accuracy: 0.4648\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7910 - accuracy: 0.4941 - val_loss: 0.6968 - val_accuracy: 0.4609\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7579 - accuracy: 0.4834 - val_loss: 0.6930 - val_accuracy: 0.5000\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7456 - accuracy: 0.5234 - val_loss: 0.7172 - val_accuracy: 0.4531\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7082 - accuracy: 0.5488 - val_loss: 0.6922 - val_accuracy: 0.5195\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7411 - accuracy: 0.5049 - val_loss: 0.6949 - val_accuracy: 0.4688\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7147 - accuracy: 0.5156 - val_loss: 0.6949 - val_accuracy: 0.4922\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7252 - accuracy: 0.5068 - val_loss: 0.6955 - val_accuracy: 0.4922\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6971 - accuracy: 0.5410 - val_loss: 0.7215 - val_accuracy: 0.4844\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7202 - accuracy: 0.4902 - val_loss: 0.7014 - val_accuracy: 0.4688\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7121 - accuracy: 0.5283 - val_loss: 0.6982 - val_accuracy: 0.4922\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7147 - accuracy: 0.5264 - val_loss: 0.7050 - val_accuracy: 0.5117\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7156 - accuracy: 0.5244 - val_loss: 0.7124 - val_accuracy: 0.5195\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7068 - accuracy: 0.5078 - val_loss: 0.7117 - val_accuracy: 0.5508\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7066 - accuracy: 0.5107 - val_loss: 0.6982 - val_accuracy: 0.4844\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6988 - accuracy: 0.5146 - val_loss: 0.7167 - val_accuracy: 0.5273\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7071 - accuracy: 0.5137 - val_loss: 0.7049 - val_accuracy: 0.5156\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7060 - accuracy: 0.5215 - val_loss: 0.6907 - val_accuracy: 0.5430\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7059 - accuracy: 0.4951 - val_loss: 0.7053 - val_accuracy: 0.5195\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 14ms/step - loss: 0.8514 - accuracy: 0.5098 - val_loss: 0.7174 - val_accuracy: 0.5469\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7728 - accuracy: 0.5205 - val_loss: 0.6977 - val_accuracy: 0.5547\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.8172 - accuracy: 0.4951 - val_loss: 0.6924 - val_accuracy: 0.5430\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7436 - accuracy: 0.5107 - val_loss: 0.6930 - val_accuracy: 0.5430\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7490 - accuracy: 0.4912 - val_loss: 0.7017 - val_accuracy: 0.5430\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7361 - accuracy: 0.5244 - val_loss: 0.6935 - val_accuracy: 0.5430\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7420 - accuracy: 0.5059 - val_loss: 0.7104 - val_accuracy: 0.5352\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7334 - accuracy: 0.5225 - val_loss: 0.6912 - val_accuracy: 0.5469\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7459 - accuracy: 0.4912 - val_loss: 0.7351 - val_accuracy: 0.5391\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7353 - accuracy: 0.5117 - val_loss: 0.7154 - val_accuracy: 0.5430\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7249 - accuracy: 0.4883 - val_loss: 0.7275 - val_accuracy: 0.5430\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7196 - accuracy: 0.5068 - val_loss: 0.7061 - val_accuracy: 0.5391\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7347 - accuracy: 0.4775 - val_loss: 0.7084 - val_accuracy: 0.5391\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7218 - accuracy: 0.5146 - val_loss: 0.7016 - val_accuracy: 0.5469\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7173 - accuracy: 0.5098 - val_loss: 0.6941 - val_accuracy: 0.5430\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7231 - accuracy: 0.4834 - val_loss: 0.7009 - val_accuracy: 0.5508\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7199 - accuracy: 0.5176 - val_loss: 0.6916 - val_accuracy: 0.5391\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7189 - accuracy: 0.4834 - val_loss: 0.6898 - val_accuracy: 0.5586\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7032 - accuracy: 0.5283 - val_loss: 0.6870 - val_accuracy: 0.5586\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7048 - accuracy: 0.5137 - val_loss: 0.6971 - val_accuracy: 0.5586\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 19ms/step - loss: 0.8519 - accuracy: 0.5088 - val_loss: 0.7055 - val_accuracy: 0.4531\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7895 - accuracy: 0.4961 - val_loss: 0.6929 - val_accuracy: 0.5469\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7867 - accuracy: 0.4883 - val_loss: 0.6941 - val_accuracy: 0.5156\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7667 - accuracy: 0.5088 - val_loss: 0.6895 - val_accuracy: 0.5625\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7325 - accuracy: 0.5371 - val_loss: 0.6902 - val_accuracy: 0.5430\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7443 - accuracy: 0.5020 - val_loss: 0.6882 - val_accuracy: 0.5586\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7241 - accuracy: 0.5322 - val_loss: 0.6959 - val_accuracy: 0.4961\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7175 - accuracy: 0.5146 - val_loss: 0.6897 - val_accuracy: 0.5625\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7370 - accuracy: 0.4990 - val_loss: 0.6951 - val_accuracy: 0.5195\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7202 - accuracy: 0.5449 - val_loss: 0.6884 - val_accuracy: 0.5352\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7108 - accuracy: 0.5352 - val_loss: 0.6924 - val_accuracy: 0.5078\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7407 - accuracy: 0.5039 - val_loss: 0.6978 - val_accuracy: 0.5273\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7323 - accuracy: 0.5000 - val_loss: 0.6962 - val_accuracy: 0.4922\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7311 - accuracy: 0.5283 - val_loss: 0.6882 - val_accuracy: 0.5273\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7279 - accuracy: 0.4941 - val_loss: 0.7162 - val_accuracy: 0.5117\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6978 - accuracy: 0.5420 - val_loss: 0.6957 - val_accuracy: 0.5234\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7034 - accuracy: 0.5244 - val_loss: 0.7013 - val_accuracy: 0.5078\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7183 - accuracy: 0.4922 - val_loss: 0.7069 - val_accuracy: 0.5000\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7093 - accuracy: 0.5059 - val_loss: 0.6979 - val_accuracy: 0.5117\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7016 - accuracy: 0.5410 - val_loss: 0.6904 - val_accuracy: 0.5234\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 13ms/step - loss: 0.8287 - accuracy: 0.5029 - val_loss: 0.6927 - val_accuracy: 0.5469\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7977 - accuracy: 0.5107 - val_loss: 0.6897 - val_accuracy: 0.5625\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7718 - accuracy: 0.4990 - val_loss: 0.6905 - val_accuracy: 0.5430\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7732 - accuracy: 0.5010 - val_loss: 0.6882 - val_accuracy: 0.5586\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7394 - accuracy: 0.5156 - val_loss: 0.6885 - val_accuracy: 0.5586\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7285 - accuracy: 0.5137 - val_loss: 0.6887 - val_accuracy: 0.5586\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7206 - accuracy: 0.5146 - val_loss: 0.6893 - val_accuracy: 0.5391\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7170 - accuracy: 0.5254 - val_loss: 0.6991 - val_accuracy: 0.4883\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7277 - accuracy: 0.4668 - val_loss: 0.6917 - val_accuracy: 0.5039\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7099 - accuracy: 0.5049 - val_loss: 0.6891 - val_accuracy: 0.4883\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7158 - accuracy: 0.5244 - val_loss: 0.7032 - val_accuracy: 0.4766\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7153 - accuracy: 0.5381 - val_loss: 0.6937 - val_accuracy: 0.4922\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7112 - accuracy: 0.5117 - val_loss: 0.6889 - val_accuracy: 0.5117\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7137 - accuracy: 0.5137 - val_loss: 0.6907 - val_accuracy: 0.5156\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7084 - accuracy: 0.5039 - val_loss: 0.6897 - val_accuracy: 0.5000\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7075 - accuracy: 0.5146 - val_loss: 0.6913 - val_accuracy: 0.5000\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7119 - accuracy: 0.4873 - val_loss: 0.6916 - val_accuracy: 0.5000\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7085 - accuracy: 0.5195 - val_loss: 0.6910 - val_accuracy: 0.5039\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6988 - accuracy: 0.5527 - val_loss: 0.6911 - val_accuracy: 0.5039\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7110 - accuracy: 0.5098 - val_loss: 0.6933 - val_accuracy: 0.5078\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 19ms/step - loss: 0.8477 - accuracy: 0.4961 - val_loss: 0.7014 - val_accuracy: 0.4531\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7947 - accuracy: 0.4922 - val_loss: 0.7434 - val_accuracy: 0.5234\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7844 - accuracy: 0.4922 - val_loss: 0.6890 - val_accuracy: 0.5117\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7526 - accuracy: 0.5000 - val_loss: 0.6947 - val_accuracy: 0.4961\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7419 - accuracy: 0.5225 - val_loss: 0.6925 - val_accuracy: 0.5156\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7615 - accuracy: 0.4863 - val_loss: 0.7008 - val_accuracy: 0.4805\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7296 - accuracy: 0.5068 - val_loss: 0.6913 - val_accuracy: 0.5000\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7348 - accuracy: 0.4932 - val_loss: 0.6976 - val_accuracy: 0.4688\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7300 - accuracy: 0.5098 - val_loss: 0.6981 - val_accuracy: 0.4805\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7263 - accuracy: 0.5020 - val_loss: 0.7042 - val_accuracy: 0.4727\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7113 - accuracy: 0.5127 - val_loss: 0.6978 - val_accuracy: 0.4844\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7033 - accuracy: 0.5361 - val_loss: 0.7044 - val_accuracy: 0.4727\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7043 - accuracy: 0.5283 - val_loss: 0.7192 - val_accuracy: 0.4844\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7095 - accuracy: 0.5049 - val_loss: 0.7017 - val_accuracy: 0.4883\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7154 - accuracy: 0.5029 - val_loss: 0.7007 - val_accuracy: 0.4688\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7142 - accuracy: 0.5020 - val_loss: 0.7049 - val_accuracy: 0.4844\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7068 - accuracy: 0.5068 - val_loss: 0.7062 - val_accuracy: 0.4922\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7054 - accuracy: 0.5283 - val_loss: 0.7013 - val_accuracy: 0.4727\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7014 - accuracy: 0.5234 - val_loss: 0.6981 - val_accuracy: 0.4922\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6968 - accuracy: 0.5215 - val_loss: 0.7001 - val_accuracy: 0.4727\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 13ms/step - loss: 0.8663 - accuracy: 0.4961 - val_loss: 0.8854 - val_accuracy: 0.5156\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7742 - accuracy: 0.5059 - val_loss: 0.6930 - val_accuracy: 0.5234\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7972 - accuracy: 0.4854 - val_loss: 0.7183 - val_accuracy: 0.5469\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7927 - accuracy: 0.5234 - val_loss: 0.6948 - val_accuracy: 0.5391\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7597 - accuracy: 0.4961 - val_loss: 0.7234 - val_accuracy: 0.5469\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7795 - accuracy: 0.4922 - val_loss: 0.6989 - val_accuracy: 0.5430\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7465 - accuracy: 0.4980 - val_loss: 0.7005 - val_accuracy: 0.5312\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7273 - accuracy: 0.5342 - val_loss: 0.6935 - val_accuracy: 0.5312\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7381 - accuracy: 0.5205 - val_loss: 0.6922 - val_accuracy: 0.5312\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7229 - accuracy: 0.5303 - val_loss: 0.6933 - val_accuracy: 0.5352\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7500 - accuracy: 0.4961 - val_loss: 0.6929 - val_accuracy: 0.5273\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7300 - accuracy: 0.4941 - val_loss: 0.6890 - val_accuracy: 0.5430\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7290 - accuracy: 0.4941 - val_loss: 0.6906 - val_accuracy: 0.5352\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7208 - accuracy: 0.5234 - val_loss: 0.6961 - val_accuracy: 0.5352\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7154 - accuracy: 0.5088 - val_loss: 0.6971 - val_accuracy: 0.5312\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7084 - accuracy: 0.5127 - val_loss: 0.6988 - val_accuracy: 0.5391\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7252 - accuracy: 0.5127 - val_loss: 0.6965 - val_accuracy: 0.5430\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7158 - accuracy: 0.5020 - val_loss: 0.6957 - val_accuracy: 0.5391\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7077 - accuracy: 0.5342 - val_loss: 0.6900 - val_accuracy: 0.5352\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7136 - accuracy: 0.5264 - val_loss: 0.6921 - val_accuracy: 0.5352\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 20ms/step - loss: 0.8364 - accuracy: 0.4980 - val_loss: 0.8201 - val_accuracy: 0.4844\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.8067 - accuracy: 0.5176 - val_loss: 0.7532 - val_accuracy: 0.4844\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7893 - accuracy: 0.5010 - val_loss: 0.7498 - val_accuracy: 0.5000\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7626 - accuracy: 0.5049 - val_loss: 0.6989 - val_accuracy: 0.5430\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7126 - accuracy: 0.5312 - val_loss: 0.6921 - val_accuracy: 0.5391\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7710 - accuracy: 0.4863 - val_loss: 0.7094 - val_accuracy: 0.4961\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7440 - accuracy: 0.5205 - val_loss: 0.6961 - val_accuracy: 0.5156\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7217 - accuracy: 0.5186 - val_loss: 0.6937 - val_accuracy: 0.5039\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7258 - accuracy: 0.5088 - val_loss: 0.6869 - val_accuracy: 0.5977\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7355 - accuracy: 0.5107 - val_loss: 0.6996 - val_accuracy: 0.5117\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7170 - accuracy: 0.5400 - val_loss: 0.7002 - val_accuracy: 0.5039\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7100 - accuracy: 0.5420 - val_loss: 0.6937 - val_accuracy: 0.5352\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7275 - accuracy: 0.5020 - val_loss: 0.6873 - val_accuracy: 0.5469\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7359 - accuracy: 0.4902 - val_loss: 0.6899 - val_accuracy: 0.5352\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7217 - accuracy: 0.5078 - val_loss: 0.6905 - val_accuracy: 0.5273\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7078 - accuracy: 0.5117 - val_loss: 0.6885 - val_accuracy: 0.5391\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7149 - accuracy: 0.5020 - val_loss: 0.7041 - val_accuracy: 0.5117\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7037 - accuracy: 0.5293 - val_loss: 0.6928 - val_accuracy: 0.5273\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7131 - accuracy: 0.5078 - val_loss: 0.7001 - val_accuracy: 0.5234\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7095 - accuracy: 0.5059 - val_loss: 0.7008 - val_accuracy: 0.5000\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 13ms/step - loss: 0.8973 - accuracy: 0.4951 - val_loss: 0.7730 - val_accuracy: 0.4258\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.8006 - accuracy: 0.4873 - val_loss: 0.6995 - val_accuracy: 0.4727\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7875 - accuracy: 0.4639 - val_loss: 0.8471 - val_accuracy: 0.4922\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.8042 - accuracy: 0.5010 - val_loss: 0.7111 - val_accuracy: 0.5195\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7315 - accuracy: 0.5293 - val_loss: 0.7015 - val_accuracy: 0.5000\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7310 - accuracy: 0.5029 - val_loss: 0.6967 - val_accuracy: 0.4922\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7354 - accuracy: 0.5176 - val_loss: 0.7162 - val_accuracy: 0.5078\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7142 - accuracy: 0.5234 - val_loss: 0.6992 - val_accuracy: 0.5078\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7232 - accuracy: 0.4961 - val_loss: 0.6923 - val_accuracy: 0.5273\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7249 - accuracy: 0.5156 - val_loss: 0.6942 - val_accuracy: 0.5078\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7332 - accuracy: 0.5156 - val_loss: 0.7031 - val_accuracy: 0.5078\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7285 - accuracy: 0.5078 - val_loss: 0.6975 - val_accuracy: 0.5156\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7307 - accuracy: 0.4873 - val_loss: 0.6960 - val_accuracy: 0.5078\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7043 - accuracy: 0.5156 - val_loss: 0.6942 - val_accuracy: 0.5039\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7162 - accuracy: 0.5068 - val_loss: 0.6958 - val_accuracy: 0.5078\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7003 - accuracy: 0.5225 - val_loss: 0.6976 - val_accuracy: 0.5039\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7048 - accuracy: 0.5195 - val_loss: 0.6956 - val_accuracy: 0.5000\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7068 - accuracy: 0.5264 - val_loss: 0.6945 - val_accuracy: 0.5156\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7120 - accuracy: 0.5107 - val_loss: 0.6997 - val_accuracy: 0.5039\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7009 - accuracy: 0.5264 - val_loss: 0.6998 - val_accuracy: 0.5156\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 17ms/step - loss: 0.8368 - accuracy: 0.5078 - val_loss: 0.7864 - val_accuracy: 0.5625\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7646 - accuracy: 0.5293 - val_loss: 0.6973 - val_accuracy: 0.4727\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7654 - accuracy: 0.4961 - val_loss: 0.6981 - val_accuracy: 0.4883\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7566 - accuracy: 0.5088 - val_loss: 0.7023 - val_accuracy: 0.4453\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7691 - accuracy: 0.4717 - val_loss: 0.6945 - val_accuracy: 0.4492\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7398 - accuracy: 0.4951 - val_loss: 0.6913 - val_accuracy: 0.5312\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7270 - accuracy: 0.5195 - val_loss: 0.6927 - val_accuracy: 0.5078\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7028 - accuracy: 0.5381 - val_loss: 0.6928 - val_accuracy: 0.4531\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7225 - accuracy: 0.5137 - val_loss: 0.7066 - val_accuracy: 0.5078\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7112 - accuracy: 0.5186 - val_loss: 0.7037 - val_accuracy: 0.4609\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7193 - accuracy: 0.5303 - val_loss: 0.7051 - val_accuracy: 0.5234\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7179 - accuracy: 0.5146 - val_loss: 0.6928 - val_accuracy: 0.5078\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7220 - accuracy: 0.5127 - val_loss: 0.6886 - val_accuracy: 0.5547\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7130 - accuracy: 0.5117 - val_loss: 0.6966 - val_accuracy: 0.5078\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7040 - accuracy: 0.5400 - val_loss: 0.6910 - val_accuracy: 0.5312\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7048 - accuracy: 0.5127 - val_loss: 0.6934 - val_accuracy: 0.4961\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7037 - accuracy: 0.5312 - val_loss: 0.6963 - val_accuracy: 0.4453\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6992 - accuracy: 0.5361 - val_loss: 0.6981 - val_accuracy: 0.4570\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6937 - accuracy: 0.5508 - val_loss: 0.7013 - val_accuracy: 0.4727\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7040 - accuracy: 0.5244 - val_loss: 0.7033 - val_accuracy: 0.4297\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 13ms/step - loss: 0.8893 - accuracy: 0.4951 - val_loss: 0.7605 - val_accuracy: 0.5195\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7637 - accuracy: 0.5127 - val_loss: 0.7088 - val_accuracy: 0.5391\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7619 - accuracy: 0.4951 - val_loss: 0.7042 - val_accuracy: 0.5469\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7818 - accuracy: 0.5020 - val_loss: 0.7378 - val_accuracy: 0.5547\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7519 - accuracy: 0.5283 - val_loss: 0.6897 - val_accuracy: 0.5391\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7650 - accuracy: 0.4883 - val_loss: 0.6969 - val_accuracy: 0.5664\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7558 - accuracy: 0.4902 - val_loss: 0.6883 - val_accuracy: 0.5430\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7356 - accuracy: 0.5098 - val_loss: 0.7002 - val_accuracy: 0.5508\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7372 - accuracy: 0.5352 - val_loss: 0.6897 - val_accuracy: 0.5391\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7223 - accuracy: 0.4893 - val_loss: 0.6911 - val_accuracy: 0.5547\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7339 - accuracy: 0.4717 - val_loss: 0.6884 - val_accuracy: 0.5430\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7468 - accuracy: 0.5059 - val_loss: 0.6907 - val_accuracy: 0.5586\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7189 - accuracy: 0.5029 - val_loss: 0.6892 - val_accuracy: 0.5547\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7209 - accuracy: 0.4980 - val_loss: 0.6970 - val_accuracy: 0.5547\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7192 - accuracy: 0.5078 - val_loss: 0.6889 - val_accuracy: 0.5547\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7136 - accuracy: 0.5234 - val_loss: 0.6899 - val_accuracy: 0.5508\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7111 - accuracy: 0.5254 - val_loss: 0.6861 - val_accuracy: 0.5508\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7129 - accuracy: 0.5264 - val_loss: 0.6870 - val_accuracy: 0.5625\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7132 - accuracy: 0.4990 - val_loss: 0.6913 - val_accuracy: 0.5625\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7025 - accuracy: 0.5186 - val_loss: 0.6904 - val_accuracy: 0.5586\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 13ms/step - loss: 0.8288 - accuracy: 0.5088 - val_loss: 0.6957 - val_accuracy: 0.5586\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7850 - accuracy: 0.4961 - val_loss: 0.6911 - val_accuracy: 0.5391\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7620 - accuracy: 0.5166 - val_loss: 0.6974 - val_accuracy: 0.5469\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7412 - accuracy: 0.5400 - val_loss: 0.6954 - val_accuracy: 0.5312\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7298 - accuracy: 0.5273 - val_loss: 0.6896 - val_accuracy: 0.5508\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7197 - accuracy: 0.5205 - val_loss: 0.7071 - val_accuracy: 0.5117\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7360 - accuracy: 0.4990 - val_loss: 0.6953 - val_accuracy: 0.5234\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7257 - accuracy: 0.5039 - val_loss: 0.6972 - val_accuracy: 0.5391\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7295 - accuracy: 0.4980 - val_loss: 0.6952 - val_accuracy: 0.5273\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7183 - accuracy: 0.4990 - val_loss: 0.6941 - val_accuracy: 0.5352\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7200 - accuracy: 0.5176 - val_loss: 0.7012 - val_accuracy: 0.5352\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7069 - accuracy: 0.5332 - val_loss: 0.6955 - val_accuracy: 0.5352\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7246 - accuracy: 0.5117 - val_loss: 0.6969 - val_accuracy: 0.5430\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7149 - accuracy: 0.5234 - val_loss: 0.6959 - val_accuracy: 0.5469\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7021 - accuracy: 0.5508 - val_loss: 0.6932 - val_accuracy: 0.5469\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7101 - accuracy: 0.5049 - val_loss: 0.6917 - val_accuracy: 0.5430\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6984 - accuracy: 0.5352 - val_loss: 0.6998 - val_accuracy: 0.5234\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7099 - accuracy: 0.5332 - val_loss: 0.6999 - val_accuracy: 0.5391\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7050 - accuracy: 0.5322 - val_loss: 0.6956 - val_accuracy: 0.5312\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7030 - accuracy: 0.5361 - val_loss: 0.6898 - val_accuracy: 0.5352\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 13ms/step - loss: 0.8056 - accuracy: 0.5000 - val_loss: 0.8581 - val_accuracy: 0.4766\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.8079 - accuracy: 0.5107 - val_loss: 0.7122 - val_accuracy: 0.5234\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7596 - accuracy: 0.5146 - val_loss: 0.6909 - val_accuracy: 0.5391\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7470 - accuracy: 0.5098 - val_loss: 0.7004 - val_accuracy: 0.5469\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7541 - accuracy: 0.5000 - val_loss: 0.6922 - val_accuracy: 0.5469\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7447 - accuracy: 0.5166 - val_loss: 0.6977 - val_accuracy: 0.5391\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7258 - accuracy: 0.5205 - val_loss: 0.6902 - val_accuracy: 0.5430\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7299 - accuracy: 0.5146 - val_loss: 0.6861 - val_accuracy: 0.5391\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7416 - accuracy: 0.5020 - val_loss: 0.6937 - val_accuracy: 0.5430\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7299 - accuracy: 0.5107 - val_loss: 0.6905 - val_accuracy: 0.5312\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7304 - accuracy: 0.5078 - val_loss: 0.6906 - val_accuracy: 0.5391\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7174 - accuracy: 0.5205 - val_loss: 0.6948 - val_accuracy: 0.5391\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7206 - accuracy: 0.5312 - val_loss: 0.7063 - val_accuracy: 0.5469\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7299 - accuracy: 0.4902 - val_loss: 0.6914 - val_accuracy: 0.5469\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7187 - accuracy: 0.5234 - val_loss: 0.6913 - val_accuracy: 0.5391\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7151 - accuracy: 0.5039 - val_loss: 0.6904 - val_accuracy: 0.5547\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7068 - accuracy: 0.4902 - val_loss: 0.6888 - val_accuracy: 0.5469\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7180 - accuracy: 0.4980 - val_loss: 0.6880 - val_accuracy: 0.5547\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7169 - accuracy: 0.5059 - val_loss: 0.6953 - val_accuracy: 0.5469\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7152 - accuracy: 0.5107 - val_loss: 0.6893 - val_accuracy: 0.5391\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 14ms/step - loss: 0.8337 - accuracy: 0.5088 - val_loss: 0.7285 - val_accuracy: 0.5469\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.8077 - accuracy: 0.5088 - val_loss: 0.7077 - val_accuracy: 0.5469\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7841 - accuracy: 0.4805 - val_loss: 0.6923 - val_accuracy: 0.5469\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7806 - accuracy: 0.4883 - val_loss: 0.6949 - val_accuracy: 0.5234\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7590 - accuracy: 0.5195 - val_loss: 0.6931 - val_accuracy: 0.5469\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7352 - accuracy: 0.5107 - val_loss: 0.6929 - val_accuracy: 0.5391\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7347 - accuracy: 0.5186 - val_loss: 0.6922 - val_accuracy: 0.5469\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7550 - accuracy: 0.4814 - val_loss: 0.6905 - val_accuracy: 0.5469\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7170 - accuracy: 0.5137 - val_loss: 0.7033 - val_accuracy: 0.4727\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7355 - accuracy: 0.4941 - val_loss: 0.6879 - val_accuracy: 0.5469\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7170 - accuracy: 0.5186 - val_loss: 0.6904 - val_accuracy: 0.5352\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7127 - accuracy: 0.5381 - val_loss: 0.6980 - val_accuracy: 0.4883\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7176 - accuracy: 0.5029 - val_loss: 0.6966 - val_accuracy: 0.4922\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7226 - accuracy: 0.5049 - val_loss: 0.7086 - val_accuracy: 0.4766\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7219 - accuracy: 0.5059 - val_loss: 0.6854 - val_accuracy: 0.5469\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7150 - accuracy: 0.4961 - val_loss: 0.6902 - val_accuracy: 0.5508\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7138 - accuracy: 0.5146 - val_loss: 0.6978 - val_accuracy: 0.5000\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7067 - accuracy: 0.5322 - val_loss: 0.6949 - val_accuracy: 0.4883\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7052 - accuracy: 0.5176 - val_loss: 0.6948 - val_accuracy: 0.4805\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7082 - accuracy: 0.4971 - val_loss: 0.6925 - val_accuracy: 0.5312\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 14ms/step - loss: 0.8786 - accuracy: 0.4980 - val_loss: 0.7466 - val_accuracy: 0.5469\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7924 - accuracy: 0.5215 - val_loss: 0.7295 - val_accuracy: 0.5586\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7696 - accuracy: 0.5098 - val_loss: 0.7116 - val_accuracy: 0.5469\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7917 - accuracy: 0.5088 - val_loss: 0.7200 - val_accuracy: 0.5664\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7589 - accuracy: 0.5039 - val_loss: 0.7054 - val_accuracy: 0.5508\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7396 - accuracy: 0.5127 - val_loss: 0.7139 - val_accuracy: 0.5547\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7504 - accuracy: 0.4902 - val_loss: 0.7033 - val_accuracy: 0.5469\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7370 - accuracy: 0.5059 - val_loss: 0.6917 - val_accuracy: 0.5586\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7290 - accuracy: 0.5088 - val_loss: 0.6950 - val_accuracy: 0.5547\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7476 - accuracy: 0.5039 - val_loss: 0.6872 - val_accuracy: 0.5547\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7436 - accuracy: 0.4941 - val_loss: 0.6979 - val_accuracy: 0.5469\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7101 - accuracy: 0.5176 - val_loss: 0.6899 - val_accuracy: 0.5547\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7038 - accuracy: 0.5381 - val_loss: 0.6904 - val_accuracy: 0.5469\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7359 - accuracy: 0.4805 - val_loss: 0.6895 - val_accuracy: 0.5469\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7207 - accuracy: 0.5205 - val_loss: 0.6894 - val_accuracy: 0.5586\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7253 - accuracy: 0.5176 - val_loss: 0.6890 - val_accuracy: 0.5508\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7215 - accuracy: 0.5146 - val_loss: 0.6929 - val_accuracy: 0.5469\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.7051 - accuracy: 0.5283 - val_loss: 0.6901 - val_accuracy: 0.5469\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7102 - accuracy: 0.5166 - val_loss: 0.6885 - val_accuracy: 0.5547\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7181 - accuracy: 0.5449 - val_loss: 0.6971 - val_accuracy: 0.5508\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "       frontal  central  parietal  occipital\n",
            "theta    52.73    51.56     51.95      55.86\n",
            "alpha    52.34    50.78     47.27      53.52\n",
            "beta     50.00    51.56     42.97      55.86\n",
            "gamma    53.52    53.91     53.12      55.08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_conf(\"beta\",\"frontal\",\"arousal\",\"cnn\")"
      ],
      "metadata": {
        "id": "Dine7YBivO0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_conf(\"beta\",\"occipital\",\"valence\",\"cnn\")"
      ],
      "metadata": {
        "id": "8kyuEIz_vPts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy(\"arousal\",\"ann\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uESH51Unv9T",
        "outputId": "ad8b4488-5a31-4be1-c16a-c853cfd0b5cf"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "32/32 [==============================] - 1s 10ms/step - loss: 0.7005 - accuracy: 0.5146 - val_loss: 0.6845 - val_accuracy: 0.5938\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6901 - accuracy: 0.5488 - val_loss: 0.6837 - val_accuracy: 0.5742\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6847 - accuracy: 0.5479 - val_loss: 0.6995 - val_accuracy: 0.5586\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6865 - accuracy: 0.5674 - val_loss: 0.6929 - val_accuracy: 0.5664\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6870 - accuracy: 0.5654 - val_loss: 0.6914 - val_accuracy: 0.5547\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6818 - accuracy: 0.5566 - val_loss: 0.7021 - val_accuracy: 0.5625\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6840 - accuracy: 0.5557 - val_loss: 0.6995 - val_accuracy: 0.5469\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6821 - accuracy: 0.5605 - val_loss: 0.6974 - val_accuracy: 0.5742\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6801 - accuracy: 0.5596 - val_loss: 0.7048 - val_accuracy: 0.5664\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6810 - accuracy: 0.5654 - val_loss: 0.6971 - val_accuracy: 0.6055\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6837 - accuracy: 0.5684 - val_loss: 0.7240 - val_accuracy: 0.5391\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6799 - accuracy: 0.5811 - val_loss: 0.6944 - val_accuracy: 0.5781\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6776 - accuracy: 0.5664 - val_loss: 0.7026 - val_accuracy: 0.5430\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6763 - accuracy: 0.5527 - val_loss: 0.7184 - val_accuracy: 0.5391\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6748 - accuracy: 0.5752 - val_loss: 0.6984 - val_accuracy: 0.5742\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6762 - accuracy: 0.5752 - val_loss: 0.7075 - val_accuracy: 0.5625\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6739 - accuracy: 0.5811 - val_loss: 0.7039 - val_accuracy: 0.5664\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6789 - accuracy: 0.5615 - val_loss: 0.7006 - val_accuracy: 0.5664\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6719 - accuracy: 0.5850 - val_loss: 0.7157 - val_accuracy: 0.5977\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6753 - accuracy: 0.5635 - val_loss: 0.7053 - val_accuracy: 0.5664\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 0.7031 - accuracy: 0.5029 - val_loss: 0.6894 - val_accuracy: 0.5508\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6844 - accuracy: 0.5488 - val_loss: 0.6927 - val_accuracy: 0.5117\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6835 - accuracy: 0.5449 - val_loss: 0.6920 - val_accuracy: 0.5625\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6805 - accuracy: 0.5566 - val_loss: 0.6949 - val_accuracy: 0.5195\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6784 - accuracy: 0.5361 - val_loss: 0.6967 - val_accuracy: 0.5547\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6778 - accuracy: 0.5615 - val_loss: 0.6989 - val_accuracy: 0.5508\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6769 - accuracy: 0.5547 - val_loss: 0.6947 - val_accuracy: 0.5469\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6762 - accuracy: 0.5645 - val_loss: 0.6950 - val_accuracy: 0.5273\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6739 - accuracy: 0.5693 - val_loss: 0.6984 - val_accuracy: 0.5273\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6730 - accuracy: 0.5615 - val_loss: 0.6990 - val_accuracy: 0.5117\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6743 - accuracy: 0.5674 - val_loss: 0.7017 - val_accuracy: 0.5469\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6762 - accuracy: 0.5762 - val_loss: 0.6922 - val_accuracy: 0.5586\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6703 - accuracy: 0.5674 - val_loss: 0.7045 - val_accuracy: 0.5625\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6756 - accuracy: 0.5527 - val_loss: 0.7013 - val_accuracy: 0.5586\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6734 - accuracy: 0.5586 - val_loss: 0.6898 - val_accuracy: 0.5312\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6726 - accuracy: 0.5449 - val_loss: 0.7000 - val_accuracy: 0.5156\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6698 - accuracy: 0.5713 - val_loss: 0.6936 - val_accuracy: 0.5586\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6710 - accuracy: 0.5664 - val_loss: 0.7033 - val_accuracy: 0.5430\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6745 - accuracy: 0.5488 - val_loss: 0.6944 - val_accuracy: 0.5664\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6681 - accuracy: 0.5732 - val_loss: 0.6972 - val_accuracy: 0.5547\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 1s 10ms/step - loss: 0.6894 - accuracy: 0.5264 - val_loss: 0.7030 - val_accuracy: 0.5508\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6810 - accuracy: 0.5742 - val_loss: 0.6939 - val_accuracy: 0.5078\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6762 - accuracy: 0.5703 - val_loss: 0.7012 - val_accuracy: 0.5391\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6753 - accuracy: 0.5742 - val_loss: 0.6952 - val_accuracy: 0.5352\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6753 - accuracy: 0.5762 - val_loss: 0.6810 - val_accuracy: 0.5547\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6720 - accuracy: 0.5732 - val_loss: 0.7067 - val_accuracy: 0.5312\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6670 - accuracy: 0.5820 - val_loss: 0.6902 - val_accuracy: 0.5273\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6672 - accuracy: 0.5791 - val_loss: 0.6969 - val_accuracy: 0.5430\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6668 - accuracy: 0.5889 - val_loss: 0.7099 - val_accuracy: 0.5391\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6679 - accuracy: 0.5869 - val_loss: 0.6937 - val_accuracy: 0.5352\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6625 - accuracy: 0.5928 - val_loss: 0.7131 - val_accuracy: 0.5430\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6651 - accuracy: 0.5811 - val_loss: 0.6958 - val_accuracy: 0.5352\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6643 - accuracy: 0.5986 - val_loss: 0.6931 - val_accuracy: 0.5664\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6620 - accuracy: 0.5869 - val_loss: 0.6965 - val_accuracy: 0.5391\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6582 - accuracy: 0.5928 - val_loss: 0.6878 - val_accuracy: 0.5469\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6603 - accuracy: 0.5811 - val_loss: 0.6923 - val_accuracy: 0.5977\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6586 - accuracy: 0.5957 - val_loss: 0.6859 - val_accuracy: 0.5547\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6593 - accuracy: 0.5996 - val_loss: 0.7059 - val_accuracy: 0.5508\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6566 - accuracy: 0.6113 - val_loss: 0.6888 - val_accuracy: 0.5430\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6579 - accuracy: 0.5957 - val_loss: 0.6859 - val_accuracy: 0.5703\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 10ms/step - loss: 0.6981 - accuracy: 0.4980 - val_loss: 0.6853 - val_accuracy: 0.5469\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6929 - accuracy: 0.5049 - val_loss: 0.6857 - val_accuracy: 0.5273\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6886 - accuracy: 0.5283 - val_loss: 0.6986 - val_accuracy: 0.5391\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6911 - accuracy: 0.5352 - val_loss: 0.7007 - val_accuracy: 0.5430\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6874 - accuracy: 0.5361 - val_loss: 0.6849 - val_accuracy: 0.5469\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6866 - accuracy: 0.5518 - val_loss: 0.6872 - val_accuracy: 0.5664\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6852 - accuracy: 0.5361 - val_loss: 0.6904 - val_accuracy: 0.5664\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6840 - accuracy: 0.5566 - val_loss: 0.6901 - val_accuracy: 0.5547\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6858 - accuracy: 0.5518 - val_loss: 0.6880 - val_accuracy: 0.5703\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6836 - accuracy: 0.5400 - val_loss: 0.7008 - val_accuracy: 0.5547\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6877 - accuracy: 0.5596 - val_loss: 0.6903 - val_accuracy: 0.5547\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6821 - accuracy: 0.5605 - val_loss: 0.6900 - val_accuracy: 0.5430\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6826 - accuracy: 0.5527 - val_loss: 0.6812 - val_accuracy: 0.5586\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6813 - accuracy: 0.5498 - val_loss: 0.6860 - val_accuracy: 0.5703\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6812 - accuracy: 0.5645 - val_loss: 0.6873 - val_accuracy: 0.5586\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6793 - accuracy: 0.5635 - val_loss: 0.6872 - val_accuracy: 0.5820\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6798 - accuracy: 0.5684 - val_loss: 0.6948 - val_accuracy: 0.5703\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6792 - accuracy: 0.5527 - val_loss: 0.6947 - val_accuracy: 0.5703\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6799 - accuracy: 0.5723 - val_loss: 0.6912 - val_accuracy: 0.5664\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6784 - accuracy: 0.5723 - val_loss: 0.6862 - val_accuracy: 0.5625\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 0.6980 - accuracy: 0.4912 - val_loss: 0.6877 - val_accuracy: 0.5117\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6934 - accuracy: 0.5215 - val_loss: 0.6998 - val_accuracy: 0.5508\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6888 - accuracy: 0.5254 - val_loss: 0.6906 - val_accuracy: 0.5273\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6866 - accuracy: 0.5439 - val_loss: 0.6943 - val_accuracy: 0.5312\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6867 - accuracy: 0.5488 - val_loss: 0.7081 - val_accuracy: 0.5703\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6824 - accuracy: 0.5557 - val_loss: 0.6992 - val_accuracy: 0.5859\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6800 - accuracy: 0.5645 - val_loss: 0.7056 - val_accuracy: 0.5625\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6808 - accuracy: 0.5762 - val_loss: 0.6989 - val_accuracy: 0.5430\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6796 - accuracy: 0.5586 - val_loss: 0.7003 - val_accuracy: 0.5820\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6781 - accuracy: 0.5576 - val_loss: 0.7065 - val_accuracy: 0.5859\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6772 - accuracy: 0.5654 - val_loss: 0.7066 - val_accuracy: 0.5391\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6763 - accuracy: 0.5732 - val_loss: 0.7067 - val_accuracy: 0.5781\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6773 - accuracy: 0.5732 - val_loss: 0.7052 - val_accuracy: 0.5859\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6756 - accuracy: 0.5645 - val_loss: 0.7039 - val_accuracy: 0.5781\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6772 - accuracy: 0.5664 - val_loss: 0.7101 - val_accuracy: 0.5508\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6733 - accuracy: 0.5869 - val_loss: 0.7015 - val_accuracy: 0.6094\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6732 - accuracy: 0.5762 - val_loss: 0.6999 - val_accuracy: 0.5938\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6743 - accuracy: 0.5791 - val_loss: 0.7088 - val_accuracy: 0.5859\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6744 - accuracy: 0.5693 - val_loss: 0.7080 - val_accuracy: 0.5898\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6713 - accuracy: 0.5791 - val_loss: 0.7076 - val_accuracy: 0.5781\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 1s 10ms/step - loss: 0.6987 - accuracy: 0.4922 - val_loss: 0.6984 - val_accuracy: 0.5078\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6877 - accuracy: 0.5518 - val_loss: 0.7004 - val_accuracy: 0.4922\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6830 - accuracy: 0.5234 - val_loss: 0.7119 - val_accuracy: 0.4844\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6799 - accuracy: 0.5693 - val_loss: 0.7120 - val_accuracy: 0.4766\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6799 - accuracy: 0.5449 - val_loss: 0.7112 - val_accuracy: 0.4570\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6787 - accuracy: 0.5381 - val_loss: 0.7183 - val_accuracy: 0.4609\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6787 - accuracy: 0.5508 - val_loss: 0.7132 - val_accuracy: 0.4727\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6772 - accuracy: 0.5625 - val_loss: 0.7191 - val_accuracy: 0.4961\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6753 - accuracy: 0.5576 - val_loss: 0.7151 - val_accuracy: 0.4492\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6738 - accuracy: 0.5586 - val_loss: 0.7174 - val_accuracy: 0.4922\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6743 - accuracy: 0.5488 - val_loss: 0.7154 - val_accuracy: 0.4531\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6770 - accuracy: 0.5684 - val_loss: 0.7185 - val_accuracy: 0.4609\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6732 - accuracy: 0.5615 - val_loss: 0.7161 - val_accuracy: 0.4844\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6722 - accuracy: 0.5488 - val_loss: 0.7167 - val_accuracy: 0.4961\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6743 - accuracy: 0.5645 - val_loss: 0.7116 - val_accuracy: 0.4492\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6747 - accuracy: 0.5391 - val_loss: 0.7149 - val_accuracy: 0.4961\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6723 - accuracy: 0.5664 - val_loss: 0.7160 - val_accuracy: 0.4805\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6719 - accuracy: 0.5508 - val_loss: 0.7190 - val_accuracy: 0.4805\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6688 - accuracy: 0.5605 - val_loss: 0.7127 - val_accuracy: 0.5117\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6716 - accuracy: 0.5654 - val_loss: 0.7134 - val_accuracy: 0.5117\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 17ms/step - loss: 0.6938 - accuracy: 0.5283 - val_loss: 0.6991 - val_accuracy: 0.4883\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6851 - accuracy: 0.5410 - val_loss: 0.7073 - val_accuracy: 0.5312\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6809 - accuracy: 0.5605 - val_loss: 0.7065 - val_accuracy: 0.5312\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6779 - accuracy: 0.5732 - val_loss: 0.7073 - val_accuracy: 0.5039\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6758 - accuracy: 0.5625 - val_loss: 0.7030 - val_accuracy: 0.5195\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6731 - accuracy: 0.5674 - val_loss: 0.7090 - val_accuracy: 0.5039\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6724 - accuracy: 0.5684 - val_loss: 0.7121 - val_accuracy: 0.5352\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6693 - accuracy: 0.5859 - val_loss: 0.7105 - val_accuracy: 0.5273\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6688 - accuracy: 0.5742 - val_loss: 0.7018 - val_accuracy: 0.5430\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6657 - accuracy: 0.5820 - val_loss: 0.7083 - val_accuracy: 0.5391\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6662 - accuracy: 0.5664 - val_loss: 0.7031 - val_accuracy: 0.5234\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6677 - accuracy: 0.5723 - val_loss: 0.7071 - val_accuracy: 0.5117\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6642 - accuracy: 0.5967 - val_loss: 0.7074 - val_accuracy: 0.5352\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6620 - accuracy: 0.5850 - val_loss: 0.7005 - val_accuracy: 0.5273\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6624 - accuracy: 0.5908 - val_loss: 0.7000 - val_accuracy: 0.5508\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6601 - accuracy: 0.5947 - val_loss: 0.7022 - val_accuracy: 0.5664\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6623 - accuracy: 0.5859 - val_loss: 0.7101 - val_accuracy: 0.5156\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6597 - accuracy: 0.6094 - val_loss: 0.7014 - val_accuracy: 0.5469\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6588 - accuracy: 0.5918 - val_loss: 0.6953 - val_accuracy: 0.5547\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6579 - accuracy: 0.6074 - val_loss: 0.6990 - val_accuracy: 0.5625\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 1s 9ms/step - loss: 0.6951 - accuracy: 0.5361 - val_loss: 0.6906 - val_accuracy: 0.5156\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6883 - accuracy: 0.5508 - val_loss: 0.6901 - val_accuracy: 0.5391\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6876 - accuracy: 0.5391 - val_loss: 0.6845 - val_accuracy: 0.5391\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6948 - accuracy: 0.5449 - val_loss: 0.6897 - val_accuracy: 0.5742\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6846 - accuracy: 0.5674 - val_loss: 0.6919 - val_accuracy: 0.5391\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6864 - accuracy: 0.5293 - val_loss: 0.6919 - val_accuracy: 0.5547\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6836 - accuracy: 0.5527 - val_loss: 0.7010 - val_accuracy: 0.5469\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6838 - accuracy: 0.5684 - val_loss: 0.6860 - val_accuracy: 0.5781\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6820 - accuracy: 0.5537 - val_loss: 0.7062 - val_accuracy: 0.5625\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6891 - accuracy: 0.5596 - val_loss: 0.6935 - val_accuracy: 0.5352\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6844 - accuracy: 0.5547 - val_loss: 0.6897 - val_accuracy: 0.5664\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6828 - accuracy: 0.5684 - val_loss: 0.6904 - val_accuracy: 0.5664\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6799 - accuracy: 0.5654 - val_loss: 0.6960 - val_accuracy: 0.5430\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6798 - accuracy: 0.5664 - val_loss: 0.6921 - val_accuracy: 0.5703\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6793 - accuracy: 0.5732 - val_loss: 0.6976 - val_accuracy: 0.5664\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6787 - accuracy: 0.5654 - val_loss: 0.6956 - val_accuracy: 0.5586\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6783 - accuracy: 0.5420 - val_loss: 0.6971 - val_accuracy: 0.5469\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6776 - accuracy: 0.5645 - val_loss: 0.6990 - val_accuracy: 0.5547\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6792 - accuracy: 0.5596 - val_loss: 0.7014 - val_accuracy: 0.5469\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6783 - accuracy: 0.5654 - val_loss: 0.6945 - val_accuracy: 0.5430\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 1s 10ms/step - loss: 0.6971 - accuracy: 0.5283 - val_loss: 0.6898 - val_accuracy: 0.5391\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6924 - accuracy: 0.5312 - val_loss: 0.7008 - val_accuracy: 0.5234\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6877 - accuracy: 0.5449 - val_loss: 0.6917 - val_accuracy: 0.5625\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6867 - accuracy: 0.5527 - val_loss: 0.7026 - val_accuracy: 0.5508\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6848 - accuracy: 0.5576 - val_loss: 0.6902 - val_accuracy: 0.5664\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6846 - accuracy: 0.5576 - val_loss: 0.6977 - val_accuracy: 0.5781\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6825 - accuracy: 0.5557 - val_loss: 0.6967 - val_accuracy: 0.5352\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6803 - accuracy: 0.5713 - val_loss: 0.6934 - val_accuracy: 0.5664\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6800 - accuracy: 0.5742 - val_loss: 0.6953 - val_accuracy: 0.5625\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6808 - accuracy: 0.5586 - val_loss: 0.6887 - val_accuracy: 0.5820\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6819 - accuracy: 0.5830 - val_loss: 0.6904 - val_accuracy: 0.5664\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6772 - accuracy: 0.5752 - val_loss: 0.6980 - val_accuracy: 0.5430\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6762 - accuracy: 0.5791 - val_loss: 0.6865 - val_accuracy: 0.5508\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6793 - accuracy: 0.5723 - val_loss: 0.7015 - val_accuracy: 0.5703\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6747 - accuracy: 0.5850 - val_loss: 0.6911 - val_accuracy: 0.5703\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6757 - accuracy: 0.5684 - val_loss: 0.6882 - val_accuracy: 0.5547\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6744 - accuracy: 0.5811 - val_loss: 0.6915 - val_accuracy: 0.5742\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6735 - accuracy: 0.5684 - val_loss: 0.6981 - val_accuracy: 0.5586\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6730 - accuracy: 0.5752 - val_loss: 0.6964 - val_accuracy: 0.5781\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6730 - accuracy: 0.5713 - val_loss: 0.7008 - val_accuracy: 0.5664\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 1s 10ms/step - loss: 0.6976 - accuracy: 0.5029 - val_loss: 0.6935 - val_accuracy: 0.5430\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6837 - accuracy: 0.5527 - val_loss: 0.7050 - val_accuracy: 0.5156\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6802 - accuracy: 0.5596 - val_loss: 0.7043 - val_accuracy: 0.5156\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6793 - accuracy: 0.5527 - val_loss: 0.7057 - val_accuracy: 0.5273\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6799 - accuracy: 0.5459 - val_loss: 0.7026 - val_accuracy: 0.5195\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6789 - accuracy: 0.5605 - val_loss: 0.7038 - val_accuracy: 0.5547\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6786 - accuracy: 0.5459 - val_loss: 0.7101 - val_accuracy: 0.5273\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6762 - accuracy: 0.5615 - val_loss: 0.7059 - val_accuracy: 0.5312\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6767 - accuracy: 0.5674 - val_loss: 0.7076 - val_accuracy: 0.5234\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6765 - accuracy: 0.5537 - val_loss: 0.7068 - val_accuracy: 0.5273\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6760 - accuracy: 0.5684 - val_loss: 0.7074 - val_accuracy: 0.5273\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6743 - accuracy: 0.5693 - val_loss: 0.7067 - val_accuracy: 0.5430\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6739 - accuracy: 0.5605 - val_loss: 0.7086 - val_accuracy: 0.5508\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6734 - accuracy: 0.5605 - val_loss: 0.7087 - val_accuracy: 0.5352\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6740 - accuracy: 0.5654 - val_loss: 0.7057 - val_accuracy: 0.5391\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6723 - accuracy: 0.5576 - val_loss: 0.7062 - val_accuracy: 0.5352\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6774 - accuracy: 0.5791 - val_loss: 0.7094 - val_accuracy: 0.5273\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6750 - accuracy: 0.5371 - val_loss: 0.7071 - val_accuracy: 0.5352\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6741 - accuracy: 0.5693 - val_loss: 0.7027 - val_accuracy: 0.5430\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6736 - accuracy: 0.5459 - val_loss: 0.7069 - val_accuracy: 0.5273\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 1s 9ms/step - loss: 0.6973 - accuracy: 0.5156 - val_loss: 0.7103 - val_accuracy: 0.5195\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6829 - accuracy: 0.5469 - val_loss: 0.7096 - val_accuracy: 0.5312\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6791 - accuracy: 0.5684 - val_loss: 0.7079 - val_accuracy: 0.5508\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6769 - accuracy: 0.5693 - val_loss: 0.7028 - val_accuracy: 0.5586\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6764 - accuracy: 0.5635 - val_loss: 0.7055 - val_accuracy: 0.5586\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6758 - accuracy: 0.5625 - val_loss: 0.7042 - val_accuracy: 0.5664\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6740 - accuracy: 0.5781 - val_loss: 0.7042 - val_accuracy: 0.5664\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6717 - accuracy: 0.5830 - val_loss: 0.7044 - val_accuracy: 0.5664\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6705 - accuracy: 0.5791 - val_loss: 0.7074 - val_accuracy: 0.5430\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6728 - accuracy: 0.5801 - val_loss: 0.7012 - val_accuracy: 0.5664\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6683 - accuracy: 0.5703 - val_loss: 0.6988 - val_accuracy: 0.5742\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6712 - accuracy: 0.5850 - val_loss: 0.7030 - val_accuracy: 0.5625\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6691 - accuracy: 0.5703 - val_loss: 0.7028 - val_accuracy: 0.5703\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6671 - accuracy: 0.5830 - val_loss: 0.7016 - val_accuracy: 0.5742\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6697 - accuracy: 0.5586 - val_loss: 0.7056 - val_accuracy: 0.5664\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6675 - accuracy: 0.5762 - val_loss: 0.7050 - val_accuracy: 0.5312\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6660 - accuracy: 0.5840 - val_loss: 0.6976 - val_accuracy: 0.5938\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6652 - accuracy: 0.5918 - val_loss: 0.7068 - val_accuracy: 0.5586\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6641 - accuracy: 0.6035 - val_loss: 0.6967 - val_accuracy: 0.5430\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6656 - accuracy: 0.5762 - val_loss: 0.7027 - val_accuracy: 0.5664\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 1s 9ms/step - loss: 0.6936 - accuracy: 0.5244 - val_loss: 0.6970 - val_accuracy: 0.5117\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6923 - accuracy: 0.5479 - val_loss: 0.7042 - val_accuracy: 0.5312\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6870 - accuracy: 0.5215 - val_loss: 0.7021 - val_accuracy: 0.5078\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6848 - accuracy: 0.5508 - val_loss: 0.6996 - val_accuracy: 0.5273\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6833 - accuracy: 0.5771 - val_loss: 0.6949 - val_accuracy: 0.5195\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6838 - accuracy: 0.5596 - val_loss: 0.6942 - val_accuracy: 0.5469\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6802 - accuracy: 0.5723 - val_loss: 0.7002 - val_accuracy: 0.5195\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6816 - accuracy: 0.5840 - val_loss: 0.7015 - val_accuracy: 0.5117\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6814 - accuracy: 0.5713 - val_loss: 0.6967 - val_accuracy: 0.5195\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6783 - accuracy: 0.5869 - val_loss: 0.6966 - val_accuracy: 0.5078\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6779 - accuracy: 0.5771 - val_loss: 0.6935 - val_accuracy: 0.5078\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6771 - accuracy: 0.5967 - val_loss: 0.6962 - val_accuracy: 0.5117\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6790 - accuracy: 0.5830 - val_loss: 0.6921 - val_accuracy: 0.5312\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6743 - accuracy: 0.5723 - val_loss: 0.7015 - val_accuracy: 0.5195\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6765 - accuracy: 0.5801 - val_loss: 0.6913 - val_accuracy: 0.5391\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6734 - accuracy: 0.5850 - val_loss: 0.6958 - val_accuracy: 0.5312\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6737 - accuracy: 0.5918 - val_loss: 0.6925 - val_accuracy: 0.5352\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6737 - accuracy: 0.5820 - val_loss: 0.7067 - val_accuracy: 0.5391\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6794 - accuracy: 0.5762 - val_loss: 0.6879 - val_accuracy: 0.5781\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6761 - accuracy: 0.5830 - val_loss: 0.6949 - val_accuracy: 0.5312\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 14ms/step - loss: 0.6998 - accuracy: 0.5156 - val_loss: 0.6968 - val_accuracy: 0.4766\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6887 - accuracy: 0.5430 - val_loss: 0.6923 - val_accuracy: 0.5312\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6871 - accuracy: 0.5625 - val_loss: 0.6864 - val_accuracy: 0.5117\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6825 - accuracy: 0.5420 - val_loss: 0.6932 - val_accuracy: 0.5352\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6845 - accuracy: 0.5420 - val_loss: 0.6920 - val_accuracy: 0.5664\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6833 - accuracy: 0.5635 - val_loss: 0.6900 - val_accuracy: 0.5078\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6794 - accuracy: 0.5557 - val_loss: 0.6915 - val_accuracy: 0.5273\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6790 - accuracy: 0.5605 - val_loss: 0.6938 - val_accuracy: 0.5430\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6781 - accuracy: 0.5850 - val_loss: 0.6968 - val_accuracy: 0.5430\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6760 - accuracy: 0.5576 - val_loss: 0.7005 - val_accuracy: 0.5156\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6769 - accuracy: 0.5430 - val_loss: 0.6944 - val_accuracy: 0.5234\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6741 - accuracy: 0.5781 - val_loss: 0.6977 - val_accuracy: 0.5469\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6731 - accuracy: 0.5781 - val_loss: 0.7004 - val_accuracy: 0.5117\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6755 - accuracy: 0.5654 - val_loss: 0.6941 - val_accuracy: 0.5430\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6736 - accuracy: 0.5713 - val_loss: 0.6928 - val_accuracy: 0.5508\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6798 - accuracy: 0.5664 - val_loss: 0.6931 - val_accuracy: 0.5234\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6740 - accuracy: 0.5586 - val_loss: 0.6942 - val_accuracy: 0.5430\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6728 - accuracy: 0.5723 - val_loss: 0.6983 - val_accuracy: 0.5273\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6713 - accuracy: 0.5752 - val_loss: 0.6988 - val_accuracy: 0.5352\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6707 - accuracy: 0.5801 - val_loss: 0.6937 - val_accuracy: 0.5312\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 1s 10ms/step - loss: 0.7139 - accuracy: 0.5146 - val_loss: 0.7037 - val_accuracy: 0.4805\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6827 - accuracy: 0.5469 - val_loss: 0.6987 - val_accuracy: 0.5352\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6826 - accuracy: 0.5303 - val_loss: 0.6996 - val_accuracy: 0.5039\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6827 - accuracy: 0.5664 - val_loss: 0.6984 - val_accuracy: 0.5469\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6803 - accuracy: 0.5566 - val_loss: 0.7013 - val_accuracy: 0.5273\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6808 - accuracy: 0.5527 - val_loss: 0.7002 - val_accuracy: 0.5391\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6773 - accuracy: 0.5586 - val_loss: 0.7016 - val_accuracy: 0.5273\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6771 - accuracy: 0.5547 - val_loss: 0.6995 - val_accuracy: 0.5391\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6806 - accuracy: 0.5518 - val_loss: 0.6948 - val_accuracy: 0.5586\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6763 - accuracy: 0.5605 - val_loss: 0.6974 - val_accuracy: 0.5117\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6743 - accuracy: 0.5576 - val_loss: 0.6993 - val_accuracy: 0.5547\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6753 - accuracy: 0.5430 - val_loss: 0.7012 - val_accuracy: 0.5078\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6735 - accuracy: 0.5439 - val_loss: 0.7003 - val_accuracy: 0.5312\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6728 - accuracy: 0.5752 - val_loss: 0.6974 - val_accuracy: 0.5469\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6742 - accuracy: 0.5693 - val_loss: 0.7047 - val_accuracy: 0.5039\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6710 - accuracy: 0.5791 - val_loss: 0.6964 - val_accuracy: 0.5352\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6722 - accuracy: 0.5781 - val_loss: 0.7012 - val_accuracy: 0.5352\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6698 - accuracy: 0.5645 - val_loss: 0.6990 - val_accuracy: 0.5273\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6693 - accuracy: 0.5566 - val_loss: 0.6994 - val_accuracy: 0.5430\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6698 - accuracy: 0.5557 - val_loss: 0.7007 - val_accuracy: 0.5469\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 1s 12ms/step - loss: 0.6998 - accuracy: 0.5381 - val_loss: 0.6930 - val_accuracy: 0.5156\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6847 - accuracy: 0.5664 - val_loss: 0.6998 - val_accuracy: 0.5195\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6846 - accuracy: 0.5693 - val_loss: 0.7042 - val_accuracy: 0.5078\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6826 - accuracy: 0.5791 - val_loss: 0.6990 - val_accuracy: 0.5352\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6805 - accuracy: 0.5781 - val_loss: 0.7009 - val_accuracy: 0.5273\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6792 - accuracy: 0.5859 - val_loss: 0.7003 - val_accuracy: 0.5273\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6807 - accuracy: 0.5508 - val_loss: 0.7061 - val_accuracy: 0.5195\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6784 - accuracy: 0.5781 - val_loss: 0.7002 - val_accuracy: 0.5195\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6780 - accuracy: 0.5771 - val_loss: 0.7005 - val_accuracy: 0.5312\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6780 - accuracy: 0.5527 - val_loss: 0.7043 - val_accuracy: 0.5195\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6763 - accuracy: 0.5762 - val_loss: 0.6985 - val_accuracy: 0.5312\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6762 - accuracy: 0.5811 - val_loss: 0.7034 - val_accuracy: 0.5312\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6761 - accuracy: 0.5684 - val_loss: 0.7030 - val_accuracy: 0.5391\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6761 - accuracy: 0.5693 - val_loss: 0.7029 - val_accuracy: 0.5156\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6761 - accuracy: 0.5762 - val_loss: 0.7029 - val_accuracy: 0.5312\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6722 - accuracy: 0.5791 - val_loss: 0.6999 - val_accuracy: 0.5312\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6717 - accuracy: 0.5859 - val_loss: 0.6999 - val_accuracy: 0.5156\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6713 - accuracy: 0.5898 - val_loss: 0.6955 - val_accuracy: 0.5156\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6720 - accuracy: 0.5801 - val_loss: 0.7026 - val_accuracy: 0.5273\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6721 - accuracy: 0.5645 - val_loss: 0.7053 - val_accuracy: 0.5234\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 1s 9ms/step - loss: 0.6935 - accuracy: 0.5293 - val_loss: 0.7001 - val_accuracy: 0.4766\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6883 - accuracy: 0.5332 - val_loss: 0.6984 - val_accuracy: 0.5039\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6874 - accuracy: 0.5586 - val_loss: 0.7009 - val_accuracy: 0.5000\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6826 - accuracy: 0.5713 - val_loss: 0.6930 - val_accuracy: 0.5117\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6835 - accuracy: 0.5859 - val_loss: 0.6953 - val_accuracy: 0.5117\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6805 - accuracy: 0.5693 - val_loss: 0.7052 - val_accuracy: 0.5508\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6815 - accuracy: 0.5820 - val_loss: 0.6983 - val_accuracy: 0.5117\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6773 - accuracy: 0.5674 - val_loss: 0.6924 - val_accuracy: 0.5391\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6763 - accuracy: 0.5791 - val_loss: 0.6925 - val_accuracy: 0.5234\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6778 - accuracy: 0.5664 - val_loss: 0.6965 - val_accuracy: 0.5625\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6756 - accuracy: 0.5732 - val_loss: 0.6952 - val_accuracy: 0.5508\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6778 - accuracy: 0.5576 - val_loss: 0.7065 - val_accuracy: 0.5039\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6771 - accuracy: 0.5723 - val_loss: 0.7035 - val_accuracy: 0.5156\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6734 - accuracy: 0.5811 - val_loss: 0.6909 - val_accuracy: 0.5781\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6760 - accuracy: 0.5654 - val_loss: 0.6934 - val_accuracy: 0.5586\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6720 - accuracy: 0.5947 - val_loss: 0.6951 - val_accuracy: 0.5312\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6695 - accuracy: 0.5830 - val_loss: 0.6935 - val_accuracy: 0.5547\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6717 - accuracy: 0.5898 - val_loss: 0.7050 - val_accuracy: 0.5352\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6718 - accuracy: 0.5938 - val_loss: 0.7001 - val_accuracy: 0.5352\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6709 - accuracy: 0.5684 - val_loss: 0.6930 - val_accuracy: 0.5508\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "       frontal  central  parietal  occipital\n",
            "theta    56.64    55.47     57.03      56.25\n",
            "alpha    57.81    51.17     56.25      54.30\n",
            "beta     56.64    52.73     56.64      53.12\n",
            "gamma    53.12    54.69     52.34      55.08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy(\"valence\",\"ann\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4WbY9-Onyqy",
        "outputId": "7b2452f5-5fc1-485b-b6a7-15abd3e0e636"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "32/32 [==============================] - 1s 9ms/step - loss: 0.6948 - accuracy: 0.5273 - val_loss: 0.6992 - val_accuracy: 0.5352\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6889 - accuracy: 0.5537 - val_loss: 0.6988 - val_accuracy: 0.5391\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6867 - accuracy: 0.5586 - val_loss: 0.6918 - val_accuracy: 0.5430\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6874 - accuracy: 0.5537 - val_loss: 0.6889 - val_accuracy: 0.5352\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5576 - val_loss: 0.6940 - val_accuracy: 0.5312\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6853 - accuracy: 0.5615 - val_loss: 0.6957 - val_accuracy: 0.5352\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6841 - accuracy: 0.5654 - val_loss: 0.6942 - val_accuracy: 0.5391\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6840 - accuracy: 0.5674 - val_loss: 0.6944 - val_accuracy: 0.5430\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6855 - accuracy: 0.5527 - val_loss: 0.6971 - val_accuracy: 0.5391\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6828 - accuracy: 0.5703 - val_loss: 0.6889 - val_accuracy: 0.5312\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6825 - accuracy: 0.5674 - val_loss: 0.6910 - val_accuracy: 0.5430\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6831 - accuracy: 0.5654 - val_loss: 0.6868 - val_accuracy: 0.5352\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6833 - accuracy: 0.5674 - val_loss: 0.6968 - val_accuracy: 0.5352\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6822 - accuracy: 0.5703 - val_loss: 0.6922 - val_accuracy: 0.5430\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6809 - accuracy: 0.5713 - val_loss: 0.6908 - val_accuracy: 0.5352\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6806 - accuracy: 0.5771 - val_loss: 0.6894 - val_accuracy: 0.5430\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6819 - accuracy: 0.5693 - val_loss: 0.6888 - val_accuracy: 0.5352\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6800 - accuracy: 0.5771 - val_loss: 0.6908 - val_accuracy: 0.5430\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6813 - accuracy: 0.5684 - val_loss: 0.6857 - val_accuracy: 0.5430\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6812 - accuracy: 0.5713 - val_loss: 0.6948 - val_accuracy: 0.5430\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 1s 9ms/step - loss: 0.6924 - accuracy: 0.5527 - val_loss: 0.6987 - val_accuracy: 0.5000\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6852 - accuracy: 0.5684 - val_loss: 0.6944 - val_accuracy: 0.5078\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6854 - accuracy: 0.5605 - val_loss: 0.6985 - val_accuracy: 0.5117\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6839 - accuracy: 0.5684 - val_loss: 0.6934 - val_accuracy: 0.5156\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6828 - accuracy: 0.5674 - val_loss: 0.6940 - val_accuracy: 0.5156\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6826 - accuracy: 0.5742 - val_loss: 0.6945 - val_accuracy: 0.5117\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6820 - accuracy: 0.5752 - val_loss: 0.6948 - val_accuracy: 0.5273\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6817 - accuracy: 0.5654 - val_loss: 0.6912 - val_accuracy: 0.5156\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6817 - accuracy: 0.5674 - val_loss: 0.6917 - val_accuracy: 0.5234\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6796 - accuracy: 0.5703 - val_loss: 0.6949 - val_accuracy: 0.5078\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6807 - accuracy: 0.5713 - val_loss: 0.6947 - val_accuracy: 0.5078\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6804 - accuracy: 0.5742 - val_loss: 0.6937 - val_accuracy: 0.5039\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6804 - accuracy: 0.5752 - val_loss: 0.6928 - val_accuracy: 0.5234\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6809 - accuracy: 0.5664 - val_loss: 0.6936 - val_accuracy: 0.5117\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6803 - accuracy: 0.5635 - val_loss: 0.6891 - val_accuracy: 0.5234\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6804 - accuracy: 0.5713 - val_loss: 0.6980 - val_accuracy: 0.5117\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6798 - accuracy: 0.5752 - val_loss: 0.6932 - val_accuracy: 0.5117\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6781 - accuracy: 0.5723 - val_loss: 0.6908 - val_accuracy: 0.5195\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6765 - accuracy: 0.5732 - val_loss: 0.6939 - val_accuracy: 0.5195\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6778 - accuracy: 0.5713 - val_loss: 0.6914 - val_accuracy: 0.5195\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 1s 9ms/step - loss: 0.7035 - accuracy: 0.4971 - val_loss: 0.6968 - val_accuracy: 0.5352\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6887 - accuracy: 0.5400 - val_loss: 0.6985 - val_accuracy: 0.5234\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.5400 - val_loss: 0.7015 - val_accuracy: 0.5078\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6845 - accuracy: 0.5596 - val_loss: 0.7035 - val_accuracy: 0.5078\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6851 - accuracy: 0.5527 - val_loss: 0.7048 - val_accuracy: 0.5000\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6836 - accuracy: 0.5547 - val_loss: 0.7033 - val_accuracy: 0.4883\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6819 - accuracy: 0.5547 - val_loss: 0.6998 - val_accuracy: 0.5000\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6810 - accuracy: 0.5566 - val_loss: 0.7000 - val_accuracy: 0.5195\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6807 - accuracy: 0.5703 - val_loss: 0.6993 - val_accuracy: 0.5156\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6808 - accuracy: 0.5615 - val_loss: 0.7022 - val_accuracy: 0.4844\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6802 - accuracy: 0.5635 - val_loss: 0.6955 - val_accuracy: 0.5078\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6796 - accuracy: 0.5713 - val_loss: 0.6987 - val_accuracy: 0.5039\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6793 - accuracy: 0.5664 - val_loss: 0.6948 - val_accuracy: 0.5156\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6796 - accuracy: 0.5625 - val_loss: 0.7027 - val_accuracy: 0.5039\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6787 - accuracy: 0.5771 - val_loss: 0.6980 - val_accuracy: 0.5078\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6789 - accuracy: 0.5684 - val_loss: 0.7015 - val_accuracy: 0.5039\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6782 - accuracy: 0.5684 - val_loss: 0.6966 - val_accuracy: 0.4922\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6764 - accuracy: 0.5703 - val_loss: 0.7028 - val_accuracy: 0.4922\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6761 - accuracy: 0.5771 - val_loss: 0.6917 - val_accuracy: 0.5273\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6776 - accuracy: 0.5732 - val_loss: 0.6866 - val_accuracy: 0.5195\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 1s 10ms/step - loss: 0.6969 - accuracy: 0.5059 - val_loss: 0.6896 - val_accuracy: 0.5430\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6881 - accuracy: 0.5586 - val_loss: 0.6978 - val_accuracy: 0.5195\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6852 - accuracy: 0.5527 - val_loss: 0.6986 - val_accuracy: 0.5195\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6880 - accuracy: 0.5557 - val_loss: 0.6991 - val_accuracy: 0.5156\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6845 - accuracy: 0.5664 - val_loss: 0.6994 - val_accuracy: 0.5195\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6848 - accuracy: 0.5615 - val_loss: 0.7049 - val_accuracy: 0.5234\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6827 - accuracy: 0.5664 - val_loss: 0.6964 - val_accuracy: 0.5195\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6831 - accuracy: 0.5654 - val_loss: 0.6986 - val_accuracy: 0.5156\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6830 - accuracy: 0.5664 - val_loss: 0.7024 - val_accuracy: 0.5234\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6826 - accuracy: 0.5605 - val_loss: 0.6967 - val_accuracy: 0.5117\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6841 - accuracy: 0.5625 - val_loss: 0.6968 - val_accuracy: 0.5234\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6805 - accuracy: 0.5635 - val_loss: 0.7050 - val_accuracy: 0.5195\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6833 - accuracy: 0.5684 - val_loss: 0.6998 - val_accuracy: 0.5195\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6801 - accuracy: 0.5557 - val_loss: 0.7009 - val_accuracy: 0.5156\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6794 - accuracy: 0.5674 - val_loss: 0.6975 - val_accuracy: 0.5156\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6805 - accuracy: 0.5605 - val_loss: 0.6996 - val_accuracy: 0.5078\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6800 - accuracy: 0.5713 - val_loss: 0.6994 - val_accuracy: 0.5234\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6796 - accuracy: 0.5596 - val_loss: 0.7011 - val_accuracy: 0.5195\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6827 - accuracy: 0.5703 - val_loss: 0.6982 - val_accuracy: 0.5156\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6787 - accuracy: 0.5635 - val_loss: 0.6971 - val_accuracy: 0.5195\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 1s 14ms/step - loss: 0.6986 - accuracy: 0.5029 - val_loss: 0.7001 - val_accuracy: 0.5391\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6914 - accuracy: 0.5508 - val_loss: 0.7036 - val_accuracy: 0.5352\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6870 - accuracy: 0.5518 - val_loss: 0.6945 - val_accuracy: 0.5391\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6861 - accuracy: 0.5498 - val_loss: 0.6966 - val_accuracy: 0.5312\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6858 - accuracy: 0.5596 - val_loss: 0.6933 - val_accuracy: 0.5391\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6854 - accuracy: 0.5625 - val_loss: 0.6966 - val_accuracy: 0.5352\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6843 - accuracy: 0.5664 - val_loss: 0.6929 - val_accuracy: 0.5352\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6854 - accuracy: 0.5557 - val_loss: 0.6974 - val_accuracy: 0.5352\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6855 - accuracy: 0.5557 - val_loss: 0.6903 - val_accuracy: 0.5312\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6835 - accuracy: 0.5654 - val_loss: 0.6951 - val_accuracy: 0.5352\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6828 - accuracy: 0.5645 - val_loss: 0.6893 - val_accuracy: 0.5352\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6853 - accuracy: 0.5635 - val_loss: 0.6889 - val_accuracy: 0.5391\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6858 - accuracy: 0.5605 - val_loss: 0.6968 - val_accuracy: 0.5312\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6831 - accuracy: 0.5654 - val_loss: 0.6904 - val_accuracy: 0.5352\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6845 - accuracy: 0.5664 - val_loss: 0.6901 - val_accuracy: 0.5352\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6808 - accuracy: 0.5684 - val_loss: 0.6914 - val_accuracy: 0.5391\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6813 - accuracy: 0.5635 - val_loss: 0.6944 - val_accuracy: 0.5352\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6812 - accuracy: 0.5684 - val_loss: 0.6931 - val_accuracy: 0.5352\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6807 - accuracy: 0.5635 - val_loss: 0.6912 - val_accuracy: 0.5352\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6813 - accuracy: 0.5684 - val_loss: 0.6938 - val_accuracy: 0.5352\n",
            "8/8 [==============================] - 0s 2ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 1s 10ms/step - loss: 0.6926 - accuracy: 0.5391 - val_loss: 0.6934 - val_accuracy: 0.5234\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6865 - accuracy: 0.5576 - val_loss: 0.6963 - val_accuracy: 0.5234\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6850 - accuracy: 0.5635 - val_loss: 0.6971 - val_accuracy: 0.5273\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6846 - accuracy: 0.5684 - val_loss: 0.6931 - val_accuracy: 0.5195\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6861 - accuracy: 0.5684 - val_loss: 0.6961 - val_accuracy: 0.5195\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6849 - accuracy: 0.5684 - val_loss: 0.6978 - val_accuracy: 0.5195\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6832 - accuracy: 0.5781 - val_loss: 0.6952 - val_accuracy: 0.5195\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6834 - accuracy: 0.5791 - val_loss: 0.6924 - val_accuracy: 0.5391\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6825 - accuracy: 0.5752 - val_loss: 0.6981 - val_accuracy: 0.5117\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6812 - accuracy: 0.5742 - val_loss: 0.6948 - val_accuracy: 0.5312\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6805 - accuracy: 0.5781 - val_loss: 0.6909 - val_accuracy: 0.5312\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6787 - accuracy: 0.5762 - val_loss: 0.6921 - val_accuracy: 0.5391\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6816 - accuracy: 0.5723 - val_loss: 0.6933 - val_accuracy: 0.5195\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6823 - accuracy: 0.5752 - val_loss: 0.6963 - val_accuracy: 0.5273\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6801 - accuracy: 0.5693 - val_loss: 0.6979 - val_accuracy: 0.5078\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6782 - accuracy: 0.5791 - val_loss: 0.6917 - val_accuracy: 0.5312\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6794 - accuracy: 0.5732 - val_loss: 0.6979 - val_accuracy: 0.5078\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6772 - accuracy: 0.5840 - val_loss: 0.6932 - val_accuracy: 0.5273\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6775 - accuracy: 0.5752 - val_loss: 0.6933 - val_accuracy: 0.5039\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6815 - accuracy: 0.5703 - val_loss: 0.6930 - val_accuracy: 0.5078\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 1s 10ms/step - loss: 0.6947 - accuracy: 0.5312 - val_loss: 0.7082 - val_accuracy: 0.5039\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6854 - accuracy: 0.5449 - val_loss: 0.6964 - val_accuracy: 0.5156\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6831 - accuracy: 0.5547 - val_loss: 0.7025 - val_accuracy: 0.4961\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6842 - accuracy: 0.5547 - val_loss: 0.7004 - val_accuracy: 0.5078\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6822 - accuracy: 0.5469 - val_loss: 0.7013 - val_accuracy: 0.5000\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6843 - accuracy: 0.5527 - val_loss: 0.7009 - val_accuracy: 0.4883\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6822 - accuracy: 0.5605 - val_loss: 0.6992 - val_accuracy: 0.5234\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 0.6816 - accuracy: 0.5664 - val_loss: 0.7016 - val_accuracy: 0.5117\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6803 - accuracy: 0.5596 - val_loss: 0.6982 - val_accuracy: 0.5234\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6799 - accuracy: 0.5635 - val_loss: 0.7000 - val_accuracy: 0.5117\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6792 - accuracy: 0.5605 - val_loss: 0.6966 - val_accuracy: 0.5156\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6778 - accuracy: 0.5723 - val_loss: 0.6964 - val_accuracy: 0.5156\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6788 - accuracy: 0.5723 - val_loss: 0.6970 - val_accuracy: 0.5117\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6777 - accuracy: 0.5674 - val_loss: 0.6950 - val_accuracy: 0.5117\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6760 - accuracy: 0.5654 - val_loss: 0.6967 - val_accuracy: 0.5195\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6755 - accuracy: 0.5693 - val_loss: 0.6995 - val_accuracy: 0.5156\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6749 - accuracy: 0.5801 - val_loss: 0.6936 - val_accuracy: 0.5156\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6765 - accuracy: 0.5869 - val_loss: 0.7032 - val_accuracy: 0.5078\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6756 - accuracy: 0.5762 - val_loss: 0.6931 - val_accuracy: 0.5312\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6736 - accuracy: 0.5742 - val_loss: 0.6951 - val_accuracy: 0.5117\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 19ms/step - loss: 0.6934 - accuracy: 0.5361 - val_loss: 0.6951 - val_accuracy: 0.5234\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6874 - accuracy: 0.5527 - val_loss: 0.7007 - val_accuracy: 0.5234\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6871 - accuracy: 0.5576 - val_loss: 0.6993 - val_accuracy: 0.5156\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6852 - accuracy: 0.5586 - val_loss: 0.7019 - val_accuracy: 0.5117\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6857 - accuracy: 0.5596 - val_loss: 0.7008 - val_accuracy: 0.5234\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6852 - accuracy: 0.5557 - val_loss: 0.7029 - val_accuracy: 0.5039\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6862 - accuracy: 0.5605 - val_loss: 0.7024 - val_accuracy: 0.5117\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6839 - accuracy: 0.5635 - val_loss: 0.7013 - val_accuracy: 0.5195\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6849 - accuracy: 0.5586 - val_loss: 0.7004 - val_accuracy: 0.5234\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.6867 - accuracy: 0.5596 - val_loss: 0.7061 - val_accuracy: 0.5195\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6823 - accuracy: 0.5576 - val_loss: 0.7012 - val_accuracy: 0.5117\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.6825 - accuracy: 0.5596 - val_loss: 0.7027 - val_accuracy: 0.5234\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.6842 - accuracy: 0.5664 - val_loss: 0.6973 - val_accuracy: 0.5195\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6836 - accuracy: 0.5654 - val_loss: 0.7131 - val_accuracy: 0.5195\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6844 - accuracy: 0.5586 - val_loss: 0.7000 - val_accuracy: 0.5195\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6828 - accuracy: 0.5645 - val_loss: 0.6956 - val_accuracy: 0.5156\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6798 - accuracy: 0.5684 - val_loss: 0.7063 - val_accuracy: 0.5195\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6835 - accuracy: 0.5596 - val_loss: 0.6986 - val_accuracy: 0.5117\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6812 - accuracy: 0.5635 - val_loss: 0.6969 - val_accuracy: 0.5234\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6803 - accuracy: 0.5693 - val_loss: 0.6977 - val_accuracy: 0.5312\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 23ms/step - loss: 0.6930 - accuracy: 0.5312 - val_loss: 0.6962 - val_accuracy: 0.5430\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6873 - accuracy: 0.5576 - val_loss: 0.6951 - val_accuracy: 0.5430\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.6865 - accuracy: 0.5596 - val_loss: 0.6917 - val_accuracy: 0.5586\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.6856 - accuracy: 0.5703 - val_loss: 0.6963 - val_accuracy: 0.5508\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.6839 - accuracy: 0.5664 - val_loss: 0.6911 - val_accuracy: 0.5547\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6853 - accuracy: 0.5674 - val_loss: 0.6952 - val_accuracy: 0.5547\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 1s 16ms/step - loss: 0.6847 - accuracy: 0.5645 - val_loss: 0.6954 - val_accuracy: 0.5547\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.6826 - accuracy: 0.5674 - val_loss: 0.6914 - val_accuracy: 0.5547\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 1s 21ms/step - loss: 0.6846 - accuracy: 0.5674 - val_loss: 0.6989 - val_accuracy: 0.5547\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 1s 16ms/step - loss: 0.6831 - accuracy: 0.5615 - val_loss: 0.6945 - val_accuracy: 0.5547\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 1s 21ms/step - loss: 0.6819 - accuracy: 0.5557 - val_loss: 0.6959 - val_accuracy: 0.5547\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 1s 25ms/step - loss: 0.6809 - accuracy: 0.5703 - val_loss: 0.6926 - val_accuracy: 0.5547\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 1s 29ms/step - loss: 0.6807 - accuracy: 0.5703 - val_loss: 0.6945 - val_accuracy: 0.5508\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 1s 18ms/step - loss: 0.6817 - accuracy: 0.5645 - val_loss: 0.7003 - val_accuracy: 0.5547\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 1s 19ms/step - loss: 0.6828 - accuracy: 0.5674 - val_loss: 0.6938 - val_accuracy: 0.5508\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 1s 18ms/step - loss: 0.6785 - accuracy: 0.5723 - val_loss: 0.6932 - val_accuracy: 0.5547\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.6806 - accuracy: 0.5625 - val_loss: 0.6898 - val_accuracy: 0.5547\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.6789 - accuracy: 0.5732 - val_loss: 0.6990 - val_accuracy: 0.5547\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 1s 21ms/step - loss: 0.6800 - accuracy: 0.5742 - val_loss: 0.6977 - val_accuracy: 0.5508\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 1s 27ms/step - loss: 0.6782 - accuracy: 0.5732 - val_loss: 0.6918 - val_accuracy: 0.5547\n",
            "8/8 [==============================] - 0s 11ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 2s 20ms/step - loss: 0.6927 - accuracy: 0.5195 - val_loss: 0.6918 - val_accuracy: 0.5312\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6898 - accuracy: 0.5410 - val_loss: 0.6916 - val_accuracy: 0.5273\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6871 - accuracy: 0.5518 - val_loss: 0.6946 - val_accuracy: 0.5273\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.6903 - accuracy: 0.5566 - val_loss: 0.6964 - val_accuracy: 0.5234\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6870 - accuracy: 0.5547 - val_loss: 0.6972 - val_accuracy: 0.5234\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6864 - accuracy: 0.5566 - val_loss: 0.6968 - val_accuracy: 0.5312\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6898 - accuracy: 0.5449 - val_loss: 0.6980 - val_accuracy: 0.5312\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6847 - accuracy: 0.5703 - val_loss: 0.6938 - val_accuracy: 0.5352\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 0.6870 - accuracy: 0.5664 - val_loss: 0.6986 - val_accuracy: 0.5430\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 0.6834 - accuracy: 0.5762 - val_loss: 0.6953 - val_accuracy: 0.5391\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6821 - accuracy: 0.5693 - val_loss: 0.6973 - val_accuracy: 0.5508\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.6814 - accuracy: 0.5801 - val_loss: 0.6943 - val_accuracy: 0.5430\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.6818 - accuracy: 0.5762 - val_loss: 0.7012 - val_accuracy: 0.5547\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6826 - accuracy: 0.5732 - val_loss: 0.6959 - val_accuracy: 0.5391\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.6788 - accuracy: 0.5742 - val_loss: 0.6947 - val_accuracy: 0.5469\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 0.6806 - accuracy: 0.5742 - val_loss: 0.6942 - val_accuracy: 0.5312\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.6822 - accuracy: 0.5615 - val_loss: 0.6944 - val_accuracy: 0.5469\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 0.6808 - accuracy: 0.5781 - val_loss: 0.6974 - val_accuracy: 0.5352\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 1s 28ms/step - loss: 0.6802 - accuracy: 0.5752 - val_loss: 0.6981 - val_accuracy: 0.5312\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 1s 32ms/step - loss: 0.6779 - accuracy: 0.5723 - val_loss: 0.6923 - val_accuracy: 0.5430\n",
            "8/8 [==============================] - 0s 5ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 29ms/step - loss: 0.6927 - accuracy: 0.5225 - val_loss: 0.6959 - val_accuracy: 0.5078\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6888 - accuracy: 0.5498 - val_loss: 0.6963 - val_accuracy: 0.4922\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6862 - accuracy: 0.5566 - val_loss: 0.7000 - val_accuracy: 0.5156\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6864 - accuracy: 0.5557 - val_loss: 0.7025 - val_accuracy: 0.4883\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6845 - accuracy: 0.5635 - val_loss: 0.6969 - val_accuracy: 0.5078\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6847 - accuracy: 0.5566 - val_loss: 0.7024 - val_accuracy: 0.5078\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 0.6826 - accuracy: 0.5635 - val_loss: 0.7013 - val_accuracy: 0.5195\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.6810 - accuracy: 0.5742 - val_loss: 0.6965 - val_accuracy: 0.5273\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.6837 - accuracy: 0.5605 - val_loss: 0.7010 - val_accuracy: 0.5117\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6817 - accuracy: 0.5693 - val_loss: 0.7027 - val_accuracy: 0.5117\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6808 - accuracy: 0.5674 - val_loss: 0.7015 - val_accuracy: 0.5195\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6798 - accuracy: 0.5703 - val_loss: 0.7047 - val_accuracy: 0.5234\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6791 - accuracy: 0.5674 - val_loss: 0.7020 - val_accuracy: 0.5156\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6774 - accuracy: 0.5713 - val_loss: 0.6981 - val_accuracy: 0.5195\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6767 - accuracy: 0.5664 - val_loss: 0.7009 - val_accuracy: 0.5234\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 0.6777 - accuracy: 0.5713 - val_loss: 0.7037 - val_accuracy: 0.5312\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6758 - accuracy: 0.5703 - val_loss: 0.7036 - val_accuracy: 0.5234\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6771 - accuracy: 0.5742 - val_loss: 0.6991 - val_accuracy: 0.5312\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.6755 - accuracy: 0.5742 - val_loss: 0.7045 - val_accuracy: 0.5234\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6752 - accuracy: 0.5762 - val_loss: 0.7015 - val_accuracy: 0.5195\n",
            "8/8 [==============================] - 0s 9ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 4s 59ms/step - loss: 0.7011 - accuracy: 0.5352 - val_loss: 0.6985 - val_accuracy: 0.5195\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.6889 - accuracy: 0.5537 - val_loss: 0.7017 - val_accuracy: 0.5273\n",
            "Epoch 3/20\n",
            "17/32 [==============>...............] - ETA: 0s - loss: 0.6808 - accuracy: 0.5846"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_conf(\"alpha\",\"frontal\",\"arousal\",\"ann\")"
      ],
      "metadata": {
        "id": "_XkXtZ7Yvjk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_conf(\"alpha\",\"frontal\",\"arousal\",\"ann\") # check this again"
      ],
      "metadata": {
        "id": "SoJLV8Dnvka8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy(\"arousal\",\"lstm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0GSOdCXn2h8",
        "outputId": "17db4ac2-00f8-4cbc-df83-c16d223206f2"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 27ms/step - loss: 0.7510 - accuracy: 0.5029 - val_loss: 0.6971 - val_accuracy: 0.4688\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7453 - accuracy: 0.4824 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7174 - accuracy: 0.5059 - val_loss: 0.6921 - val_accuracy: 0.5078\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7224 - accuracy: 0.5020 - val_loss: 0.6911 - val_accuracy: 0.5312\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7157 - accuracy: 0.5107 - val_loss: 0.6908 - val_accuracy: 0.5273\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7168 - accuracy: 0.4844 - val_loss: 0.6905 - val_accuracy: 0.5352\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7148 - accuracy: 0.4824 - val_loss: 0.6912 - val_accuracy: 0.5195\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7015 - accuracy: 0.5127 - val_loss: 0.6906 - val_accuracy: 0.5547\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7015 - accuracy: 0.4932 - val_loss: 0.6910 - val_accuracy: 0.5078\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7090 - accuracy: 0.4971 - val_loss: 0.6928 - val_accuracy: 0.4922\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6952 - accuracy: 0.5273 - val_loss: 0.6921 - val_accuracy: 0.5000\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6990 - accuracy: 0.5049 - val_loss: 0.6911 - val_accuracy: 0.5469\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6951 - accuracy: 0.5215 - val_loss: 0.6902 - val_accuracy: 0.5273\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6963 - accuracy: 0.4893 - val_loss: 0.6901 - val_accuracy: 0.5547\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7015 - accuracy: 0.4824 - val_loss: 0.6903 - val_accuracy: 0.5312\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6983 - accuracy: 0.4961 - val_loss: 0.6919 - val_accuracy: 0.5039\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6980 - accuracy: 0.5156 - val_loss: 0.6902 - val_accuracy: 0.5352\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6907 - accuracy: 0.5059 - val_loss: 0.6895 - val_accuracy: 0.5469\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6910 - accuracy: 0.5625 - val_loss: 0.6902 - val_accuracy: 0.5469\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6972 - accuracy: 0.5098 - val_loss: 0.6899 - val_accuracy: 0.5430\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 5s 88ms/step - loss: 0.7394 - accuracy: 0.4814 - val_loss: 0.6926 - val_accuracy: 0.5508\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7080 - accuracy: 0.5039 - val_loss: 0.6921 - val_accuracy: 0.5273\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7140 - accuracy: 0.5264 - val_loss: 0.6932 - val_accuracy: 0.5156\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7032 - accuracy: 0.5049 - val_loss: 0.6913 - val_accuracy: 0.5234\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7017 - accuracy: 0.4941 - val_loss: 0.6909 - val_accuracy: 0.5312\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6998 - accuracy: 0.5127 - val_loss: 0.6938 - val_accuracy: 0.4883\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6976 - accuracy: 0.4893 - val_loss: 0.6921 - val_accuracy: 0.5312\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7012 - accuracy: 0.4824 - val_loss: 0.6911 - val_accuracy: 0.5234\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6954 - accuracy: 0.5049 - val_loss: 0.6914 - val_accuracy: 0.5234\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6974 - accuracy: 0.5117 - val_loss: 0.6920 - val_accuracy: 0.5078\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6932 - accuracy: 0.5029 - val_loss: 0.6922 - val_accuracy: 0.4961\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6951 - accuracy: 0.5098 - val_loss: 0.6920 - val_accuracy: 0.5312\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6920 - accuracy: 0.5029 - val_loss: 0.6918 - val_accuracy: 0.5234\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6952 - accuracy: 0.4834 - val_loss: 0.6919 - val_accuracy: 0.4883\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6903 - accuracy: 0.5186 - val_loss: 0.6922 - val_accuracy: 0.4922\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6972 - accuracy: 0.4941 - val_loss: 0.6915 - val_accuracy: 0.5156\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6950 - accuracy: 0.5127 - val_loss: 0.6924 - val_accuracy: 0.4883\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6928 - accuracy: 0.4717 - val_loss: 0.6912 - val_accuracy: 0.5312\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6912 - accuracy: 0.5146 - val_loss: 0.6914 - val_accuracy: 0.5273\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6917 - accuracy: 0.4883 - val_loss: 0.6915 - val_accuracy: 0.4883\n",
            "8/8 [==============================] - 1s 4ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 26ms/step - loss: 0.7432 - accuracy: 0.4717 - val_loss: 0.6893 - val_accuracy: 0.5781\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7196 - accuracy: 0.4941 - val_loss: 0.6895 - val_accuracy: 0.5781\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7003 - accuracy: 0.5195 - val_loss: 0.6901 - val_accuracy: 0.5430\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7082 - accuracy: 0.4990 - val_loss: 0.6921 - val_accuracy: 0.5156\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6972 - accuracy: 0.5166 - val_loss: 0.6980 - val_accuracy: 0.4922\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7020 - accuracy: 0.4893 - val_loss: 0.6888 - val_accuracy: 0.5547\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6984 - accuracy: 0.5059 - val_loss: 0.6893 - val_accuracy: 0.5430\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7002 - accuracy: 0.5117 - val_loss: 0.6899 - val_accuracy: 0.5469\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6982 - accuracy: 0.5059 - val_loss: 0.6910 - val_accuracy: 0.5391\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6969 - accuracy: 0.5029 - val_loss: 0.6890 - val_accuracy: 0.5508\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6956 - accuracy: 0.5195 - val_loss: 0.6895 - val_accuracy: 0.5547\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6960 - accuracy: 0.5254 - val_loss: 0.6900 - val_accuracy: 0.5508\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6987 - accuracy: 0.5049 - val_loss: 0.6935 - val_accuracy: 0.4922\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7005 - accuracy: 0.4990 - val_loss: 0.6923 - val_accuracy: 0.4922\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6945 - accuracy: 0.5176 - val_loss: 0.6929 - val_accuracy: 0.4883\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7034 - accuracy: 0.4834 - val_loss: 0.6925 - val_accuracy: 0.5000\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6967 - accuracy: 0.5176 - val_loss: 0.6920 - val_accuracy: 0.4883\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6936 - accuracy: 0.5303 - val_loss: 0.6920 - val_accuracy: 0.4922\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6934 - accuracy: 0.5244 - val_loss: 0.6920 - val_accuracy: 0.4922\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6945 - accuracy: 0.5117 - val_loss: 0.6926 - val_accuracy: 0.4922\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 35ms/step - loss: 0.7212 - accuracy: 0.4795 - val_loss: 0.6936 - val_accuracy: 0.5078\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7113 - accuracy: 0.5234 - val_loss: 0.6914 - val_accuracy: 0.5469\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7047 - accuracy: 0.5000 - val_loss: 0.6901 - val_accuracy: 0.5391\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6983 - accuracy: 0.5088 - val_loss: 0.6915 - val_accuracy: 0.5859\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6968 - accuracy: 0.4980 - val_loss: 0.6920 - val_accuracy: 0.5156\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7016 - accuracy: 0.4834 - val_loss: 0.6915 - val_accuracy: 0.5430\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6918 - accuracy: 0.5049 - val_loss: 0.6915 - val_accuracy: 0.5156\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6959 - accuracy: 0.5107 - val_loss: 0.6909 - val_accuracy: 0.5312\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6921 - accuracy: 0.4941 - val_loss: 0.6900 - val_accuracy: 0.5312\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6917 - accuracy: 0.5186 - val_loss: 0.6896 - val_accuracy: 0.5430\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6971 - accuracy: 0.4736 - val_loss: 0.6899 - val_accuracy: 0.5586\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6958 - accuracy: 0.5186 - val_loss: 0.6902 - val_accuracy: 0.5469\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6914 - accuracy: 0.5049 - val_loss: 0.6908 - val_accuracy: 0.5234\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6937 - accuracy: 0.5020 - val_loss: 0.6904 - val_accuracy: 0.5312\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6908 - accuracy: 0.5244 - val_loss: 0.6901 - val_accuracy: 0.5312\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6871 - accuracy: 0.5439 - val_loss: 0.6907 - val_accuracy: 0.4961\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6914 - accuracy: 0.5400 - val_loss: 0.6908 - val_accuracy: 0.4844\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6895 - accuracy: 0.5283 - val_loss: 0.6903 - val_accuracy: 0.4883\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6972 - accuracy: 0.5068 - val_loss: 0.6902 - val_accuracy: 0.5000\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6936 - accuracy: 0.5195 - val_loss: 0.6903 - val_accuracy: 0.5352\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 24ms/step - loss: 0.7361 - accuracy: 0.4727 - val_loss: 0.6948 - val_accuracy: 0.5117\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7181 - accuracy: 0.4971 - val_loss: 0.6940 - val_accuracy: 0.5312\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7136 - accuracy: 0.4922 - val_loss: 0.6931 - val_accuracy: 0.5312\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7180 - accuracy: 0.4697 - val_loss: 0.6920 - val_accuracy: 0.5352\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7079 - accuracy: 0.5068 - val_loss: 0.6911 - val_accuracy: 0.5312\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6980 - accuracy: 0.5195 - val_loss: 0.6924 - val_accuracy: 0.5312\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7011 - accuracy: 0.5332 - val_loss: 0.6931 - val_accuracy: 0.4961\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7117 - accuracy: 0.4824 - val_loss: 0.6921 - val_accuracy: 0.5195\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6959 - accuracy: 0.5117 - val_loss: 0.6928 - val_accuracy: 0.4844\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6984 - accuracy: 0.5068 - val_loss: 0.6925 - val_accuracy: 0.5117\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6954 - accuracy: 0.5176 - val_loss: 0.6915 - val_accuracy: 0.5352\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6917 - accuracy: 0.5156 - val_loss: 0.6934 - val_accuracy: 0.4844\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6915 - accuracy: 0.5186 - val_loss: 0.6916 - val_accuracy: 0.5508\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.6920 - accuracy: 0.5205 - val_loss: 0.6915 - val_accuracy: 0.5273\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6954 - accuracy: 0.5029 - val_loss: 0.6913 - val_accuracy: 0.5156\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6899 - accuracy: 0.5371 - val_loss: 0.6918 - val_accuracy: 0.5312\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 0.6918 - accuracy: 0.5342 - val_loss: 0.6912 - val_accuracy: 0.5391\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.6892 - accuracy: 0.5176 - val_loss: 0.6921 - val_accuracy: 0.5312\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6890 - accuracy: 0.5312 - val_loss: 0.6918 - val_accuracy: 0.5469\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.6893 - accuracy: 0.5352 - val_loss: 0.6911 - val_accuracy: 0.5547\n",
            "8/8 [==============================] - 1s 4ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 4s 28ms/step - loss: 0.7229 - accuracy: 0.5225 - val_loss: 0.6909 - val_accuracy: 0.5508\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7155 - accuracy: 0.4766 - val_loss: 0.6923 - val_accuracy: 0.5078\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7100 - accuracy: 0.4893 - val_loss: 0.6928 - val_accuracy: 0.4805\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7058 - accuracy: 0.4883 - val_loss: 0.6946 - val_accuracy: 0.4883\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6990 - accuracy: 0.5039 - val_loss: 0.6931 - val_accuracy: 0.4844\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7031 - accuracy: 0.4941 - val_loss: 0.6942 - val_accuracy: 0.4531\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6936 - accuracy: 0.5059 - val_loss: 0.6957 - val_accuracy: 0.4766\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6954 - accuracy: 0.5205 - val_loss: 0.6946 - val_accuracy: 0.4375\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6935 - accuracy: 0.5352 - val_loss: 0.6947 - val_accuracy: 0.4375\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7009 - accuracy: 0.4873 - val_loss: 0.6954 - val_accuracy: 0.4492\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6940 - accuracy: 0.5215 - val_loss: 0.6949 - val_accuracy: 0.4297\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6959 - accuracy: 0.4873 - val_loss: 0.6937 - val_accuracy: 0.5039\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6953 - accuracy: 0.5000 - val_loss: 0.6935 - val_accuracy: 0.5352\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6915 - accuracy: 0.5234 - val_loss: 0.6941 - val_accuracy: 0.5352\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 2s 67ms/step - loss: 0.6917 - accuracy: 0.5244 - val_loss: 0.6951 - val_accuracy: 0.4531\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 1s 30ms/step - loss: 0.6983 - accuracy: 0.5000 - val_loss: 0.6937 - val_accuracy: 0.5352\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 2s 51ms/step - loss: 0.6930 - accuracy: 0.5225 - val_loss: 0.6942 - val_accuracy: 0.4414\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 1s 23ms/step - loss: 0.6988 - accuracy: 0.4854 - val_loss: 0.6945 - val_accuracy: 0.4883\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 0.6920 - accuracy: 0.5322 - val_loss: 0.6945 - val_accuracy: 0.4648\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6940 - accuracy: 0.5098 - val_loss: 0.6946 - val_accuracy: 0.4609\n",
            "8/8 [==============================] - 1s 5ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 24ms/step - loss: 0.7421 - accuracy: 0.5029 - val_loss: 0.6902 - val_accuracy: 0.5156\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7263 - accuracy: 0.4932 - val_loss: 0.6886 - val_accuracy: 0.5391\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7112 - accuracy: 0.5059 - val_loss: 0.6901 - val_accuracy: 0.5195\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7085 - accuracy: 0.4922 - val_loss: 0.6900 - val_accuracy: 0.5430\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7078 - accuracy: 0.5078 - val_loss: 0.6899 - val_accuracy: 0.5469\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7033 - accuracy: 0.5098 - val_loss: 0.6892 - val_accuracy: 0.5469\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6961 - accuracy: 0.5225 - val_loss: 0.6907 - val_accuracy: 0.6094\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6969 - accuracy: 0.5215 - val_loss: 0.6907 - val_accuracy: 0.5391\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6961 - accuracy: 0.5146 - val_loss: 0.6913 - val_accuracy: 0.5312\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6983 - accuracy: 0.5137 - val_loss: 0.6906 - val_accuracy: 0.5391\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6987 - accuracy: 0.4961 - val_loss: 0.6910 - val_accuracy: 0.5664\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6929 - accuracy: 0.5176 - val_loss: 0.6912 - val_accuracy: 0.5430\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6939 - accuracy: 0.5234 - val_loss: 0.6921 - val_accuracy: 0.5469\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6951 - accuracy: 0.5117 - val_loss: 0.6936 - val_accuracy: 0.4805\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5234 - val_loss: 0.6917 - val_accuracy: 0.4688\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6970 - accuracy: 0.5010 - val_loss: 0.6912 - val_accuracy: 0.5430\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6925 - accuracy: 0.5107 - val_loss: 0.6911 - val_accuracy: 0.5625\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6878 - accuracy: 0.5527 - val_loss: 0.6911 - val_accuracy: 0.4883\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6970 - accuracy: 0.4912 - val_loss: 0.6905 - val_accuracy: 0.5703\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6912 - accuracy: 0.5410 - val_loss: 0.6906 - val_accuracy: 0.5195\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 36ms/step - loss: 0.7215 - accuracy: 0.5020 - val_loss: 0.6921 - val_accuracy: 0.4961\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7214 - accuracy: 0.4941 - val_loss: 0.6922 - val_accuracy: 0.4922\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7117 - accuracy: 0.5010 - val_loss: 0.6924 - val_accuracy: 0.4844\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7126 - accuracy: 0.4785 - val_loss: 0.6918 - val_accuracy: 0.4961\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7037 - accuracy: 0.5264 - val_loss: 0.6924 - val_accuracy: 0.5000\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7026 - accuracy: 0.4902 - val_loss: 0.6912 - val_accuracy: 0.5430\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6942 - accuracy: 0.5273 - val_loss: 0.6915 - val_accuracy: 0.5273\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7040 - accuracy: 0.4795 - val_loss: 0.6906 - val_accuracy: 0.5664\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6933 - accuracy: 0.5156 - val_loss: 0.6902 - val_accuracy: 0.5469\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.7009 - accuracy: 0.4863 - val_loss: 0.6902 - val_accuracy: 0.5586\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6952 - accuracy: 0.4941 - val_loss: 0.6896 - val_accuracy: 0.5430\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6967 - accuracy: 0.4912 - val_loss: 0.6900 - val_accuracy: 0.5234\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6944 - accuracy: 0.5137 - val_loss: 0.6902 - val_accuracy: 0.5273\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6942 - accuracy: 0.4922 - val_loss: 0.6898 - val_accuracy: 0.5273\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6935 - accuracy: 0.5039 - val_loss: 0.6897 - val_accuracy: 0.5234\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6895 - accuracy: 0.5352 - val_loss: 0.6892 - val_accuracy: 0.5508\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6920 - accuracy: 0.5107 - val_loss: 0.6894 - val_accuracy: 0.5117\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6915 - accuracy: 0.5225 - val_loss: 0.6888 - val_accuracy: 0.5352\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6969 - accuracy: 0.4912 - val_loss: 0.6887 - val_accuracy: 0.5430\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6917 - accuracy: 0.5303 - val_loss: 0.6888 - val_accuracy: 0.5234\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 25ms/step - loss: 0.7357 - accuracy: 0.5078 - val_loss: 0.6931 - val_accuracy: 0.4883\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7106 - accuracy: 0.4990 - val_loss: 0.6924 - val_accuracy: 0.5391\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7049 - accuracy: 0.4971 - val_loss: 0.6906 - val_accuracy: 0.5352\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6975 - accuracy: 0.5195 - val_loss: 0.6903 - val_accuracy: 0.5586\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7013 - accuracy: 0.5156 - val_loss: 0.6898 - val_accuracy: 0.5703\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6997 - accuracy: 0.4814 - val_loss: 0.6900 - val_accuracy: 0.5508\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6968 - accuracy: 0.5127 - val_loss: 0.6885 - val_accuracy: 0.5273\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6965 - accuracy: 0.5254 - val_loss: 0.6896 - val_accuracy: 0.5625\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6924 - accuracy: 0.5264 - val_loss: 0.6895 - val_accuracy: 0.5625\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6964 - accuracy: 0.5117 - val_loss: 0.6894 - val_accuracy: 0.5625\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6941 - accuracy: 0.5381 - val_loss: 0.6880 - val_accuracy: 0.5625\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6946 - accuracy: 0.5234 - val_loss: 0.6884 - val_accuracy: 0.5586\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6950 - accuracy: 0.5117 - val_loss: 0.6880 - val_accuracy: 0.5664\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6952 - accuracy: 0.5215 - val_loss: 0.6898 - val_accuracy: 0.5469\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6918 - accuracy: 0.5449 - val_loss: 0.6895 - val_accuracy: 0.5430\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6871 - accuracy: 0.5400 - val_loss: 0.6887 - val_accuracy: 0.5586\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6933 - accuracy: 0.5049 - val_loss: 0.6888 - val_accuracy: 0.5508\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.6930 - accuracy: 0.5312 - val_loss: 0.6878 - val_accuracy: 0.5586\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6935 - accuracy: 0.5449 - val_loss: 0.6890 - val_accuracy: 0.5586\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6941 - accuracy: 0.5234 - val_loss: 0.6894 - val_accuracy: 0.5547\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 25ms/step - loss: 0.7863 - accuracy: 0.4863 - val_loss: 0.6904 - val_accuracy: 0.5312\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7247 - accuracy: 0.5117 - val_loss: 0.6902 - val_accuracy: 0.5469\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7420 - accuracy: 0.4775 - val_loss: 0.6936 - val_accuracy: 0.4922\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7103 - accuracy: 0.4961 - val_loss: 0.6927 - val_accuracy: 0.5547\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7043 - accuracy: 0.4980 - val_loss: 0.6896 - val_accuracy: 0.5664\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7105 - accuracy: 0.4805 - val_loss: 0.6907 - val_accuracy: 0.5391\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7052 - accuracy: 0.5068 - val_loss: 0.6918 - val_accuracy: 0.5312\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7005 - accuracy: 0.4932 - val_loss: 0.6933 - val_accuracy: 0.5352\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6967 - accuracy: 0.5020 - val_loss: 0.6937 - val_accuracy: 0.4727\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7027 - accuracy: 0.5098 - val_loss: 0.6933 - val_accuracy: 0.5352\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7053 - accuracy: 0.4941 - val_loss: 0.6935 - val_accuracy: 0.4883\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6974 - accuracy: 0.5088 - val_loss: 0.6935 - val_accuracy: 0.5078\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7002 - accuracy: 0.4805 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6931 - accuracy: 0.5293 - val_loss: 0.6940 - val_accuracy: 0.4805\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6965 - accuracy: 0.5029 - val_loss: 0.6930 - val_accuracy: 0.5234\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6966 - accuracy: 0.5205 - val_loss: 0.6935 - val_accuracy: 0.5195\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6961 - accuracy: 0.5146 - val_loss: 0.6937 - val_accuracy: 0.4805\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7011 - accuracy: 0.4805 - val_loss: 0.6927 - val_accuracy: 0.5312\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6907 - accuracy: 0.5342 - val_loss: 0.6932 - val_accuracy: 0.5312\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6956 - accuracy: 0.4893 - val_loss: 0.6928 - val_accuracy: 0.5469\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 4s 33ms/step - loss: 0.7463 - accuracy: 0.5029 - val_loss: 0.6895 - val_accuracy: 0.5430\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7353 - accuracy: 0.5049 - val_loss: 0.6892 - val_accuracy: 0.5469\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7227 - accuracy: 0.5146 - val_loss: 0.6928 - val_accuracy: 0.4961\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7154 - accuracy: 0.4932 - val_loss: 0.6925 - val_accuracy: 0.4766\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7081 - accuracy: 0.5010 - val_loss: 0.6902 - val_accuracy: 0.5664\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7043 - accuracy: 0.4990 - val_loss: 0.6906 - val_accuracy: 0.5430\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6974 - accuracy: 0.5215 - val_loss: 0.6908 - val_accuracy: 0.5586\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6931 - accuracy: 0.5303 - val_loss: 0.6915 - val_accuracy: 0.5352\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6947 - accuracy: 0.5264 - val_loss: 0.6911 - val_accuracy: 0.5273\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6999 - accuracy: 0.4990 - val_loss: 0.6909 - val_accuracy: 0.5234\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6905 - accuracy: 0.5469 - val_loss: 0.6897 - val_accuracy: 0.5352\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6972 - accuracy: 0.5078 - val_loss: 0.6897 - val_accuracy: 0.5352\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6963 - accuracy: 0.4883 - val_loss: 0.6907 - val_accuracy: 0.5312\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6907 - accuracy: 0.5342 - val_loss: 0.6900 - val_accuracy: 0.5312\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6914 - accuracy: 0.5391 - val_loss: 0.6901 - val_accuracy: 0.5312\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6900 - accuracy: 0.5605 - val_loss: 0.6892 - val_accuracy: 0.5391\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6931 - accuracy: 0.5068 - val_loss: 0.6902 - val_accuracy: 0.5273\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6911 - accuracy: 0.5293 - val_loss: 0.6895 - val_accuracy: 0.5391\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6910 - accuracy: 0.5332 - val_loss: 0.6899 - val_accuracy: 0.5352\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6937 - accuracy: 0.5322 - val_loss: 0.6892 - val_accuracy: 0.5352\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 4s 33ms/step - loss: 0.7260 - accuracy: 0.4863 - val_loss: 0.6920 - val_accuracy: 0.5195\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7100 - accuracy: 0.4951 - val_loss: 0.6918 - val_accuracy: 0.5195\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7098 - accuracy: 0.4756 - val_loss: 0.6917 - val_accuracy: 0.5195\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7020 - accuracy: 0.5068 - val_loss: 0.6916 - val_accuracy: 0.5273\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7012 - accuracy: 0.5059 - val_loss: 0.6918 - val_accuracy: 0.5195\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6995 - accuracy: 0.4990 - val_loss: 0.6919 - val_accuracy: 0.5156\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6977 - accuracy: 0.5020 - val_loss: 0.6920 - val_accuracy: 0.5156\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6931 - accuracy: 0.5127 - val_loss: 0.6919 - val_accuracy: 0.5195\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6973 - accuracy: 0.5000 - val_loss: 0.6920 - val_accuracy: 0.5078\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7011 - accuracy: 0.4990 - val_loss: 0.6921 - val_accuracy: 0.5156\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6965 - accuracy: 0.5098 - val_loss: 0.6918 - val_accuracy: 0.5117\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6944 - accuracy: 0.5146 - val_loss: 0.6917 - val_accuracy: 0.5273\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6945 - accuracy: 0.5225 - val_loss: 0.6911 - val_accuracy: 0.5156\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6938 - accuracy: 0.5039 - val_loss: 0.6917 - val_accuracy: 0.5156\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6932 - accuracy: 0.5078 - val_loss: 0.6917 - val_accuracy: 0.5195\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6944 - accuracy: 0.4951 - val_loss: 0.6917 - val_accuracy: 0.5156\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6956 - accuracy: 0.5088 - val_loss: 0.6916 - val_accuracy: 0.5117\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6907 - accuracy: 0.5029 - val_loss: 0.6917 - val_accuracy: 0.5156\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6928 - accuracy: 0.5059 - val_loss: 0.6914 - val_accuracy: 0.5234\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6961 - accuracy: 0.4980 - val_loss: 0.6909 - val_accuracy: 0.5234\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 5s 100ms/step - loss: 0.7420 - accuracy: 0.4990 - val_loss: 0.6960 - val_accuracy: 0.4336\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7059 - accuracy: 0.5137 - val_loss: 0.6963 - val_accuracy: 0.4766\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6911 - accuracy: 0.5576 - val_loss: 0.6926 - val_accuracy: 0.5391\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6935 - accuracy: 0.5283 - val_loss: 0.6928 - val_accuracy: 0.5273\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.6967 - accuracy: 0.5244 - val_loss: 0.6935 - val_accuracy: 0.5234\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6868 - accuracy: 0.5479 - val_loss: 0.6921 - val_accuracy: 0.5430\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6928 - accuracy: 0.5234 - val_loss: 0.6916 - val_accuracy: 0.5469\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6935 - accuracy: 0.5303 - val_loss: 0.6916 - val_accuracy: 0.5508\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6941 - accuracy: 0.5449 - val_loss: 0.6919 - val_accuracy: 0.5508\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.6889 - accuracy: 0.5459 - val_loss: 0.6901 - val_accuracy: 0.5508\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.6901 - accuracy: 0.5322 - val_loss: 0.6914 - val_accuracy: 0.5469\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6891 - accuracy: 0.5430 - val_loss: 0.6906 - val_accuracy: 0.5391\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6915 - accuracy: 0.5410 - val_loss: 0.6915 - val_accuracy: 0.5508\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6922 - accuracy: 0.5361 - val_loss: 0.6907 - val_accuracy: 0.5508\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6839 - accuracy: 0.5547 - val_loss: 0.6907 - val_accuracy: 0.5547\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6903 - accuracy: 0.5439 - val_loss: 0.6895 - val_accuracy: 0.5547\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6884 - accuracy: 0.5254 - val_loss: 0.6881 - val_accuracy: 0.5508\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6851 - accuracy: 0.5586 - val_loss: 0.6887 - val_accuracy: 0.5430\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6890 - accuracy: 0.5439 - val_loss: 0.6891 - val_accuracy: 0.5469\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6892 - accuracy: 0.5469 - val_loss: 0.6900 - val_accuracy: 0.5430\n",
            "8/8 [==============================] - 1s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 27ms/step - loss: 0.7486 - accuracy: 0.4648 - val_loss: 0.6921 - val_accuracy: 0.5117\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7116 - accuracy: 0.4902 - val_loss: 0.6935 - val_accuracy: 0.5117\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7041 - accuracy: 0.4941 - val_loss: 0.6940 - val_accuracy: 0.5195\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6972 - accuracy: 0.5049 - val_loss: 0.6938 - val_accuracy: 0.5195\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7010 - accuracy: 0.5000 - val_loss: 0.6941 - val_accuracy: 0.5195\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6979 - accuracy: 0.5098 - val_loss: 0.6935 - val_accuracy: 0.5117\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6985 - accuracy: 0.5068 - val_loss: 0.6938 - val_accuracy: 0.5078\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6943 - accuracy: 0.5215 - val_loss: 0.6931 - val_accuracy: 0.5312\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7007 - accuracy: 0.4922 - val_loss: 0.6928 - val_accuracy: 0.5273\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6942 - accuracy: 0.5293 - val_loss: 0.6921 - val_accuracy: 0.5508\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.6966 - accuracy: 0.4932 - val_loss: 0.6919 - val_accuracy: 0.5430\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6963 - accuracy: 0.5059 - val_loss: 0.6922 - val_accuracy: 0.5312\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.6933 - accuracy: 0.5029 - val_loss: 0.6920 - val_accuracy: 0.5352\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6941 - accuracy: 0.5117 - val_loss: 0.6920 - val_accuracy: 0.5312\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.6928 - accuracy: 0.5088 - val_loss: 0.6913 - val_accuracy: 0.5430\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6942 - accuracy: 0.4883 - val_loss: 0.6908 - val_accuracy: 0.5273\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6966 - accuracy: 0.4893 - val_loss: 0.6918 - val_accuracy: 0.5508\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6938 - accuracy: 0.4971 - val_loss: 0.6916 - val_accuracy: 0.5312\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.6960 - accuracy: 0.4717 - val_loss: 0.6923 - val_accuracy: 0.5312\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.6938 - accuracy: 0.5068 - val_loss: 0.6924 - val_accuracy: 0.5469\n",
            "8/8 [==============================] - 1s 7ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 27ms/step - loss: 0.7372 - accuracy: 0.4863 - val_loss: 0.6913 - val_accuracy: 0.5234\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7067 - accuracy: 0.5127 - val_loss: 0.6907 - val_accuracy: 0.5195\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7001 - accuracy: 0.5107 - val_loss: 0.6922 - val_accuracy: 0.5508\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6931 - accuracy: 0.5303 - val_loss: 0.6933 - val_accuracy: 0.5312\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6991 - accuracy: 0.5273 - val_loss: 0.6926 - val_accuracy: 0.5273\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7078 - accuracy: 0.4824 - val_loss: 0.6947 - val_accuracy: 0.5312\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7025 - accuracy: 0.5146 - val_loss: 0.6940 - val_accuracy: 0.5234\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6973 - accuracy: 0.5098 - val_loss: 0.6956 - val_accuracy: 0.5273\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6971 - accuracy: 0.5068 - val_loss: 0.6943 - val_accuracy: 0.5234\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6867 - accuracy: 0.5400 - val_loss: 0.6944 - val_accuracy: 0.5117\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6934 - accuracy: 0.5391 - val_loss: 0.6932 - val_accuracy: 0.5273\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6972 - accuracy: 0.5254 - val_loss: 0.6950 - val_accuracy: 0.5195\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6950 - accuracy: 0.5195 - val_loss: 0.6934 - val_accuracy: 0.5312\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6909 - accuracy: 0.5322 - val_loss: 0.6925 - val_accuracy: 0.5391\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6844 - accuracy: 0.5498 - val_loss: 0.6931 - val_accuracy: 0.5312\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6906 - accuracy: 0.5371 - val_loss: 0.6936 - val_accuracy: 0.5312\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6922 - accuracy: 0.5293 - val_loss: 0.6936 - val_accuracy: 0.5312\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6947 - accuracy: 0.5234 - val_loss: 0.6934 - val_accuracy: 0.5391\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6856 - accuracy: 0.5693 - val_loss: 0.6937 - val_accuracy: 0.5391\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6909 - accuracy: 0.5400 - val_loss: 0.6922 - val_accuracy: 0.5430\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 25ms/step - loss: 0.7228 - accuracy: 0.4971 - val_loss: 0.6917 - val_accuracy: 0.5391\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7088 - accuracy: 0.4717 - val_loss: 0.6918 - val_accuracy: 0.5156\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6974 - accuracy: 0.5283 - val_loss: 0.6928 - val_accuracy: 0.4805\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6987 - accuracy: 0.4912 - val_loss: 0.6921 - val_accuracy: 0.4961\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7007 - accuracy: 0.5029 - val_loss: 0.6932 - val_accuracy: 0.4805\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6876 - accuracy: 0.5303 - val_loss: 0.6922 - val_accuracy: 0.4961\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6959 - accuracy: 0.5205 - val_loss: 0.6930 - val_accuracy: 0.5078\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6938 - accuracy: 0.5303 - val_loss: 0.6930 - val_accuracy: 0.5039\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6974 - accuracy: 0.5068 - val_loss: 0.6933 - val_accuracy: 0.4883\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6949 - accuracy: 0.5049 - val_loss: 0.6925 - val_accuracy: 0.5078\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6908 - accuracy: 0.5293 - val_loss: 0.6922 - val_accuracy: 0.5078\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6915 - accuracy: 0.5186 - val_loss: 0.6922 - val_accuracy: 0.4922\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6893 - accuracy: 0.5234 - val_loss: 0.6918 - val_accuracy: 0.5078\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6890 - accuracy: 0.5391 - val_loss: 0.6928 - val_accuracy: 0.4922\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6877 - accuracy: 0.5430 - val_loss: 0.6919 - val_accuracy: 0.5117\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6928 - accuracy: 0.5156 - val_loss: 0.6931 - val_accuracy: 0.5195\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6899 - accuracy: 0.5420 - val_loss: 0.6903 - val_accuracy: 0.5195\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6837 - accuracy: 0.5713 - val_loss: 0.6902 - val_accuracy: 0.5352\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6840 - accuracy: 0.5547 - val_loss: 0.6914 - val_accuracy: 0.5039\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6854 - accuracy: 0.5430 - val_loss: 0.6897 - val_accuracy: 0.5352\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "       frontal  central  parietal  occipital\n",
            "theta    54.30    48.83     49.22      53.52\n",
            "alpha    55.47    46.09     51.95      52.34\n",
            "beta     55.47    54.69     53.52      52.34\n",
            "gamma    54.30    54.69     54.30      53.52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_accuracy(\"valence\",\"lstm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGJqkAz0n4VX",
        "outputId": "e7544a79-8d74-488e-f4aa-61b741df07f5"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "32/32 [==============================] - 4s 38ms/step - loss: 0.7548 - accuracy: 0.5098 - val_loss: 0.6938 - val_accuracy: 0.4883\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7201 - accuracy: 0.4980 - val_loss: 0.6951 - val_accuracy: 0.4570\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7191 - accuracy: 0.5098 - val_loss: 0.6937 - val_accuracy: 0.4688\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7019 - accuracy: 0.5225 - val_loss: 0.6932 - val_accuracy: 0.4961\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6928 - accuracy: 0.5361 - val_loss: 0.6945 - val_accuracy: 0.4453\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7011 - accuracy: 0.5195 - val_loss: 0.6927 - val_accuracy: 0.5312\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6958 - accuracy: 0.5283 - val_loss: 0.6932 - val_accuracy: 0.4922\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7112 - accuracy: 0.4814 - val_loss: 0.6960 - val_accuracy: 0.4688\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7064 - accuracy: 0.4707 - val_loss: 0.6915 - val_accuracy: 0.5156\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7007 - accuracy: 0.5195 - val_loss: 0.6926 - val_accuracy: 0.4883\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7025 - accuracy: 0.5049 - val_loss: 0.6906 - val_accuracy: 0.5469\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6972 - accuracy: 0.5039 - val_loss: 0.6910 - val_accuracy: 0.5430\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7025 - accuracy: 0.4912 - val_loss: 0.6913 - val_accuracy: 0.5508\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6959 - accuracy: 0.5273 - val_loss: 0.6909 - val_accuracy: 0.5547\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6977 - accuracy: 0.5000 - val_loss: 0.6908 - val_accuracy: 0.5469\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6957 - accuracy: 0.5039 - val_loss: 0.6916 - val_accuracy: 0.5508\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6968 - accuracy: 0.5156 - val_loss: 0.6919 - val_accuracy: 0.5586\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6934 - accuracy: 0.5322 - val_loss: 0.6907 - val_accuracy: 0.5469\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6926 - accuracy: 0.5352 - val_loss: 0.6910 - val_accuracy: 0.5469\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6932 - accuracy: 0.5381 - val_loss: 0.6909 - val_accuracy: 0.5469\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 24ms/step - loss: 0.7457 - accuracy: 0.5029 - val_loss: 0.6899 - val_accuracy: 0.5469\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7200 - accuracy: 0.4941 - val_loss: 0.6904 - val_accuracy: 0.5469\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7017 - accuracy: 0.5107 - val_loss: 0.6905 - val_accuracy: 0.5469\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7089 - accuracy: 0.5166 - val_loss: 0.6905 - val_accuracy: 0.5664\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7096 - accuracy: 0.5010 - val_loss: 0.6912 - val_accuracy: 0.5469\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7046 - accuracy: 0.5234 - val_loss: 0.6919 - val_accuracy: 0.5469\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7056 - accuracy: 0.4844 - val_loss: 0.6919 - val_accuracy: 0.5469\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6964 - accuracy: 0.5166 - val_loss: 0.6924 - val_accuracy: 0.5469\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7036 - accuracy: 0.4971 - val_loss: 0.6926 - val_accuracy: 0.5469\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.6992 - accuracy: 0.5020 - val_loss: 0.6917 - val_accuracy: 0.5469\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6940 - accuracy: 0.5186 - val_loss: 0.6925 - val_accuracy: 0.5469\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6982 - accuracy: 0.5107 - val_loss: 0.6911 - val_accuracy: 0.5469\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6906 - accuracy: 0.5420 - val_loss: 0.6915 - val_accuracy: 0.5469\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6949 - accuracy: 0.5107 - val_loss: 0.6917 - val_accuracy: 0.5469\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.6947 - accuracy: 0.5176 - val_loss: 0.6928 - val_accuracy: 0.5469\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6978 - accuracy: 0.5303 - val_loss: 0.6922 - val_accuracy: 0.5469\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6889 - accuracy: 0.5410 - val_loss: 0.6926 - val_accuracy: 0.5469\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.6951 - accuracy: 0.5283 - val_loss: 0.6927 - val_accuracy: 0.5469\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7006 - accuracy: 0.4961 - val_loss: 0.6921 - val_accuracy: 0.5469\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6900 - accuracy: 0.5332 - val_loss: 0.6912 - val_accuracy: 0.5469\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 24ms/step - loss: 0.7339 - accuracy: 0.4990 - val_loss: 0.6881 - val_accuracy: 0.5547\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7172 - accuracy: 0.5117 - val_loss: 0.6897 - val_accuracy: 0.5273\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7166 - accuracy: 0.5010 - val_loss: 0.6900 - val_accuracy: 0.5312\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6991 - accuracy: 0.5088 - val_loss: 0.6895 - val_accuracy: 0.5469\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7015 - accuracy: 0.5146 - val_loss: 0.6906 - val_accuracy: 0.5469\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6983 - accuracy: 0.5068 - val_loss: 0.6895 - val_accuracy: 0.5469\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6987 - accuracy: 0.5127 - val_loss: 0.6889 - val_accuracy: 0.5469\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6926 - accuracy: 0.5430 - val_loss: 0.6892 - val_accuracy: 0.5469\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6959 - accuracy: 0.5146 - val_loss: 0.6891 - val_accuracy: 0.5469\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6993 - accuracy: 0.4805 - val_loss: 0.6892 - val_accuracy: 0.5469\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6941 - accuracy: 0.5029 - val_loss: 0.6895 - val_accuracy: 0.5586\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6982 - accuracy: 0.5078 - val_loss: 0.6890 - val_accuracy: 0.5625\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6948 - accuracy: 0.5059 - val_loss: 0.6888 - val_accuracy: 0.5469\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6928 - accuracy: 0.5244 - val_loss: 0.6873 - val_accuracy: 0.5469\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6917 - accuracy: 0.5410 - val_loss: 0.6885 - val_accuracy: 0.5195\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6890 - accuracy: 0.5264 - val_loss: 0.6876 - val_accuracy: 0.5469\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6913 - accuracy: 0.5186 - val_loss: 0.6886 - val_accuracy: 0.5469\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6938 - accuracy: 0.5195 - val_loss: 0.6881 - val_accuracy: 0.5469\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6923 - accuracy: 0.5215 - val_loss: 0.6879 - val_accuracy: 0.5469\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6942 - accuracy: 0.5264 - val_loss: 0.6888 - val_accuracy: 0.5469\n",
            "8/8 [==============================] - 1s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 24ms/step - loss: 0.7487 - accuracy: 0.5059 - val_loss: 0.6885 - val_accuracy: 0.5625\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7284 - accuracy: 0.4854 - val_loss: 0.6896 - val_accuracy: 0.5469\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7023 - accuracy: 0.5322 - val_loss: 0.6921 - val_accuracy: 0.5430\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7180 - accuracy: 0.4814 - val_loss: 0.6927 - val_accuracy: 0.5469\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6965 - accuracy: 0.5186 - val_loss: 0.6926 - val_accuracy: 0.5469\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6991 - accuracy: 0.5156 - val_loss: 0.6915 - val_accuracy: 0.5469\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6962 - accuracy: 0.5244 - val_loss: 0.6911 - val_accuracy: 0.5469\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7030 - accuracy: 0.5039 - val_loss: 0.6909 - val_accuracy: 0.5469\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7011 - accuracy: 0.4893 - val_loss: 0.6908 - val_accuracy: 0.5469\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6982 - accuracy: 0.4961 - val_loss: 0.6905 - val_accuracy: 0.5469\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6889 - accuracy: 0.5459 - val_loss: 0.6912 - val_accuracy: 0.5469\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6965 - accuracy: 0.5166 - val_loss: 0.6911 - val_accuracy: 0.5469\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6933 - accuracy: 0.5137 - val_loss: 0.6915 - val_accuracy: 0.5469\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6923 - accuracy: 0.5195 - val_loss: 0.6917 - val_accuracy: 0.5469\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6903 - accuracy: 0.5469 - val_loss: 0.6916 - val_accuracy: 0.5469\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6978 - accuracy: 0.4912 - val_loss: 0.6917 - val_accuracy: 0.5469\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6899 - accuracy: 0.5391 - val_loss: 0.6915 - val_accuracy: 0.5469\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6903 - accuracy: 0.5234 - val_loss: 0.6918 - val_accuracy: 0.5469\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6919 - accuracy: 0.5234 - val_loss: 0.6914 - val_accuracy: 0.5469\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.6945 - accuracy: 0.5107 - val_loss: 0.6911 - val_accuracy: 0.5469\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 24ms/step - loss: 0.8234 - accuracy: 0.4961 - val_loss: 0.6915 - val_accuracy: 0.5469\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7479 - accuracy: 0.5166 - val_loss: 0.6920 - val_accuracy: 0.5508\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7369 - accuracy: 0.4971 - val_loss: 0.6911 - val_accuracy: 0.5391\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7045 - accuracy: 0.5293 - val_loss: 0.6905 - val_accuracy: 0.5547\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7109 - accuracy: 0.5322 - val_loss: 0.6915 - val_accuracy: 0.5469\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7055 - accuracy: 0.5254 - val_loss: 0.6928 - val_accuracy: 0.5469\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7010 - accuracy: 0.5312 - val_loss: 0.6927 - val_accuracy: 0.5352\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7013 - accuracy: 0.5098 - val_loss: 0.6910 - val_accuracy: 0.5469\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6922 - accuracy: 0.5225 - val_loss: 0.6909 - val_accuracy: 0.5469\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6967 - accuracy: 0.5137 - val_loss: 0.6910 - val_accuracy: 0.5391\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7001 - accuracy: 0.5078 - val_loss: 0.6903 - val_accuracy: 0.5469\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6940 - accuracy: 0.5234 - val_loss: 0.6905 - val_accuracy: 0.5469\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7016 - accuracy: 0.4980 - val_loss: 0.6908 - val_accuracy: 0.5469\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6936 - accuracy: 0.5312 - val_loss: 0.6903 - val_accuracy: 0.5469\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6925 - accuracy: 0.5225 - val_loss: 0.6903 - val_accuracy: 0.5469\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6935 - accuracy: 0.5117 - val_loss: 0.6907 - val_accuracy: 0.5469\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6919 - accuracy: 0.5127 - val_loss: 0.6904 - val_accuracy: 0.5469\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6952 - accuracy: 0.5117 - val_loss: 0.6900 - val_accuracy: 0.5469\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.6941 - accuracy: 0.5234 - val_loss: 0.6903 - val_accuracy: 0.5469\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7005 - accuracy: 0.5068 - val_loss: 0.6908 - val_accuracy: 0.5469\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 24ms/step - loss: 0.7330 - accuracy: 0.4932 - val_loss: 0.6960 - val_accuracy: 0.4492\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7165 - accuracy: 0.5176 - val_loss: 0.6904 - val_accuracy: 0.5391\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7052 - accuracy: 0.5127 - val_loss: 0.6948 - val_accuracy: 0.4688\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6962 - accuracy: 0.5576 - val_loss: 0.6888 - val_accuracy: 0.5469\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7009 - accuracy: 0.5342 - val_loss: 0.6897 - val_accuracy: 0.5469\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7053 - accuracy: 0.4961 - val_loss: 0.6896 - val_accuracy: 0.5469\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7025 - accuracy: 0.5186 - val_loss: 0.6896 - val_accuracy: 0.5469\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6962 - accuracy: 0.5225 - val_loss: 0.6902 - val_accuracy: 0.5469\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6917 - accuracy: 0.5479 - val_loss: 0.6929 - val_accuracy: 0.5469\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6961 - accuracy: 0.5107 - val_loss: 0.6927 - val_accuracy: 0.5469\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7026 - accuracy: 0.4902 - val_loss: 0.6921 - val_accuracy: 0.5469\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6971 - accuracy: 0.4922 - val_loss: 0.6926 - val_accuracy: 0.5469\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6846 - accuracy: 0.5518 - val_loss: 0.6923 - val_accuracy: 0.5430\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6897 - accuracy: 0.5273 - val_loss: 0.6931 - val_accuracy: 0.5586\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.6987 - accuracy: 0.5098 - val_loss: 0.6928 - val_accuracy: 0.5469\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6976 - accuracy: 0.5098 - val_loss: 0.6930 - val_accuracy: 0.5469\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6884 - accuracy: 0.5586 - val_loss: 0.6923 - val_accuracy: 0.5469\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6942 - accuracy: 0.5273 - val_loss: 0.6929 - val_accuracy: 0.5469\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6920 - accuracy: 0.5488 - val_loss: 0.6941 - val_accuracy: 0.5273\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6882 - accuracy: 0.5430 - val_loss: 0.6937 - val_accuracy: 0.5469\n",
            "8/8 [==============================] - 1s 4ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 24ms/step - loss: 0.7397 - accuracy: 0.4971 - val_loss: 0.6943 - val_accuracy: 0.4648\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7058 - accuracy: 0.5098 - val_loss: 0.6960 - val_accuracy: 0.4570\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7130 - accuracy: 0.5049 - val_loss: 0.6943 - val_accuracy: 0.4883\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7101 - accuracy: 0.5049 - val_loss: 0.6932 - val_accuracy: 0.4922\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7074 - accuracy: 0.5010 - val_loss: 0.6913 - val_accuracy: 0.5195\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7048 - accuracy: 0.4971 - val_loss: 0.6902 - val_accuracy: 0.5625\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6973 - accuracy: 0.5156 - val_loss: 0.6892 - val_accuracy: 0.5234\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7006 - accuracy: 0.5039 - val_loss: 0.6906 - val_accuracy: 0.5234\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7008 - accuracy: 0.4961 - val_loss: 0.6893 - val_accuracy: 0.5469\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6963 - accuracy: 0.5146 - val_loss: 0.6898 - val_accuracy: 0.5469\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6980 - accuracy: 0.5137 - val_loss: 0.6891 - val_accuracy: 0.5469\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6952 - accuracy: 0.5010 - val_loss: 0.6899 - val_accuracy: 0.5664\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6956 - accuracy: 0.5049 - val_loss: 0.6889 - val_accuracy: 0.5469\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6930 - accuracy: 0.5312 - val_loss: 0.6882 - val_accuracy: 0.5469\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6912 - accuracy: 0.5264 - val_loss: 0.6894 - val_accuracy: 0.5469\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6906 - accuracy: 0.5303 - val_loss: 0.6897 - val_accuracy: 0.5547\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6961 - accuracy: 0.5059 - val_loss: 0.6893 - val_accuracy: 0.5508\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6940 - accuracy: 0.5010 - val_loss: 0.6900 - val_accuracy: 0.5469\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6884 - accuracy: 0.5410 - val_loss: 0.6899 - val_accuracy: 0.5391\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6944 - accuracy: 0.5264 - val_loss: 0.6891 - val_accuracy: 0.5391\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 27ms/step - loss: 0.7592 - accuracy: 0.4873 - val_loss: 0.6901 - val_accuracy: 0.5469\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7204 - accuracy: 0.5156 - val_loss: 0.6899 - val_accuracy: 0.5469\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7288 - accuracy: 0.5039 - val_loss: 0.6910 - val_accuracy: 0.5469\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7094 - accuracy: 0.5059 - val_loss: 0.6923 - val_accuracy: 0.5469\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.7058 - accuracy: 0.4805 - val_loss: 0.6923 - val_accuracy: 0.5469\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7028 - accuracy: 0.5137 - val_loss: 0.6918 - val_accuracy: 0.5469\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7079 - accuracy: 0.5137 - val_loss: 0.6918 - val_accuracy: 0.5469\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7011 - accuracy: 0.4932 - val_loss: 0.6918 - val_accuracy: 0.5547\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6962 - accuracy: 0.5088 - val_loss: 0.6919 - val_accuracy: 0.5430\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6958 - accuracy: 0.5215 - val_loss: 0.6912 - val_accuracy: 0.5469\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.6928 - accuracy: 0.5205 - val_loss: 0.6912 - val_accuracy: 0.5352\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6943 - accuracy: 0.5273 - val_loss: 0.6899 - val_accuracy: 0.5469\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6902 - accuracy: 0.5410 - val_loss: 0.6901 - val_accuracy: 0.5430\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6919 - accuracy: 0.5273 - val_loss: 0.6894 - val_accuracy: 0.5469\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6948 - accuracy: 0.5195 - val_loss: 0.6893 - val_accuracy: 0.5469\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6886 - accuracy: 0.5400 - val_loss: 0.6918 - val_accuracy: 0.5469\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6911 - accuracy: 0.5361 - val_loss: 0.6924 - val_accuracy: 0.5430\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6945 - accuracy: 0.5186 - val_loss: 0.6922 - val_accuracy: 0.5469\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6935 - accuracy: 0.5205 - val_loss: 0.6912 - val_accuracy: 0.5469\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6928 - accuracy: 0.5156 - val_loss: 0.6912 - val_accuracy: 0.5469\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 6s 109ms/step - loss: 0.7444 - accuracy: 0.4785 - val_loss: 0.6899 - val_accuracy: 0.5469\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7142 - accuracy: 0.5059 - val_loss: 0.6899 - val_accuracy: 0.5469\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7023 - accuracy: 0.5225 - val_loss: 0.6893 - val_accuracy: 0.5469\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7042 - accuracy: 0.5010 - val_loss: 0.6895 - val_accuracy: 0.5469\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7174 - accuracy: 0.4805 - val_loss: 0.6898 - val_accuracy: 0.5469\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.7057 - accuracy: 0.4893 - val_loss: 0.6900 - val_accuracy: 0.5469\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7048 - accuracy: 0.5010 - val_loss: 0.6893 - val_accuracy: 0.5469\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7009 - accuracy: 0.5107 - val_loss: 0.6899 - val_accuracy: 0.5469\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.7008 - accuracy: 0.4932 - val_loss: 0.6900 - val_accuracy: 0.5469\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7065 - accuracy: 0.4824 - val_loss: 0.6898 - val_accuracy: 0.5469\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6974 - accuracy: 0.5166 - val_loss: 0.6900 - val_accuracy: 0.5469\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6962 - accuracy: 0.5234 - val_loss: 0.6900 - val_accuracy: 0.5469\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6932 - accuracy: 0.5195 - val_loss: 0.6895 - val_accuracy: 0.5469\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6945 - accuracy: 0.5273 - val_loss: 0.6895 - val_accuracy: 0.5469\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6981 - accuracy: 0.5010 - val_loss: 0.6898 - val_accuracy: 0.5469\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7006 - accuracy: 0.4873 - val_loss: 0.6891 - val_accuracy: 0.5469\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6948 - accuracy: 0.5098 - val_loss: 0.6896 - val_accuracy: 0.5469\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6884 - accuracy: 0.5391 - val_loss: 0.6892 - val_accuracy: 0.5469\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6959 - accuracy: 0.5059 - val_loss: 0.6891 - val_accuracy: 0.5469\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6950 - accuracy: 0.5176 - val_loss: 0.6892 - val_accuracy: 0.5469\n",
            "8/8 [==============================] - 1s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 26ms/step - loss: 0.7613 - accuracy: 0.5078 - val_loss: 0.6884 - val_accuracy: 0.5469\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7331 - accuracy: 0.5117 - val_loss: 0.6909 - val_accuracy: 0.5508\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7271 - accuracy: 0.4805 - val_loss: 0.6938 - val_accuracy: 0.4531\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6974 - accuracy: 0.5400 - val_loss: 0.6914 - val_accuracy: 0.5469\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7048 - accuracy: 0.5195 - val_loss: 0.6901 - val_accuracy: 0.5469\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7099 - accuracy: 0.4961 - val_loss: 0.6914 - val_accuracy: 0.5469\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7074 - accuracy: 0.5098 - val_loss: 0.6932 - val_accuracy: 0.4961\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7017 - accuracy: 0.5010 - val_loss: 0.6909 - val_accuracy: 0.5469\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7054 - accuracy: 0.4990 - val_loss: 0.6895 - val_accuracy: 0.5469\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6974 - accuracy: 0.5117 - val_loss: 0.6898 - val_accuracy: 0.5469\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.6996 - accuracy: 0.5107 - val_loss: 0.6910 - val_accuracy: 0.5469\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6993 - accuracy: 0.5020 - val_loss: 0.6915 - val_accuracy: 0.5469\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7008 - accuracy: 0.5020 - val_loss: 0.6919 - val_accuracy: 0.5469\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.6932 - accuracy: 0.5088 - val_loss: 0.6925 - val_accuracy: 0.5469\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6920 - accuracy: 0.5312 - val_loss: 0.6914 - val_accuracy: 0.5469\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.6962 - accuracy: 0.4990 - val_loss: 0.6913 - val_accuracy: 0.5469\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.6916 - accuracy: 0.5352 - val_loss: 0.6912 - val_accuracy: 0.5469\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6923 - accuracy: 0.5088 - val_loss: 0.6917 - val_accuracy: 0.5469\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.6903 - accuracy: 0.5469 - val_loss: 0.6919 - val_accuracy: 0.5469\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6963 - accuracy: 0.5254 - val_loss: 0.6918 - val_accuracy: 0.5469\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 25ms/step - loss: 0.7697 - accuracy: 0.4775 - val_loss: 0.6896 - val_accuracy: 0.5469\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7225 - accuracy: 0.4873 - val_loss: 0.6898 - val_accuracy: 0.5469\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7243 - accuracy: 0.5059 - val_loss: 0.6890 - val_accuracy: 0.5469\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7290 - accuracy: 0.4902 - val_loss: 0.6970 - val_accuracy: 0.4922\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7106 - accuracy: 0.4971 - val_loss: 0.6896 - val_accuracy: 0.5469\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7127 - accuracy: 0.4863 - val_loss: 0.6898 - val_accuracy: 0.5469\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7074 - accuracy: 0.5127 - val_loss: 0.6878 - val_accuracy: 0.5469\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6977 - accuracy: 0.5195 - val_loss: 0.6896 - val_accuracy: 0.5469\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6943 - accuracy: 0.5283 - val_loss: 0.6899 - val_accuracy: 0.5469\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6910 - accuracy: 0.5225 - val_loss: 0.6885 - val_accuracy: 0.5469\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6904 - accuracy: 0.5146 - val_loss: 0.6895 - val_accuracy: 0.5469\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6960 - accuracy: 0.5107 - val_loss: 0.6895 - val_accuracy: 0.5469\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6955 - accuracy: 0.5273 - val_loss: 0.6902 - val_accuracy: 0.5469\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6965 - accuracy: 0.5078 - val_loss: 0.6888 - val_accuracy: 0.5469\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6985 - accuracy: 0.5000 - val_loss: 0.6891 - val_accuracy: 0.5469\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6951 - accuracy: 0.5166 - val_loss: 0.6891 - val_accuracy: 0.5469\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6908 - accuracy: 0.5322 - val_loss: 0.6888 - val_accuracy: 0.5469\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6950 - accuracy: 0.4971 - val_loss: 0.6890 - val_accuracy: 0.5469\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6944 - accuracy: 0.5039 - val_loss: 0.6890 - val_accuracy: 0.5469\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6937 - accuracy: 0.5312 - val_loss: 0.6889 - val_accuracy: 0.5469\n",
            "8/8 [==============================] - 1s 5ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 4s 24ms/step - loss: 0.7462 - accuracy: 0.4971 - val_loss: 0.6904 - val_accuracy: 0.5469\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7108 - accuracy: 0.5049 - val_loss: 0.6917 - val_accuracy: 0.5469\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7118 - accuracy: 0.4912 - val_loss: 0.6932 - val_accuracy: 0.5430\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6958 - accuracy: 0.5293 - val_loss: 0.6944 - val_accuracy: 0.5469\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6995 - accuracy: 0.5186 - val_loss: 0.6957 - val_accuracy: 0.5195\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6974 - accuracy: 0.5254 - val_loss: 0.6942 - val_accuracy: 0.5469\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6989 - accuracy: 0.5264 - val_loss: 0.6939 - val_accuracy: 0.5352\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7017 - accuracy: 0.5254 - val_loss: 0.6919 - val_accuracy: 0.5312\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6982 - accuracy: 0.5166 - val_loss: 0.6912 - val_accuracy: 0.5469\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.7009 - accuracy: 0.5039 - val_loss: 0.6906 - val_accuracy: 0.5469\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6969 - accuracy: 0.5000 - val_loss: 0.6905 - val_accuracy: 0.5469\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6876 - accuracy: 0.5361 - val_loss: 0.6911 - val_accuracy: 0.5508\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6973 - accuracy: 0.5020 - val_loss: 0.6916 - val_accuracy: 0.5469\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6924 - accuracy: 0.5254 - val_loss: 0.6912 - val_accuracy: 0.5469\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6927 - accuracy: 0.5322 - val_loss: 0.6924 - val_accuracy: 0.5469\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6923 - accuracy: 0.5312 - val_loss: 0.6915 - val_accuracy: 0.5625\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6948 - accuracy: 0.5098 - val_loss: 0.6912 - val_accuracy: 0.5430\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6899 - accuracy: 0.5293 - val_loss: 0.6917 - val_accuracy: 0.5508\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6951 - accuracy: 0.5176 - val_loss: 0.6906 - val_accuracy: 0.5508\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6928 - accuracy: 0.5312 - val_loss: 0.6901 - val_accuracy: 0.5469\n",
            "8/8 [==============================] - 0s 3ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 25ms/step - loss: 0.7233 - accuracy: 0.4873 - val_loss: 0.6882 - val_accuracy: 0.5391\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7127 - accuracy: 0.4961 - val_loss: 0.6856 - val_accuracy: 0.5469\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7035 - accuracy: 0.5137 - val_loss: 0.6876 - val_accuracy: 0.5508\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7050 - accuracy: 0.4697 - val_loss: 0.6866 - val_accuracy: 0.5469\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6921 - accuracy: 0.5293 - val_loss: 0.6859 - val_accuracy: 0.5469\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7013 - accuracy: 0.5107 - val_loss: 0.6869 - val_accuracy: 0.5469\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6991 - accuracy: 0.5127 - val_loss: 0.6878 - val_accuracy: 0.5469\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6990 - accuracy: 0.4951 - val_loss: 0.6881 - val_accuracy: 0.5469\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6906 - accuracy: 0.5176 - val_loss: 0.6877 - val_accuracy: 0.5469\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6931 - accuracy: 0.5098 - val_loss: 0.6869 - val_accuracy: 0.5469\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6959 - accuracy: 0.5127 - val_loss: 0.6874 - val_accuracy: 0.5469\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6923 - accuracy: 0.5273 - val_loss: 0.6874 - val_accuracy: 0.5469\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6988 - accuracy: 0.5049 - val_loss: 0.6891 - val_accuracy: 0.5430\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6922 - accuracy: 0.5293 - val_loss: 0.6886 - val_accuracy: 0.5508\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6924 - accuracy: 0.5215 - val_loss: 0.6883 - val_accuracy: 0.5508\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6923 - accuracy: 0.5283 - val_loss: 0.6880 - val_accuracy: 0.5469\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6910 - accuracy: 0.5283 - val_loss: 0.6868 - val_accuracy: 0.5547\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6859 - accuracy: 0.5488 - val_loss: 0.6867 - val_accuracy: 0.5469\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6930 - accuracy: 0.5225 - val_loss: 0.6854 - val_accuracy: 0.5469\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6951 - accuracy: 0.5166 - val_loss: 0.6855 - val_accuracy: 0.5469\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 27ms/step - loss: 0.7153 - accuracy: 0.5068 - val_loss: 0.6902 - val_accuracy: 0.5469\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7041 - accuracy: 0.5078 - val_loss: 0.6918 - val_accuracy: 0.5469\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7037 - accuracy: 0.5068 - val_loss: 0.6935 - val_accuracy: 0.5078\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6976 - accuracy: 0.5186 - val_loss: 0.6925 - val_accuracy: 0.5430\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7013 - accuracy: 0.5078 - val_loss: 0.6925 - val_accuracy: 0.5469\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6888 - accuracy: 0.5303 - val_loss: 0.6920 - val_accuracy: 0.5430\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6945 - accuracy: 0.5244 - val_loss: 0.6916 - val_accuracy: 0.5469\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6999 - accuracy: 0.5068 - val_loss: 0.6918 - val_accuracy: 0.5469\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6944 - accuracy: 0.5254 - val_loss: 0.6910 - val_accuracy: 0.5469\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6925 - accuracy: 0.5137 - val_loss: 0.6909 - val_accuracy: 0.5469\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6891 - accuracy: 0.5449 - val_loss: 0.6904 - val_accuracy: 0.5469\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.6966 - accuracy: 0.5088 - val_loss: 0.6903 - val_accuracy: 0.5469\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6972 - accuracy: 0.4902 - val_loss: 0.6910 - val_accuracy: 0.5469\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6906 - accuracy: 0.5293 - val_loss: 0.6907 - val_accuracy: 0.5469\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.6942 - accuracy: 0.5049 - val_loss: 0.6918 - val_accuracy: 0.5469\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.6907 - accuracy: 0.5166 - val_loss: 0.6925 - val_accuracy: 0.5469\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.6917 - accuracy: 0.5371 - val_loss: 0.6927 - val_accuracy: 0.5469\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6919 - accuracy: 0.5176 - val_loss: 0.6927 - val_accuracy: 0.5469\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6935 - accuracy: 0.5195 - val_loss: 0.6931 - val_accuracy: 0.5430\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.6918 - accuracy: 0.5195 - val_loss: 0.6922 - val_accuracy: 0.5469\n",
            "8/8 [==============================] - 0s 4ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 3s 24ms/step - loss: 0.7219 - accuracy: 0.4883 - val_loss: 0.6886 - val_accuracy: 0.5703\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7036 - accuracy: 0.5420 - val_loss: 0.6875 - val_accuracy: 0.5469\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7202 - accuracy: 0.4844 - val_loss: 0.6897 - val_accuracy: 0.5430\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6994 - accuracy: 0.5273 - val_loss: 0.6891 - val_accuracy: 0.5469\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7047 - accuracy: 0.4854 - val_loss: 0.6899 - val_accuracy: 0.5469\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6974 - accuracy: 0.5137 - val_loss: 0.6893 - val_accuracy: 0.5469\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6926 - accuracy: 0.5156 - val_loss: 0.6902 - val_accuracy: 0.5469\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6936 - accuracy: 0.5137 - val_loss: 0.6901 - val_accuracy: 0.5469\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6949 - accuracy: 0.5039 - val_loss: 0.6905 - val_accuracy: 0.5469\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6960 - accuracy: 0.5137 - val_loss: 0.6897 - val_accuracy: 0.5469\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6951 - accuracy: 0.5156 - val_loss: 0.6897 - val_accuracy: 0.5469\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6938 - accuracy: 0.5273 - val_loss: 0.6903 - val_accuracy: 0.5469\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7000 - accuracy: 0.4863 - val_loss: 0.6912 - val_accuracy: 0.5469\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6977 - accuracy: 0.4912 - val_loss: 0.6898 - val_accuracy: 0.5469\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6945 - accuracy: 0.5244 - val_loss: 0.6914 - val_accuracy: 0.5547\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6930 - accuracy: 0.5137 - val_loss: 0.6903 - val_accuracy: 0.5469\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6949 - accuracy: 0.5098 - val_loss: 0.6912 - val_accuracy: 0.5430\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6928 - accuracy: 0.5254 - val_loss: 0.6908 - val_accuracy: 0.5469\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6941 - accuracy: 0.5156 - val_loss: 0.6902 - val_accuracy: 0.5469\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6884 - accuracy: 0.5312 - val_loss: 0.6902 - val_accuracy: 0.5469\n",
            "8/8 [==============================] - 1s 5ms/step\n",
            "Epoch 1/20\n",
            "32/32 [==============================] - 4s 26ms/step - loss: 0.7097 - accuracy: 0.5176 - val_loss: 0.6852 - val_accuracy: 0.5391\n",
            "Epoch 2/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7092 - accuracy: 0.5000 - val_loss: 0.6871 - val_accuracy: 0.5859\n",
            "Epoch 3/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.7092 - accuracy: 0.4912 - val_loss: 0.6888 - val_accuracy: 0.5469\n",
            "Epoch 4/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7003 - accuracy: 0.4990 - val_loss: 0.6880 - val_accuracy: 0.5625\n",
            "Epoch 5/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.7061 - accuracy: 0.4844 - val_loss: 0.6882 - val_accuracy: 0.5469\n",
            "Epoch 6/20\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.6959 - accuracy: 0.5117 - val_loss: 0.6893 - val_accuracy: 0.5469\n",
            "Epoch 7/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6990 - accuracy: 0.5000 - val_loss: 0.6872 - val_accuracy: 0.5469\n",
            "Epoch 8/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6907 - accuracy: 0.5088 - val_loss: 0.6888 - val_accuracy: 0.5391\n",
            "Epoch 9/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6958 - accuracy: 0.5049 - val_loss: 0.6889 - val_accuracy: 0.5430\n",
            "Epoch 10/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6939 - accuracy: 0.5195 - val_loss: 0.6885 - val_accuracy: 0.5469\n",
            "Epoch 11/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6913 - accuracy: 0.5537 - val_loss: 0.6881 - val_accuracy: 0.5508\n",
            "Epoch 12/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6942 - accuracy: 0.5283 - val_loss: 0.6895 - val_accuracy: 0.5430\n",
            "Epoch 13/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6925 - accuracy: 0.5391 - val_loss: 0.6902 - val_accuracy: 0.5391\n",
            "Epoch 14/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6940 - accuracy: 0.5264 - val_loss: 0.6900 - val_accuracy: 0.5352\n",
            "Epoch 15/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6935 - accuracy: 0.5322 - val_loss: 0.6898 - val_accuracy: 0.5352\n",
            "Epoch 16/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6911 - accuracy: 0.5195 - val_loss: 0.6902 - val_accuracy: 0.5352\n",
            "Epoch 17/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6896 - accuracy: 0.5312 - val_loss: 0.6910 - val_accuracy: 0.5312\n",
            "Epoch 18/20\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 0.6914 - accuracy: 0.5176 - val_loss: 0.6901 - val_accuracy: 0.5469\n",
            "Epoch 19/20\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.6936 - accuracy: 0.5283 - val_loss: 0.6911 - val_accuracy: 0.5273\n",
            "Epoch 20/20\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.6904 - accuracy: 0.5293 - val_loss: 0.6907 - val_accuracy: 0.5312\n",
            "8/8 [==============================] - 1s 6ms/step\n",
            "       frontal  central  parietal  occipital\n",
            "theta    54.69    54.69     54.69      54.69\n",
            "alpha    54.69    54.69     53.91      54.69\n",
            "beta     54.69    54.69     54.69      54.69\n",
            "gamma    54.69    54.69     54.69      53.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_conf(\"alpha\",\"frontal\",\"arousal\",\"lstm\")"
      ],
      "metadata": {
        "id": "Qou97TYPwNRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_conf(\"alpha\",\"frontal\",\"valence\",\"lstm\")"
      ],
      "metadata": {
        "id": "giFgrIogwOEu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}